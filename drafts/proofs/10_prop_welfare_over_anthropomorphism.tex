% Proof of Proposition: Welfare Effects of Elevated Anthropomorphism
% Appendix material for main manuscript
% Condensed version: 2026-01-14 (details moved to Online Appendix OA.6.2)

\begin{proof}[Proof of Proposition~\ref{prop:over-anthro}]
Two parts. Both require (A1)--(A3), (A2') attribution monotonicity, and (A2'') attribution non-degeneracy. Part 1 additionally requires (E) cooperation efficiency. Part 2 additionally requires (G') positive guilt sensitivity.

%------------------------------------------------------------------------------
% ZERO-ANTHROPOMORPHISM BENCHMARK
%------------------------------------------------------------------------------

\medskip
\textbf{Zero-Anthropomorphism Benchmark.}
Define the \emph{zero-anthropomorphism benchmark} as:
\begin{equation}\label{eq:rational-benchmark}
    \tilde{h}_i^{(2,j),ZA} \equiv \phi_i(\theta_j, x_j, 0).
\end{equation}
This represents what a non-anthropomorphising agent ($\omega_i = 0$) would attribute. Under standard attribution functions, $\tilde{h}_i^{(2,j),ZA} = \underline{h}$ (machine-like baseline).

\medskip
\textbf{Elevated Anthropomorphism.}
Human $i$ exhibits elevated anthropomorphism toward AI $j$ when:
\begin{equation}
    \tilde{h}_i^{(2,j)} > \tilde{h}_i^{(2,j),ZA}.
\end{equation}
Under (A2') and (A2''), this is equivalent to $\omega_i > 0$. See Online Appendix OA.6.2.1 for verification.

%------------------------------------------------------------------------------
% PART 1: Prosocial AI and Material Welfare
%------------------------------------------------------------------------------

\medskip
\noindent\textbf{Part 1: Elevated anthropomorphism weakly improves material welfare when AI is prosocially designed and cooperation is efficient.}

This is a direct corollary of Proposition~\ref{prop:welfare-anthro}. Higher $\omega_i$ lowers the cooperation threshold $\bar{\omega}_i$ via the indignation constraint:
\begin{equation}
    \beta_i \left[ (n_H - 1) + \lambda_i^{IND} n_A \tilde{h}_i^{(2,j)} \right] \geq E\left(1 - \frac{m}{n}\right).
\end{equation}
Since cooperation is efficient ($W^C - W^D = n(m-1)E > 0$), expanding the cooperation region weakly increases material welfare. The effect is weak ($\geq$) because improvement requires threshold crossing. See Online Appendix OA.6.2.2 for detailed steps.

This completes Part 1. \hfill $\square_1$

%------------------------------------------------------------------------------
% PART 2: Materialist AI and Extended Welfare
%------------------------------------------------------------------------------

\medskip
\noindent\textbf{Part 2: Elevated anthropomorphism may reduce extended welfare when AI is materialistically designed.}

We construct a setting demonstrating the \emph{phantom expectations} mechanism: elevated anthropomorphism of materialist AI creates welfare loss.

\medskip
\emph{Step 1 (Setup).}
Consider the trust game with materialist AI ($\rho_A = 0$). The rational benchmark is $\tilde{h}_H^{(2,A),ZA} = 0$.

\medskip
\emph{Step 2 (Phantom expectations).}
With $\omega_H > 0$, using signal-based attribution $\phi_H(0, x, \omega_H) = \omega_H \cdot 6x$:
\begin{equation}
    \tilde{h}_H^{(2,A)} = \omega_H \cdot 6x > 0 = \tilde{h}_H^{(2,A),ZA}.
\end{equation}
These attributed expectations are \textbf{phantom}---they exist in the human's model but correspond to nothing in AI preferences.

\medskip
\emph{Step 3 (Equilibrium returns).}
Under (G') with $G > 0$, $y^* = \min\{\tilde{h}_H^{(2,A)}, 3x\}$.

\medskip
\emph{Step 4 (Extended welfare comparison).}
Under zero-anthropomorphism: $y^{*,ZA} = 0$, $\psi_H^{GUILT,ZA} = 0$, $W^{ext}(\tilde{h}^{ZA}) = E + 2x$.

Under severe elevated anthropomorphism ($\omega_H > 0.5$, so $\tilde{h}_H^{(2,A)} > 3x$): $y^* = 3x$ but guilt arises:
\begin{equation}
    \psi_H^{GUILT} = -G \cdot (6\omega_H - 3)x < 0.
\end{equation}
Extended welfare: $W^{ext}(\tilde{h}) = E + 2x - G(6\omega_H - 3)x < W^{ext}(\tilde{h}^{ZA})$.

At $\omega_H = 0.75$ with $E = 10$, $x = 10$, $G = 1.5$: extended welfare falls from 30 to 7.5. See Online Appendix OA.6.2.3 for complete calculations.

\medskip
\emph{Step 5 (Mechanism).}
Extended welfare reduction requires: (1) materialist AI ($\rho_A = 0$); (2) elevated anthropomorphism ($\omega_H > 0$); (3) phantom expectations exceeding feasibility ($\tilde{h} > 3x$); (4) positive guilt sensitivity ($G > 0$). When all hold, humans incur guilt from failing to meet expectations that (a) the AI never held and (b) were impossible to satisfy---pure welfare loss.

This completes Part 2. \hfill $\square_2$
\end{proof}

\begin{remark}
Extended analysis including detailed welfare calculations, numerical examples, and policy implications appears in Online Appendix OA.6.2.
\end{remark}
