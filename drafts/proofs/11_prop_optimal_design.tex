% Proof of Proposition: Optimal AI Design
% Version: 3.0 (condensed for publication)
% Date: 2026-01-14
% Technical details moved to Online Appendix OA.7

\begin{proof}[Proof of Proposition~\ref{prop:optimal-design}]
Three parts. All parts require (A1)--(A3) regularity, (A2') attribution monotonicity, and (A2''') signal monotonicity. Part (i) additionally requires (E) cooperation efficiency, (I) indignation dominance, and (T) temptation dominance. Parts (ii) and (iii) additionally require (G') positive guilt sensitivity. Full assumption statements appear in Online Appendix OA.7.1.

The planner maximizes reduced-form welfare $\mathcal{W}(x)$ over signal $x \in [0, \bar{x}]$; existence follows from Weierstrass.

%------------------------------------------------------------------------------
% PART (i): PROSOCIAL AI
%------------------------------------------------------------------------------

\medskip
\noindent\textbf{Part (i): Prosocial AI ($\rho_A = 1$).}

Consider the public goods game with prosocial AI ($\rho_A = 1$) contributing $c_A = E$. We use material welfare $W(s^*(x))$, justified because cooperation meets attributed expectations, so guilt is zero.

\medskip
\emph{Step 1 (Cooperation threshold).}
Cooperation requires $\omega \geq \bar{\omega}(x)$, where:
\begin{equation}\label{eq:threshold}
    \bar{\omega}(x) = \frac{E(1 - m/n) - \beta(n_H - 1)}{\beta \lambda^{IND} n_A (\bar{h}_H + \eta x)}.
\end{equation}
Under (T) and (I), $\bar{\omega}(x) \in (0,1)$ for all $x \in [0, \bar{x}]$ (proof in Online Appendix OA.7.2).

\medskip
\emph{Step 2 (Threshold comparative static).}
Differentiating \eqref{eq:threshold}:
\begin{equation}
    \frac{\partial \bar{\omega}}{\partial x} = -\frac{E(1 - m/n) - \beta(n_H - 1)}{\beta \lambda^{IND} n_A} \cdot \frac{\eta}{(\bar{h}_H + \eta x)^2} < 0.
\end{equation}
Higher $x$ lowers the cooperation threshold.

\medskip
\emph{Step 3 (Optimal signal).}
Material welfare is $W^C = nmE$ under cooperation, $W^D < W^C$ under defection. Three cases arise:

\textbf{Case 1:} $\omega \geq \bar{\omega}(0)$. Cooperation at $x = 0$; set $x^* = 0$.

\textbf{Case 2:} $\omega < \bar{\omega}(\bar{x})$. Cooperation unattainable; any $x$ is optimal.

\textbf{Case 3:} $\bar{\omega}(\bar{x}) \leq \omega < \bar{\omega}(0)$. Set $x^* = x_{crit}$ where $\bar{\omega}(x_{crit}) = \omega$:
\begin{equation}\label{eq:opt-x-i}
    x^* = \max\left\{ 0, \frac{1}{\eta}\left[ \frac{E(1-m/n) - \beta(n_H-1)}{\beta \lambda^{IND} n_A \omega} - \bar{h}_H \right] \right\}.
\end{equation}

\medskip
\emph{Step 4 (Comparative static).}
From \eqref{eq:opt-x-i}:
\begin{equation}
    \frac{\partial x^*}{\partial m} = \frac{1}{\eta} \cdot \frac{-E/n}{\beta \lambda^{IND} n_A \omega} < 0.
\end{equation}
Higher efficiency reduces the required signal because the material temptation $E(1 - m/n)$ decreases.

This completes Part (i). \hfill $\square$

%------------------------------------------------------------------------------
% PART (ii): MATERIALIST AI
%------------------------------------------------------------------------------

\medskip
\noindent\textbf{Part (ii): Materialist AI ($\rho_A = 0$).}

Consider the trust game. With $\rho_A = 0$, all attributed expectations are phantom: $\tilde{h}_H^{(2,A)} = \omega_H \eta x$.

\medskip
\emph{Step 1 (Equilibrium return).}
Under (G') with $G > 0$, the guilt-averse human sets $y^* = \min\{\omega_H \eta x, 3x_{send}\}$.

\medskip
\emph{Step 2 (Extended welfare).}
Complete case analysis (Online Appendix OA.7.3) shows:
\begin{equation}
    W^{ext}(x) = \begin{cases}
        E + 2x_{send} & \text{if } \omega_H \eta x \leq 3x_{send} \\
        E + 2x_{send} - G(\omega_H \eta x - 3x_{send}) & \text{if } \omega_H \eta x > 3x_{send}
    \end{cases}
\end{equation}

For all $x \geq 0$: $W^{ext}(x) \leq W^{ext}(0)$, with strict inequality when phantom expectations exceed feasibility. Therefore:
\begin{equation}
    x^* = 0.
\end{equation}

Minimal anthropomorphism is optimal: anthropomorphic presentation creates potential guilt costs without material benefit.

This completes Part (ii). \hfill $\square$

%------------------------------------------------------------------------------
% PART (iii): MIXED AI
%------------------------------------------------------------------------------

\medskip
\noindent\textbf{Part (iii): Mixed AI ($\rho_A \in (0,1)$).}

With partial prosociality, the planner faces a tradeoff: higher $x$ expands cooperation (benefit) but raises guilt from phantom expectations (cost).

\medskip
\emph{Step 1 (Planner's tradeoff).}
Extended welfare with heterogeneous $\omega_i \sim F$:
\begin{equation}
    \mathcal{W}(x) = W^{mat}(x) + \Psi(x),
\end{equation}
where $W^{mat}(x)$ is expected material welfare and $\Psi(x)$ is expected aggregate psychological welfare.

\medskip
\emph{Step 2 (First-order condition).}
At interior optimum $x^* \in (0, \bar{x})$, the FOC $\mathcal{W}'(x^*) = 0$ balances marginal cooperation benefits against marginal guilt costs. The SOC $\mathcal{W}''(x^*) < 0$ is verified in Online Appendix OA.7.4.

\medskip
\emph{Step 3 (Comparative statics).}
Via implicit function theorem (derivations in Online Appendix OA.7.4):
\begin{equation}
    \frac{\partial x^*}{\partial m} > 0, \quad \frac{\partial x^*}{\partial G} < 0, \quad \frac{\partial x^*}{\partial \omega} < 0.
\end{equation}

Higher efficiency justifies more signal (larger cooperation gains). Higher guilt sensitivity or anthropomorphism tendency justifies less signal (larger psychological costs).

\medskip
\emph{Reconciliation with Part (i).}
Part (i) shows $\partial x^*/\partial m < 0$; Part (iii) shows $\partial x^*/\partial m > 0$. These are not contradictory: Part (i) finds the \emph{minimal threshold-crossing signal}, which decreases with $m$; Part (iii) balances welfare at the margin, where higher $m$ justifies more signal. As $\rho_A \to 1$, guilt costs vanish and Part (iii) degenerates to Part (i).

This completes Part (iii). \hfill $\square$
\end{proof}

\begin{remark}
Technical details including full assumption statements, threshold analysis, complete case analysis for Part (ii), SOC/IFT derivations, and extended discussion appear in Online Appendix OA.7.
\end{remark}
