% Section 2: The Formal Framework
\section{The Formal Framework}
\label{sec:framework}

This section presents the formal structure of asymmetric psychological games with human and AI players. We introduce the primitives, belief hierarchies, and the attribution function that captures how humans project mental states onto AI agents.

\subsection{Players and Types}

The population consists of two disjoint player sets: a set $N_H$ of \textbf{human players} with $|N_H| = n_H \geq 1$, and a set $N_A$ of \textbf{AI agents} with $|N_A| = n_A \geq 0$. The total player set is $N = N_H \cup N_A$ with $|N| = n = n_H + n_A$. Define the human population share as $\alpha = n_H / n \in (0, 1]$.

Each player $i \in N$ has a finite strategy set $S_i$, with mixed strategies $\sigma_i \in \Delta(S_i)$. The strategy profile space is $S = \prod_{i \in N} S_i$.

Players are characterised by type parameters. For humans $i \in N_H$, the type $t_i = (\beta_i, \gamma_i, \omega_i, \ldots) \in T_i$ encodes psychological characteristics: indignation sensitivity $\beta_i$, guilt sensitivity $\gamma_i$, and anthropomorphism tendency $\omega_i \in [0,1]$. For AI $j \in N_A$, the design parameters $\theta_j \in \Theta_j$ encode programmed objectives.

\subsection{Payoffs}

Payoffs decompose into material and psychological components. For all players, $\pi_i(s)$ denotes the material payoff given strategy profile $s$. For humans, total utility is
\begin{equation}
    U_i^H(s, h_i, \tilde{h}_i) = \pi_i(s) + \psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)}),
\end{equation}
where $\psi_i$ is the psychological payoff depending on second-order beliefs (defined formally in Definitions \ref{def:indignation}--\ref{def:guilt} below). We write $U_i^H(s_i, s_{-i}, h_i, \tilde{h}_i)$ when emphasizing individual strategy choice. AI utility is design-dependent with no psychological component:
\begin{equation}
    U_j^A(s; \theta_j) = f_j(s; \theta_j).
\end{equation}

Common AI specifications include materialist ($U_j^A = \pi_j(s)$), prosocial ($U_j^A = \pi_j(s) + \rho_j \sum_{k \in N} \pi_k(s)$), and conditional objectives.

\subsection{Belief Hierarchies}

Following \citet{mertens1985formulation} and \citet{battigalli2009dynamic}, we construct belief hierarchies recursively. For human $i \in N_H$:
\begin{align}
    h_i^{(0)} &= t_i \in T_i \quad \text{(type)} \\
    h_i^{(1)} &\in \Delta(S_{-i}) \quad \text{(first-order beliefs about others' play)} \\
    h_i^{(2,k)} &\in \Delta(\Delta(S_{-k})) \quad \text{(second-order beliefs: what } k \text{ expects from others)}
\end{align}
We use $h_i^{(n)}$ instead of the $\beta_i^{(n)}$ notation from \citet{battigalli2009dynamic} to avoid confusion with the indignation sensitivity parameter $\beta$. We write $h_i^{(1,k)}$ for the marginal of $h_i^{(1)}$ on player $k$'s strategy. For action $a \in S_k$, we write $h_i^{(2,k)}(a)$ for the probability that player $i$ believes player $k$ expected action $a$. We write $h_i^{(2)} = \{h_i^{(2,k)}\}_{k \neq i}$ for the collection of second-order beliefs.

\subsection{Attributed Beliefs}

The key departure from standard psychological game theory is the distinction between genuine and attributed beliefs.

\begin{defn}[Attributed Beliefs]
\label{def:attributed-beliefs}
For human $i \in N_H$ interacting with AI $j \in N_A$, the \textbf{attributed second-order belief} is:
\begin{equation}
    \tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i)
\end{equation}
where $\theta_j \in \Theta_j$ is the AI's design parameters, $x_j \in X$ is observable signals (interface design, behavioural cues), and $\omega_i \in [0,1]$ is human $i$'s anthropomorphism tendency.
\end{defn}

The attributed belief $\tilde{h}_i^{(2,j)} \in \Delta(S_i)$ represents ``what human $i$ believes AI $j$ expected human $i$ to do.'' This is the belief that triggers guilt or indignation when human $i$ disappoints the AI's perceived expectations.

The key distinction: \textbf{genuine beliefs} $h_i^{(2,k)}$ about other humans $k \in N_H$ are formed through observation and Bayesian updating; \textbf{attributed beliefs} $\tilde{h}_i^{(2,j)}$ about AI $j \in N_A$ are formed through psychological projection via the attribution function.

\subsection{The Attribution Function}

The attribution function is the novel primitive of ABE theory, formalising how humans project mental states onto AI agents.

\begin{defn}[Attribution Function]
\label{def:attribution-function}
For each human $i \in N_H$, the attribution function is a mapping
\begin{equation}
    \phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)
\end{equation}
that determines how human $i$ attributes expectations to AI $j$ based on AI's design parameters $\theta_j$, observable signals $x_j$, and the human's anthropomorphism tendency $\omega_i$.
\end{defn}

\begin{defn}[Zero-Anthropomorphism Benchmark]
\label{def:zero-anthro}
For human $i \in N_H$ interacting with AI $j \in N_A$, the \textbf{zero-anthropomorphism attributed belief} is:
\begin{equation}
    \tilde{h}_i^{(2,j),ZA} = \phi_i(\theta_j, x_j, 0)
\end{equation}
This represents the attributed belief when anthropomorphism is absent ($\omega_i = 0$). The zero-anthropomorphism benchmark is descriptive: it characterizes attribution without anthropomorphic bias, not a normative standard for ``correct'' beliefs.
\end{defn}

The attribution function formalises the empirically established process of mind perception \citep{gray2007dimensions,epley2007seeing}. Its inputs correspond to well-established antecedents of intentionality attribution \citep{wiese2017robots}. Attribution intensity varies with human-like cues including voice \citep{schroeder2016voice}, gaze behaviour, and movement patterns, all of which can be captured in the signal vector $x_j$.

Three main approaches to specifying $\phi_i$ are:
\begin{enumerate}
    \item \textbf{Behavioural attribution}: $\phi_i^{beh}(\theta_j, x_j, \omega_i) = g(s_j^{obs}, \omega_i)$, depending on AI's observed behaviour
    \item \textbf{Signal-based attribution}: $\phi_i^{sig}(\theta_j, x_j, \omega_i) = g(x_j, \omega_i)$, depending primarily on observable signals
    \item \textbf{Dispositional attribution}: $\phi_i^{disp}(\theta_j, x_j, \omega_i) = \omega_i \cdot \bar{h} + (1 - \omega_i) \cdot \underline{h}$, depending primarily on the human's anthropomorphism tendency
\end{enumerate}

\begin{example}[Linear Attribution]
\label{ex:linear-attribution}
Linearity is tractable: first-order approximations yield closed-form equilibria and testable predictions. Mind perception research finds monotonic relationships between anthropomorphism and attributed mental states---linearity captures this without untested curvature. We adopt:
\begin{equation}
    \tilde{h}_i^{(2,j)}(C) = \omega_i \cdot \left( \rho_j + \eta \cdot x_j \right)
\end{equation}
The parameter $\omega_i \in [0,1]$ measures individual anthropomorphism tendency and multiplies the entire expression: when $\omega_i = 0$, no beliefs are attributed regardless of AI characteristics, representing the limiting case of purely mechanistic perception. Inside the bracket, $\rho_j \in [0,1]$ captures AI prosociality embedded in design, while $\eta \cdot x_j$ (with signal strength $x_j \in [0,1]$ and sensitivity $\eta > 0$) captures presentation effects from anthropomorphic cues. These terms are additive because they represent distinct attribution channels---what the AI is designed to do versus how human-like it appears. Section~\ref{sec:empirical-implementation} derives testable predictions; Section~\ref{sec:mind-perception} grounds this specification in established mind perception theory.
\end{example}

\subsubsection{Empirical Implementation and Testability}
\label{sec:empirical-implementation}

The attribution function $\phi_i$ is identified through experimental variation in its arguments. We describe the measurement strategy, illustrate with a trust game design, and derive testable predictions.

\paragraph{Identification Strategy.}

To identify $\phi_i$, we measure attributed beliefs $\tilde{h}_i^{(2,j)}$ while varying AI signals $x_j$ and measuring individual anthropomorphism $\omega_i$. Three components enable identification.

\emph{Belief elicitation.} We elicit attributed beliefs using incentive-compatible mechanisms. Quadratic scoring rules \citep{gachter2010peer} reward accuracy: subjects report their belief that the AI expects cooperation, and payment increases quadratically in the accuracy of that belief. To address hedging---subjects adjusting stated beliefs to insure against game outcomes---we follow \citet{blanco2010belief} in paying either the belief elicitation task or the strategic game, never both. This eliminates the incentive to hedge beliefs against strategic payoffs.

\emph{Strategy method.} Complete mapping of the attribution function requires observing $\tilde{h}_i^{(2,j)}$ across the full domain of $(x_j, \omega_i)$. The strategy method \citep{brandts2011strategy} elicits beliefs for each possible signal level within-subject, recovering the full $\phi_i$ mapping without relying on between-subject variation alone. Subjects state their attributed belief conditional on each interface treatment before learning which treatment applies.

\emph{Anthropomorphism measurement.} Individual anthropomorphism $\omega_i$ is measured using the Individual Differences in Anthropomorphism Questionnaire (IDAQ; \citealp{waytz2010who}). The IDAQ contains 15 items measuring attribution of mental states to non-human entities (e.g., ``To what extent does a robot have intentions?''). We use the technology subscale, which correlates with trust in automated systems \citep{waytz2014trusting}.

\paragraph{Application: Trust Game with Interface Manipulation.}

We illustrate identification with a trust game where an AI trustor sends $x \in [0, E]$ to a human trustee who returns $y \in [0, 3x]$. Between-subject manipulation varies interface anthropomorphism across three treatments: robotic ($x_A = 0$), humanoid ($x_A = 1$), and control ($x_A = 0.5$), holding AI behavior constant. Subjects complete the IDAQ, observe the AI interface, report attributed beliefs via incentivized elicitation, and choose return amounts. Random payment selection (belief or choice) eliminates hedging.

\paragraph{Parameter Recovery.}

Under the linear attribution specification from Example 1:
\begin{equation}
    \tilde{h}_i^{(2,A)} = \omega_i \cdot \left( \rho_A + \eta \cdot x_A \right)
\end{equation}
treatment variation identifies $\eta$; $\rho_A$ is assumed known from AI design. The reduced-form regression is:
\begin{equation}
    \tilde{h}_{it}^{(2,A)} = \beta_0 + \beta_1 \omega_i + \beta_2 x_{At} + \beta_3 \omega_i \cdot x_{At} + \varepsilon_{it}
    \label{eq:attribution-regression}
\end{equation}
where $t \in \{R, H, C\}$ indexes treatments.

The theoretical model generates testable restrictions:
\begin{enumerate}
    \item $\beta_2 = 0$: Signal has no direct effect on attributed beliefs; the effect operates through anthropomorphism
    \item $\beta_3 > 0$: Anthropomorphism moderates signal effect; high-$\omega_i$ subjects respond more to interface manipulation
\end{enumerate}

The interaction coefficient $\beta_3$ directly estimates the signal sensitivity $\eta$. Given $\rho_A$ known from design, the baseline effect $\beta_1$ identifies anthropomorphism scaling.

The key testable prediction is the interaction effect $\beta_3 > 0$. Under the null hypothesis that attributed beliefs do not depend on anthropomorphism ($\omega_i$ irrelevant), $\beta_3 = 0$ and interface manipulation affects all subjects equally. A positive interaction indicates that anthropomorphic signals operate through the attribution mechanism specified in the model. Alternative specifications (logistic, threshold) accommodate saturation and uncanny valley effects; we maintain linearity for tractability.

\subsubsection{Psychological Foundations: Mind Perception Theory}
\label{sec:mind-perception}

The attribution function $\phi_i$ in Definition~\ref{def:attribution-function} formalizes how humans project mental states onto AI agents. This formalization rests on mind perception psychology.

The SEEK framework \citep{epley2007seeing} identifies three determinants of anthropomorphism: dispositional factors (sociality, effectance) corresponding to $\omega_i$, and situational triggers (elicited agent knowledge) corresponding to observable signals $x_j$. Mind perception decomposes into agency and experience \citep{gray2007dimensions}. AI scores high on agency (triggering attribution) but low on experience (driving attenuation). These dimensions operate independently, explaining their coexistence.

Anthropomorphism operates as System~1 cognition: automatic and resistant to conscious override. System~2 correction is effortful and incomplete \citep{epley2007seeing}---even users who know AI lacks mental states exhibit residual attribution. This dual-process structure grounds our functional form: the linear specification reflects SEEK's prediction that attribution increases monotonically in both dispositional tendency ($\omega_i$) and situational triggers ($x_j$).

\subsection{Psychological Payoffs}

Two main psychological mechanisms are relevant: indignation and guilt.

\begin{defn}[Indignation with Attenuation]
\label{def:indignation}
The indignation payoff captures disutility from disappointing others' expectations:
\begin{equation}
    \psi_i^{IND}(s, h_i^{(2)}, \tilde{h}_i^{(2)}) = -\beta_i \cdot \mathbf{1}_{s_i = D} \cdot \left[ \sum_{k \in N_H} h_i^{(2,k)}(C) + \lambda_i^{IND} \sum_{j \in N_A} \tilde{h}_i^{(2,j)}(C) \right]
\end{equation}
where $\beta_i > 0$ is indignation sensitivity, $h_i^{(2,k)}(C)$ is the probability that human $i$ believes human $k$ expected cooperation, $\tilde{h}_i^{(2,j)}(C)$ is the attributed probability that AI $j$ expected cooperation, $\lambda_i^{IND} \in [0,1]$ is the indignation attenuation factor toward AI, and $\mathbf{1}_{s_i = D}$ is the indicator for defection (adapted to context-specific actions in applications).
\end{defn}

The attenuation of indignation toward AI reflects intent attribution. Indignation requires perceiving malicious intent, but humans perceive AI behaviour as data-driven rather than prejudice-driven \citep{bigman2023people}.

\begin{defn}[Guilt Aversion with Attenuation]
\label{def:guilt}
The guilt payoff captures disutility from falling short of perceived obligations:
\begin{equation}
    \psi_i^{GUILT}(s, h_i^{(2)}, \tilde{h}_i^{(2)}) = -\gamma_i \cdot \left[ \sum_{k \in N_H} \max\{0, h_i^{(2,k)} - \pi_k(s)\} + \lambda_i^{GUILT} \sum_{j \in N_A} \max\{0, \tilde{h}_i^{(2,j)} - \pi_j(s)\} \right]
\end{equation}
where $\gamma_i > 0$ is guilt sensitivity and $\lambda_i^{GUILT} \in [0,1]$ is the guilt attenuation factor toward AI.
\end{defn}

\citet{demelo2017people} find that participants feel ``considerably less guilt'' when exploiting machines than humans, suggesting guilt requires attribution of experience (capacity to suffer) that humans do not readily grant to AI. Cross-cultural evidence reveals heterogeneity: Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation \citep{karpus2025cross}.

\subsection{Welfare Measures}
\label{sec:welfare-measures}

Two welfare concepts arise naturally from the payoff decomposition. We define each, establish normative foundations, and characterize divergence.

\begin{defn}[Material Welfare]
\label{def:material-welfare}
For strategy profile $s \in S$, \textbf{material welfare} is the sum of material payoffs:
\begin{equation}
    W(s) = \sum_{i \in N} \pi_i(s).
\end{equation}
\end{defn}

Material welfare counts only outcomes that are transferable, verifiable, and comparable across agents---monetary payoffs, consumption, or quantities.

\begin{defn}[Extended Welfare]
\label{def:extended-welfare}
For strategy profile $s \in S$ with belief profile $(h, \tilde{h})$, \textbf{extended welfare} is:
\begin{equation}
    W^{ext}(s, h, \tilde{h}) = W(s) + \sum_{i \in N_H} \psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)}).
\end{equation}
\end{defn}

Extended welfare includes the psychological payoffs $\psi_i$ that humans experience. AI agents contribute no psychological terms since they lack subjective experience.

\subsubsection{Normative Justification}
\label{sec:welfare-normative}

The choice between welfare measures reflects a prior normative position on the status of psychological payoffs.

\citet{kahneman1997back} distinguish \emph{decision utility}---revealed preference over choices---from \emph{experienced utility}---hedonic quality of outcomes. Material welfare $W$ aligns with decision utility: it counts what agents choose to maximize, abstracting from the subjective states accompanying those choices. Extended welfare $W^{ext}$ aligns with experienced utility: it incorporates hedonic content, including guilt and disappointment.

When should each apply? The answer depends on whether psychological payoffs have intrinsic or instrumental value.

\paragraph{Instrumental view.} If guilt and disappointment matter only because they distort decisions, then $W$ is the appropriate measure. Psychological disutility is a means to material ends---it motivates behavior but has no standing in the social objective. Under this view, a planner seeks to maximize material outcomes, treating psychological costs as constraints that shape equilibrium but not as ends in themselves.

\paragraph{Intrinsic view.} If subjective experience has inherent value, then $W^{ext}$ applies. Guilt reduces human well-being directly, not merely through its behavioral consequences. A planner who values experienced happiness includes $\psi_i$ in the objective.

Standard welfare economics adopts the instrumental view: consumer surplus and Pareto efficiency concern material allocations. Yet behavioral welfare economics increasingly recognizes that experienced utility may diverge from decision utility \citep{kahneman1997back}. In human-AI interaction, this divergence takes a specific form: humans experience guilt toward agents that cannot reciprocate the experience.

\subsubsection{When Measures Diverge}
\label{sec:welfare-divergence}

With prosocial AI ($\rho_j > 0$), the two measures typically agree. The AI's objective incorporates human welfare, so attributed beliefs reflect genuine concern. Guilt from disappointing a prosocial AI corresponds to disappointing an entity that valued cooperation---the psychological cost has a material counterpart in the AI's design.

With materialist AI ($\rho_j = 0$), the measures can diverge. The AI maximizes its own material payoff and holds no expectations about human cooperation. Yet humans may attribute such expectations through anthropomorphism. The resulting guilt is real---humans experience the disutility---but corresponds to no genuine expectation.

\begin{example}[Phantom Expectations]
\label{ex:phantom-expectations}
Consider human $i$ interacting with materialist AI $j$. The AI's objective is $U_j^A = \pi_j(s)$, containing no term that depends on $i$'s cooperation. The AI holds no expectation about $i$'s behavior.

If $\omega_i > 0$, human $i$ attributes beliefs via $\tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i) > 0$. Human $i$ believes the AI expected cooperation. When $i$ defects, guilt follows:
\begin{equation}
    \psi_i^{GUILT} = -\gamma_i \lambda_i^{GUILT} \max\{0, \tilde{h}_i^{(2,j)} - \pi_j(s)\} < 0.
\end{equation}
This guilt is phantom: it responds to attributed expectations that exist only in the human's model of the AI. Material welfare $W$ ignores this cost. Extended welfare $W^{ext}$ counts it.
\end{example}

The divergence has policy implications. If a designer maximizes $W$, phantom guilt is irrelevant---only material outcomes matter. If the designer maximizes $W^{ext}$, phantom guilt is costly even when AI is indifferent. The optimal design of AI interfaces may differ across objectives.

\paragraph{Belief Revision.} Why don't humans eliminate phantom expectations by revising attributed beliefs? Three mechanisms prevent full revision: automaticity (attribution operates as System~1 cognition), limited correction capacity (System~2 override requires effort unavailable in interactive settings), and incomplete correction residue (even informed users exhibit residual anthropomorphism). Attributed beliefs reflect cognitive architecture, not irrationality.

\subsection{Game Definition}

\begin{defn}[Asymmetric Human-AI Psychological Game]
\label{def:game}
An asymmetric psychological game is a tuple
\begin{equation}
    \Gamma = (N_H, N_A, \{T_i\}_{i \in N_H}, \{\Theta_j\}_{j \in N_A}, \{S_i\}_{i \in N}, \{U_i^H\}_{i \in N_H}, \{U_j^A\}_{j \in N_A}, \phi, p)
\end{equation}
where $N_H, N_A$ are human and AI player sets, $T_i$ and $\Theta_j$ are type spaces, $S_i$ are strategy sets, $U_i^H$ and $U_j^A$ are utility functions, $\phi = \{\phi_i\}_{i \in N_H}$ are attribution functions, and $p$ is the common prior over types.
\end{defn}

\subsection{Assumptions}

We impose the following regularity conditions:

\begin{assumption}[Regularity]
\label{ass:regularity}
(A1) Strategy spaces $S_i$ are non-empty and finite. Type spaces $T_i$ and $\Theta_j$ are non-empty, convex, and compact. Payoff functions are continuous.
\end{assumption}

\begin{assumption}[Attribution Continuity]
\label{ass:attribution}
(A2) The attribution function $\phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)$ is continuous in $(\theta_j, x_j)$ for fixed $\omega_i$.
\end{assumption}

\begin{assumption}[Bounded Psychological Payoffs]
\label{ass:bounded}
(A3) There exists $M < \infty$ such that $|\psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)})| \leq M$ for all $i \in N_H$, all $s \in S$, and all beliefs.
\end{assumption}

\subsection{Assumption Summary}
\label{sec:assumption-summary}

Table~\ref{tab:assumptions} collects all assumptions. Core assumptions (A1)--(A3) support existence. Optional extensions and context-specific conditions apply to individual results.

\begin{table}[htbp]
\centering
\caption{Summary of Assumptions}
\label{tab:assumptions}
\small
\begin{tabular}{@{}llp{7.5cm}l@{}}
\toprule
\textbf{Label} & \textbf{Name} & \textbf{Statement} & \textbf{Required for} \\
\midrule
\multicolumn{4}{@{}l}{\textit{Core Assumptions}} \\[0.5ex]
(A1) & Regularity & Strategy spaces $S_i$ are finite; type spaces $T_i$, $\Theta_j$ are compact and convex; payoffs are continuous & All results \\[0.5ex]
(A2) & Attribution Continuity & $\phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)$ is continuous in $(\theta_j, x_j)$ & All results \\[0.5ex]
(A3) & Bounded Psychological Payoffs & $|\psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)})| \leq M$ for some $M < \infty$ & All results \\[1ex]
\midrule
\multicolumn{4}{@{}l}{\textit{Optional Extensions to (A2)}} \\[0.5ex]
(A2$'$) & Attribution Monotonicity & $\omega_i' > \omega_i \Rightarrow \tilde{h}_i^{(2,j)}(\omega_i') \geq \tilde{h}_i^{(2,j)}(\omega_i)$ & Props.\ 5--10$'$, Cors.\ 2--3 \\[0.5ex]
(A2$''$) & Attribution Non-Degeneracy & $\phi_i$ is not constant in $\omega_i$ when $\bar{h}_H > \underline{h}$ & Prop.\ 9$'$ \\[0.5ex]
(A2$'''$) & Signal Monotonicity & $\partial \tilde{h}_i^{(2,j)} / \partial x_j \geq 0$ for $\omega_i > 0$ & Props.\ 10, 10$'$, Cors.\ 2--3 \\[1ex]
\midrule
\multicolumn{4}{@{}l}{\textit{Context-Specific Conditions}} \\[0.5ex]
(G) & Guilt Dominance & $G \equiv \gamma_H \lambda_H^{GUILT} > 1$ & Props.\ 5, 8 \\[0.5ex]
(G$'$) & Positive Guilt Sensitivity & $G \equiv \gamma_H \lambda_H^{GUILT} > 0$ & Props.\ 9$'$, 10, 10$'$ \\[0.5ex]
(I) & Indignation Dominance & $\beta_i[(n_H - 1) + \lambda_i^{IND} n_A] > E(1 - m/n)$ & Props.\ 6, 8, 10, 10$'$ \\[0.5ex]
(E) & Cooperation Efficiency & $m > 1$ & Props.\ 8, 9, 9$'$, 10, 10$'$ \\[0.5ex]
(C) & Signal Clarity & $x_A > 0.5$ & Prop.\ 7 \\[0.5ex]
(T) & Temptation Dominance & $E(1 - m/n) > \beta_i(n_H - 1)$ & Props.\ 10, 10$'$ \\
\bottomrule
\end{tabular}
\end{table}
