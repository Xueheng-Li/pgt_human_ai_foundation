% Section 2: The Formal Framework
\section{The Formal Framework}
\label{sec:framework}

This section presents the formal structure of asymmetric psychological games with human and AI players. We introduce the primitives, belief hierarchies, and the attribution function that captures how humans project mental states onto AI agents. Table~\ref{tab:assumptions} summarizes all assumptions; core conditions (A1)--(A3) ensure existence.

\subsection{Players and Types}

The population consists of two disjoint player sets: a set $N_H$ of \textbf{human players} with $|N_H| = n_H \geq 1$, and a set $N_A$ of \textbf{AI agents} with $|N_A| = n_A \geq 0$. The total player set is $N = N_H \cup N_A$ with $|N| = n = n_H + n_A$. Define the human population share as $\alpha = n_H / n \in (0, 1]$.

Each player $i \in N$ has a finite strategy set $S_i$, with mixed strategies $\sigma_i \in \Delta(S_i)$. The strategy profile space is $S = \prod_{i \in N} S_i$.

Players are characterised by type parameters. For humans $i \in N_H$, the type $t_i = (\beta_i, \gamma_i, \omega_i, \ldots) \in T_i$ encodes psychological characteristics: indignation sensitivity $\beta_i$, guilt sensitivity $\gamma_i$, and anthropomorphism tendency $\omega_i \in [0,1]$. For AI $j \in N_A$, the design parameters $\theta_j \in \Theta_j$ encode programmed objectives.

\subsection{Payoffs}

Payoffs decompose into material and psychological components. For all players, $\pi_i(s)$ denotes the material payoff given strategy profile $s$. For humans, total utility is
\begin{equation}
    U_i^H(s, h_i, \tilde{h}_i) = \pi_i(s) + \psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)}),
\end{equation}
where $\psi_i$ is the psychological payoff depending on second-order beliefs (defined formally in Definitions \ref{def:indignation}--\ref{def:guilt} below). We write $U_i^H(s_i, s_{-i}, h_i, \tilde{h}_i)$ when emphasizing individual strategy choice. AI utility is design-dependent with no psychological component:
\begin{equation}
    U_j^A(s; \theta_j) = f_j(s; \theta_j).
\end{equation}

Common AI specifications include materialist ($U_j^A = \pi_j(s)$), prosocial ($U_j^A = \pi_j(s) + \rho_j \sum_{k \in N} \pi_k(s)$), and conditional objectives.

\subsection{Belief Hierarchies}

Following \citet{mertens1985formulation}, \citet{battigalli2009dynamic}, and \citet{battigalli2019incorporating}, we construct belief hierarchies recursively. For human $i \in N_H$:
\begin{align}
    h_i^{(0)} &= t_i \in T_i \quad \text{(type)} \\
    h_i^{(1)} &\in \Delta(S_{-i}) \quad \text{(first-order beliefs about others' play)} \\
    h_i^{(2,k)} &\in \Delta(\Delta(S_{-k})) \quad \text{(second-order beliefs: what } k \text{ expects from others)}
\end{align}
We use $h_i^{(n)}$ instead of the $\beta_i^{(n)}$ notation from \citet{battigalli2009dynamic} to avoid confusion with the indignation sensitivity parameter $\beta$. We write $h_i^{(1,k)}$ for the marginal of $h_i^{(1)}$ on player $k$'s strategy. For action $a \in S_k$, we write $h_i^{(2,k)}(a)$ for the probability that player $i$ believes player $k$ expected action $a$. We write $h_i^{(2)} = \{h_i^{(2,k)}\}_{k \neq i}$ for the collection of second-order beliefs. In applications, we work with point beliefs where $h_i^{(2,k)}$ places mass on a single expectation; thus $h_i^{(2,k)}(a)$ denotes the probability that $i$ believes $k$ expected action $a$.

\subsection{Attributed Beliefs}

The key departure from standard psychological game theory is the distinction between genuine and attributed beliefs.

\begin{defn}[Attributed Beliefs]
\label{def:attributed-beliefs}
For human $i \in N_H$ interacting with AI $j \in N_A$, the \textbf{attributed second-order belief} is:
\begin{equation}
    \tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i)
\end{equation}
where $\theta_j \in \Theta_j$ is the AI's design parameters, $x_j \in X$ is observable interface signals---design features, not strategic choices (see Remark~\ref{rem:transfer-signal}), and $\omega_i \in [0,1]$ is human $i$'s anthropomorphism tendency.
\end{defn}

The attributed belief $\tilde{h}_i^{(2,j)} \in \Delta(S_i)$ represents ``what human $i$ believes AI $j$ expected human $i$ to do.'' This is the belief that triggers guilt or indignation when human $i$ disappoints the AI's perceived expectations.

The key distinction: \textbf{genuine beliefs} $h_i^{(2,k)}$ about other humans $k \in N_H$ are formed through observation and Bayesian updating; \textbf{attributed beliefs} $\tilde{h}_i^{(2,j)}$ about AI $j \in N_A$ are formed through psychological projection via the attribution function.

\subsection{The Attribution Function}

The attribution function is the novel primitive of ABE theory, formalising how humans project mental states onto AI agents.

\begin{defn}[Attribution Function]
\label{def:attribution-function}
For each human $i \in N_H$, the attribution function is a mapping
\begin{equation}
    \phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)
\end{equation}
that determines how human $i$ attributes expectations to AI $j$ based on AI's design parameters $\theta_j$, observable signals $x_j$, and the human's anthropomorphism tendency $\omega_i$.
\end{defn}

\begin{defn}[Zero-Anthropomorphism Benchmark]
\label{def:zero-anthro}
For human $i \in N_H$ interacting with AI $j \in N_A$, the \textbf{zero-anthropomorphism attributed belief} is:
\begin{equation}
    \tilde{h}_i^{(2,j),ZA} = \phi_i(\theta_j, x_j, 0)
\end{equation}
This represents the attributed belief when anthropomorphism is absent ($\omega_i = 0$). The zero-anthropomorphism benchmark is descriptive: it characterizes attribution without anthropomorphic bias, not a normative standard for ``correct'' beliefs.
\end{defn}

The attribution function formalises the empirically established process of mind perception \citep{gray2007dimensions,epley2007seeing}. Its inputs correspond to well-established antecedents of intentionality attribution \citep{wiese2017robots}. Attribution intensity varies with human-like cues including voice \citep{schroeder2016voice}, gaze behaviour, and movement patterns, all of which can be captured in the signal vector $x_j$.

Three main approaches to specifying $\phi_i$ are:
\begin{enumerate}
    \item \textbf{Behavioural attribution}: $\phi_i^{beh}(\theta_j, x_j, \omega_i) = g(s_j^{obs}, \omega_i)$, depending on AI's observed behaviour
    \item \textbf{Signal-based attribution}: $\phi_i^{sig}(\theta_j, x_j, \omega_i) = g(x_j, \omega_i)$, depending primarily on observable signals
    \item \textbf{Dispositional attribution}: $\phi_i^{disp}(\theta_j, x_j, \omega_i) = \omega_i \cdot \bar{h} + (1 - \omega_i) \cdot \underline{h}$, depending primarily on the human's anthropomorphism tendency
\end{enumerate}

\begin{example}[Linear Attribution]
\label{ex:linear-attribution}
Linearity is tractable: first-order approximations yield closed-form equilibria and testable predictions. Mind perception research finds monotonic relationships between anthropomorphism and attributed mental states---linearity captures this without untested curvature. We adopt:
\begin{equation}
    \tilde{h}_i^{(2,j)}(C) = \omega_i \cdot \left( \rho_j + \eta \cdot x_j \right)
\end{equation}
The parameter $\omega_i \in [0,1]$ measures individual anthropomorphism tendency and multiplies the entire expression: when $\omega_i = 0$, no beliefs are attributed regardless of AI characteristics, representing the limiting case of purely mechanistic perception. Inside the bracket, $\rho_j \in [0,1]$ captures AI prosociality embedded in design, while $\eta \cdot x_j$ (with signal strength $x_j \in [0,1]$ and sensitivity $\eta > 0$) captures presentation effects from anthropomorphic cues. These terms are additive because they represent distinct attribution channels---what the AI is designed to do versus how human-like it appears. Section~\ref{sec:empirical-implementation} derives testable predictions; Section~\ref{sec:mind-perception} grounds this specification in established mind perception theory.
\end{example}

% Specification Selection Table (from Agent 3.2)
\begin{table}[htbp]
\centering
\caption{Attribution Specification Selection Guide}
\label{tab:specification-selection}
\small
\begin{tabular}{@{}p{2.2cm}p{2.5cm}p{3.2cm}p{3cm}p{2.8cm}@{}}
\toprule
Specification & Observable Trigger & When to Use & Example Context & Identification \\
\midrule
Behavioral & AI's observed strategy $s_j^{\text{obs}}$ & Repeated interactions where AI actions inform beliefs & Multi-round trust games, learning experiments & Within-subject variation across rounds \\[6pt]
Signal-based & Interface signals $x_j$ & One-shot games with interface manipulation & Voice assistant design, chatbot framing & Between-subject treatment assignment \\[6pt]
Dispositional & Individual tendency $\omega_i$ & Fixed AI with focus on individual heterogeneity & Survey studies, field experiments & IDAQ instrument or similar scale \\
\bottomrule
\end{tabular}
\end{table}

The three specifications represent complementary modeling choices, not competing theories. Each captures a distinct source of variation in attributed beliefs.

Signal-based attribution suits one-shot experiments where researchers manipulate AI interface features---voice characteristics, avatar appearance, or conversational style. Random assignment to interface conditions provides clean between-subject identification. This specification isolates design effects from learning.

Behavioral attribution applies when humans interact with AI repeatedly and update attributed beliefs based on observed actions. Trust games with multiple rounds exemplify this setting: a player who observes cooperative AI behavior in early rounds may attribute stronger cooperative intent in later rounds. Within-subject variation across rounds identifies the attribution response $\partial \tilde{h} / \partial s_j^{\text{obs}}$.

Dispositional attribution fits studies where AI behavior remains fixed but individual differences in anthropomorphization drive outcome variation. Survey instruments like the IDAQ measure this tendency directly. Field experiments with standardized AI deployments fall in this category.

A practical diagnostic distinguishes behavioral from signal-based attribution: estimate whether $\partial \tilde{h} / \partial s_j^{\text{obs}} \neq 0$ conditional on interface signals. A significant effect indicates behavioral attribution operates; a null effect suggests signal-based attribution suffices.

\begin{remark}[Linear Specification Focus]
The linear specification in Example~\ref{ex:linear-attribution} serves as our primary working model for three reasons. First, it nests signal-based attribution as a leading special case: setting $\beta = 0$ yields attributed beliefs determined entirely by interface signals. Second, linearity delivers tractable equilibrium characterization while preserving the core insight that attributed beliefs depend on observable AI features. Third, empirical research on mind perception consistently finds monotonic relationships between anthropomorphic cues and attributed mental states \citep{waytz2010causes,epley2007seeing}. We derive general existence results for the broad class, then specialize to the linear case for applications.
\end{remark}

\subsubsection{Empirical Implementation and Testability}
\label{sec:empirical-implementation}

The attribution function $\phi_i$ is identified through experimental variation in its arguments. We describe the measurement strategy, illustrate with a trust game design, and derive testable predictions.

\paragraph{Empirical Grounding.}

The linear specification in Example~\ref{ex:linear-attribution} has both theoretical and empirical support. Meta-analyses of human-AI interaction confirm monotonic relationships between anthropomorphic cues and trust: \citet{blut2021understanding} find that across 97 effect sizes, anthropomorphism's effect on trust is consistent with linearity within observed ranges. Since trust toward AI requires attributing beliefs about the AI's expectations and intentions, this evidence supports the linear attribution specification. The multiplicative structure---anthropomorphism tendency $\omega_i$ scaling signal effects---matches IDAQ validation studies showing that individual differences moderate responses to anthropomorphic design \citep{waytz2010who}.

Mind perception research provides further grounding. \citet{gray2007dimensions} establish that agency attribution---the dimension most relevant to strategic interaction---increases continuously with behavioral cues rather than exhibiting threshold effects. The SEEK framework \citep{epley2007seeing} predicts that both dispositional ($\omega_i$) and situational ($x_j$) factors contribute additively to anthropomorphism, consistent with our additive-inside-multiplicative structure. While nonlinear specifications (logistic saturation, uncanny valley discontinuities) may better fit extreme ranges, linearity captures first-order effects and enables closed-form identification. We maintain linearity for tractability; Section~\ref{sec:conclusion} discusses nonlinear extensions.

\paragraph{Identification Strategy.}

To identify $\phi_i$, we measure attributed beliefs $\tilde{h}_i^{(2,j)}$ while varying AI signals $x_j$ and measuring individual anthropomorphism $\omega_i$. Three components enable identification.

\paragraph{Belief Elicitation.}

We elicit attributed beliefs using incentive-compatible mechanisms. Quadratic scoring rules \citep{gachter2010peer} reward accuracy: subjects report their belief that the AI expects cooperation, and payment increases quadratically in the accuracy of that belief. To address hedging---subjects adjusting stated beliefs to insure against game outcomes---we follow \citet{blanco2010belief} in paying either the belief elicitation task or the strategic game, never both.

The strategy method enables post-hoc rationalization: when subjects state beliefs after observing outcomes, they may rationalize choices rather than report true beliefs \citep{danz2022belief}. Three design features mitigate this concern. First, \emph{belief-first elicitation}: subjects report attributed beliefs before making strategic choices, preventing outcome-driven rationalization. Second, \emph{monotonicity tests}: if subjects exhibit non-monotonic beliefs across signal levels (e.g., attributing lower beliefs to more anthropomorphic interfaces), this signals measurement error or demand effects. Third, \emph{separate payment randomization}: following \citet{blanco2010belief}, we determine payment-relevant decisions after all responses are collected, eliminating hedging incentives at each elicitation stage.

\emph{Strategy method.} Complete mapping of the attribution function requires observing $\tilde{h}_i^{(2,j)}$ across the full domain of $(x_j, \omega_i)$. The strategy method \citep{brandts2011strategy} elicits beliefs for each possible signal level within-subject, recovering the full $\phi_i$ mapping without relying on between-subject variation alone.

\emph{Anthropomorphism measurement.} Individual anthropomorphism $\omega_i$ is measured using the Individual Differences in Anthropomorphism Questionnaire (IDAQ; \citealp{waytz2010who}). The IDAQ contains 15 items measuring attribution of mental states to non-human entities (e.g., ``To what extent does a robot have intentions?''). We use the technology subscale, which correlates with trust in automated systems \citep{waytz2014mind}.

\paragraph{Application: Trust Game with Interface Manipulation.}

We illustrate identification with a trust game where an AI trustor sends $x \in [0, E]$ to a human trustee who returns $y \in [0, 3x]$. Between-subject manipulation varies interface anthropomorphism across three treatments: robotic ($x_A = 0$), humanoid ($x_A = 1$), and control ($x_A = 0.5$), holding AI behavior constant. Subjects complete the IDAQ, observe the AI interface, report attributed beliefs via incentivized elicitation, and choose return amounts. Random payment selection (belief or choice) eliminates hedging.

\paragraph{Parameter Recovery.}

Under the linear attribution specification from Example 1:
\begin{equation}
    \tilde{h}_i^{(2,A)} = \omega_i \cdot \left( \rho_A + \eta \cdot x_A \right)
\end{equation}
treatment variation identifies $\eta$; $\rho_A$ is assumed known from AI design. The reduced-form regression is:
\begin{equation}
    \tilde{h}_{it}^{(2,A)} = \beta_0 + \beta_1 \omega_i + \beta_2 x_{At} + \beta_3 \omega_i \cdot x_{At} + \varepsilon_{it}
    \label{eq:attribution-regression}
\end{equation}
where $t \in \{R, H, C\}$ indexes treatments.

The theoretical model generates testable restrictions:
\begin{enumerate}
    \item $\beta_2 = 0$: Signal has no direct effect on attributed beliefs; the effect operates through anthropomorphism
    \item $\beta_3 > 0$: Anthropomorphism moderates signal effect; high-$\omega_i$ subjects respond more to interface manipulation
\end{enumerate}

\paragraph{Separating Attribution from Psychological Preferences.}

The attribution function $\phi_i$ and psychological preference parameters ($\gamma_i$ for guilt sensitivity, $\beta_i$ for indignation sensitivity) enter utilities jointly, creating an identification problem: observed behavior reflects both how beliefs are attributed and how they affect payoffs. We propose a two-stage identification strategy.

\emph{Stage 1: Estimate psychological parameters from human-human interactions.} In interactions between human players, belief attribution is standard---$\tilde{h}_i^{(2,j)} = h_i^{(2,j)}$---and the attribution function is the identity. This baseline allows estimation of psychological preference parameters using established methods \citep{bellemare2023measuring}. For guilt aversion, we estimate $\gamma_i$ from trustee behavior in human-human trust games; for indignation, from punishment decisions after human defection. The key identifying assumption is that $\gamma_i$ and $\beta_i$ are stable across human and AI counterparts.

\emph{Stage 2: Identify $\phi_i$ from human-AI interactions.} With psychological parameters estimated from Stage 1, human-AI interactions isolate the attribution function. The difference between observed behavior toward AI and predicted behavior (using $\gamma_i$ with truthful beliefs) identifies the attribution wedge. Formally, let $a_i^*(h)$ denote the optimal action given second-order belief $h$. The observed action $a_i^{AI}$ toward an AI with attributed belief $\tilde{h}_i^{(2,A)}$ satisfies:
\begin{equation}
    a_i^{AI} = a_i^*\left(\phi_i(x_A, \omega_i, \rho_A)\right)
\end{equation}
Inverting the best-response correspondence recovers $\tilde{h}_i^{(2,A)}$ and hence the attribution function.

Alternative approaches include instrumental variables: exogenous variation in interface design (satisfying exclusion from psychological preferences) instruments for attributed beliefs. \citet{bellemare2023measuring} develop related techniques for identifying belief-dependent preferences.

\paragraph{Identification with Opaque AI Objectives.}

The parameter recovery strategy assumes $\rho_A$---the AI's true prosociality---is known from design. This assumption fails when AI objectives are proprietary or emergent from training. We consider three approaches.

First, \emph{partial identification}: without point identification of $\rho_A$, we bound the attribution function. If $\rho_A \in [\underline{\rho}, \overline{\rho}]$, the estimated $\phi_i$ lies in a corresponding interval. The bounds tighten as the anthropomorphism range expands: variation in $\omega_i$ traces out the attribution function's shape even when the intercept is unknown.

Second, \emph{sensitivity analysis}: we estimate $\phi_i$ under alternative assumptions about $\rho_A$ and report which conclusions are robust. If the interaction effect $\beta_3 > 0$ holds across plausible $\rho_A$ values, the core prediction survives.

Third, \emph{direct elicitation}: subjects state beliefs about AI objectives. While potentially biased, elicited $\rho_A$ beliefs provide an alternative reference point. Comparing elicited beliefs to design-implied values reveals systematic misperception of AI objectives, a distinct phenomenon from attribution.

The interaction coefficient $\beta_3$ directly estimates the signal sensitivity $\eta$. Given $\rho_A$ known from design, the baseline effect $\beta_1$ identifies anthropomorphism scaling.

The key testable prediction is the interaction effect $\beta_3 > 0$. Under the null hypothesis that attributed beliefs do not depend on anthropomorphism ($\omega_i$ irrelevant), $\beta_3 = 0$ and interface manipulation affects all subjects equally. A positive interaction indicates that anthropomorphic signals operate through the attribution mechanism specified in the model. Alternative specifications (logistic, threshold) accommodate saturation and uncanny valley effects; we maintain linearity for tractability.

\paragraph{Falsification Criteria.}

The model generates sharp falsifiable predictions. Consider three possible empirical patterns.

If $\beta_3 = 0$ (no interaction), anthropomorphism does not moderate signal interpretation. This falsifies the core mechanism: attributed beliefs would respond to interface manipulation identically for high- and low-$\omega_i$ subjects. The attribution function would reduce to $\phi_i(x_j) = f(x_j)$, independent of individual anthropomorphism tendency---inconsistent with mind perception theory and IDAQ validation evidence.

If $\beta_2 \neq 0$ with $\beta_3 = 0$, interface signals affect attributed beliefs directly without anthropomorphism mediation. This pattern suggests alternative mechanisms: perhaps anthropomorphic cues serve as quality signals or attention directors rather than triggering belief attribution.

Both null findings would require theoretical revision. The model does not predict that anthropomorphism always matters; rather, it predicts a specific functional relationship. Rejecting this relationship advances understanding even if it refutes the proposed mechanism.

\subsubsection{Psychological Foundations: Mind Perception Theory}
\label{sec:mind-perception}

The attribution function $\phi_i$ in Definition~\ref{def:attribution-function} formalizes how humans project mental states onto AI agents. This formalization rests on mind perception psychology.

The SEEK framework \citep{epley2007seeing} identifies three determinants of anthropomorphism: dispositional factors (sociality, effectance) corresponding to $\omega_i$, and situational triggers (elicited agent knowledge) corresponding to observable signals $x_j$. Mind perception decomposes into agency and experience \citep{gray2007dimensions}. AI scores high on agency (triggering attribution) but low on experience (driving attenuation). These dimensions operate independently, explaining their coexistence.

Anthropomorphism operates as System~1 cognition: automatic and resistant to conscious override. System~2 correction is effortful and incomplete \citep{epley2007seeing}---even users who know AI lacks mental states exhibit residual attribution. This dual-process structure grounds our functional form: the linear specification reflects SEEK's prediction that attribution increases monotonically in both dispositional tendency ($\omega_i$) and situational triggers ($x_j$).

\subsection{Psychological Payoffs}

Two main psychological mechanisms are relevant: indignation and guilt.

\begin{defn}[Indignation with Attenuation]
\label{def:indignation}
The indignation payoff captures disutility from disappointing others' expectations:
\begin{equation}
    \psi_i^{IND}(s, h_i^{(2)}, \tilde{h}_i^{(2)}) = -\beta_i \cdot \mathbf{1}_{s_i = D} \cdot \left[ \sum_{k \in N_H} h_i^{(2,k)}(C) + \lambda_i^{IND} \sum_{j \in N_A} \tilde{h}_i^{(2,j)}(C) \right]
\end{equation}
where $\beta_i > 0$ is indignation sensitivity, $h_i^{(2,k)}(C)$ is the probability that human $i$ believes human $k$ expected cooperation, $\tilde{h}_i^{(2,j)}(C)$ is the attributed probability that AI $j$ expected cooperation, $\lambda_i^{IND} \in [0,1]$ is the indignation attenuation factor toward AI, and $\mathbf{1}_{s_i = D}$ is the indicator for defection (adapted to context-specific actions in applications).
\end{defn}

The attenuation of indignation toward AI reflects intent attribution. Indignation requires perceiving malicious intent, but humans perceive AI behaviour as data-driven rather than prejudice-driven \citep{bigman2023people}. Related belief-dependent emotions---frustration and anger---arise in sequential games when players respond to perceived blame \citep{battigalli2019frustration}.

\begin{defn}[Guilt Aversion with Attenuation]
\label{def:guilt}
The guilt payoff captures disutility from falling short of perceived obligations:
\begin{equation}
    \psi_i^{GUILT}(s, h_i^{(2)}, \tilde{h}_i^{(2)}) = -\gamma_i \cdot \left[ \sum_{k \in N_H} \max\{0, h_i^{(2,k)} - \pi_k(s)\} + \lambda_i^{GUILT} \sum_{j \in N_A} \max\{0, \tilde{h}_i^{(2,j)} - \pi_j(s)\} \right]
\end{equation}
where $\gamma_i > 0$ is guilt sensitivity and $\lambda_i^{GUILT} \in [0,1]$ is the guilt attenuation factor toward AI.
\end{defn}

\citet{demelo2017people} find that participants feel ``considerably less guilt'' when exploiting machines than humans, suggesting guilt requires attribution of experience (capacity to suffer) that humans do not readily grant to AI. Cross-cultural evidence reveals heterogeneity: Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation \citep{karpus2025cross}.

\subsection{Welfare Measures}
\label{sec:welfare-measures}

Two welfare concepts arise naturally from the payoff decomposition. We define each, establish normative foundations, and characterize divergence.

\begin{defn}[Material Welfare]
\label{def:material-welfare}
For strategy profile $s \in S$, \textbf{material welfare} is the sum of material payoffs:
\begin{equation}
    W(s) = \sum_{i \in N} \pi_i(s).
\end{equation}
\end{defn}

Material welfare counts only outcomes that are transferable, verifiable, and comparable across agents---monetary payoffs, consumption, or quantities.

\begin{defn}[Extended Welfare]
\label{def:extended-welfare}
For strategy profile $s \in S$ with belief profile $(h, \tilde{h})$, \textbf{extended welfare} is:
\begin{equation}
    W^{ext}(s, h, \tilde{h}) = W(s) + \sum_{i \in N_H} \psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)}).
\end{equation}
\end{defn}

Extended welfare includes the psychological payoffs $\psi_i$ that humans experience. AI agents contribute no psychological terms since they lack subjective experience.

\subsubsection{Normative Justification}
\label{sec:welfare-normative}

The choice between welfare measures reflects a prior normative position on the status of psychological payoffs.

\citet{kahneman1997back} distinguish \emph{decision utility}---revealed preference over choices---from \emph{experienced utility}---hedonic quality of outcomes. Material welfare $W$ aligns with decision utility: it counts what agents choose to maximize, abstracting from the subjective states accompanying those choices. Extended welfare $W^{ext}$ aligns with experienced utility: it incorporates hedonic content, including guilt and disappointment.

When should each apply? The answer depends on whether psychological payoffs have intrinsic or instrumental value.

\paragraph{Instrumental view.} If guilt and disappointment matter only because they distort decisions, then $W$ is the appropriate measure. Psychological disutility is a means to material ends---it motivates behavior but has no standing in the social objective. Under this view, a planner seeks to maximize material outcomes, treating psychological costs as constraints that shape equilibrium but not as ends in themselves.

\paragraph{Intrinsic view.} If subjective experience has inherent value, then $W^{ext}$ applies. Guilt reduces human well-being directly, not merely through its behavioral consequences. A planner who values experienced happiness includes $\psi_i$ in the objective.

Standard welfare economics adopts the instrumental view: consumer surplus and Pareto efficiency concern material allocations. Yet behavioral welfare economics increasingly recognizes that experienced utility may diverge from decision utility \citep{kahneman1997back}. In human-AI interaction, this divergence takes a specific form: humans experience guilt toward agents that cannot reciprocate the experience.

\subsubsection{When Measures Diverge}
\label{sec:welfare-divergence}

With prosocial AI ($\rho_j > 0$), the two measures typically agree. The AI's objective incorporates human welfare, so attributed beliefs reflect genuine concern. Guilt from disappointing a prosocial AI corresponds to disappointing an entity that valued cooperation---the psychological cost has a material counterpart in the AI's design.

With materialist AI ($\rho_j = 0$), the measures can diverge. The AI maximizes its own material payoff and holds no expectations about human cooperation. Yet humans may attribute such expectations through anthropomorphism. The resulting guilt is real---humans experience the disutility---but corresponds to no genuine expectation.

\begin{example}[Phantom Expectations]
\label{ex:phantom-expectations}
Consider human $i$ interacting with materialist AI $j$. The AI's objective is $U_j^A = \pi_j(s)$, containing no term that depends on $i$'s cooperation. The AI holds no expectation about $i$'s behavior.

If $\omega_i > 0$, human $i$ attributes beliefs via $\tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i) > 0$. Human $i$ believes the AI expected cooperation. When $i$ defects, guilt follows:
\begin{equation}
    \psi_i^{GUILT} = -\gamma_i \lambda_i^{GUILT} \max\{0, \tilde{h}_i^{(2,j)} - \pi_j(s)\} < 0.
\end{equation}
This guilt is phantom: it responds to attributed expectations that exist only in the human's model of the AI. Material welfare $W$ ignores this cost. Extended welfare $W^{ext}$ counts it.
\end{example}

The divergence has policy implications. If a designer maximizes $W$, phantom guilt is irrelevant---only material outcomes matter. If the designer maximizes $W^{ext}$, phantom guilt is costly even when AI is indifferent. The optimal design of AI interfaces may differ across objectives.

\paragraph{Belief Revision.} Why don't humans eliminate phantom expectations by revising attributed beliefs? Three mechanisms prevent full revision: automaticity (attribution operates as System~1 cognition), limited correction capacity (System~2 override requires effort unavailable in interactive settings), and incomplete correction residue (even informed users exhibit residual anthropomorphism). Attributed beliefs reflect cognitive architecture, not irrationality.

\subsection{Game Definition}

\begin{defn}[Asymmetric Human-AI Psychological Game]
\label{def:game}
An asymmetric psychological game is a tuple
\begin{equation}
    \Gamma = (N_H, N_A, \{T_i\}_{i \in N_H}, \{\Theta_j\}_{j \in N_A}, \{S_i\}_{i \in N}, \{U_i^H\}_{i \in N_H}, \{U_j^A\}_{j \in N_A}, \phi, p)
\end{equation}
where $N_H, N_A$ are human and AI player sets, $T_i$ and $\Theta_j$ are type spaces, $S_i$ are strategy sets, $U_i^H$ and $U_j^A$ are utility functions, $\phi = \{\phi_i\}_{i \in N_H}$ are attribution functions, and $p$ is the common prior over types.
\end{defn}

\subsection{Assumptions}

We impose the following regularity conditions:

\begin{assumption}[Regularity]
\label{ass:regularity}
(A1) Strategy spaces $S_i$ are non-empty and finite. Type spaces $T_i$ and $\Theta_j$ are non-empty, convex, and compact. Payoff functions are continuous.
\end{assumption}

\begin{assumption}[Attribution Continuity]
\label{ass:attribution}
(A2) The attribution function $\phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)$ is continuous in $(\theta_j, x_j)$ for fixed $\omega_i$.
\end{assumption}

\begin{assumption}[Behavioral Attribution Regularity]
\label{ass:behavioral-attribution}
(A2-beh) When attribution depends on observed AI behavior, the function $\phi_i^{beh}: \Theta_j \times X \times \Delta(S_j) \times \Omega_i \to \Delta(S_i)$ satisfies:
\begin{enumerate}
    \item \textbf{Strategy continuity}: $\phi_i^{beh}$ is continuous in $\sigma_j \in \Delta(S_j)$ for fixed $(\theta_j, x_j, \omega_i)$.
    \item \textbf{Own-strategy independence}: $\phi_i^{beh}$ depends only on AI $j$'s strategy $\sigma_j$, not on human $i$'s strategy $\sigma_i$ or other players' strategies.
\end{enumerate}
\end{assumption}

\begin{remark}[Scope of Attribution Assumptions]
\label{rem:attribution-scope}
Assumption A2 applies to exogenous attribution (signal-based or dispositional), where attributed beliefs are constant in the strategy profile. Assumption A2-beh applies to behavioral attribution, where attributed beliefs depend on AI's observed behavior. The existence theorem (Theorem~\ref{thm:existence}) holds under either A2 alone (exogenous case) or A2 and A2-beh together (behavioral case). The applications in Section~\ref{sec:applications} use exogenous attribution.
\end{remark}

\begin{assumption}[Bounded Psychological Payoffs]
\label{ass:bounded}
(A3) There exists $M < \infty$ such that $|\psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)})| \leq M$ for all $i \in N_H$, all $s \in S$, and all beliefs.
\end{assumption}

\begin{remark}[Assumption A3 Redundancy]
Under A1--A2, boundedness of psychological payoffs follows from continuity on compact domains. We state A3 explicitly for clarity in the existence proof.
\end{remark}

\subsection{Assumption Summary}
\label{sec:assumption-summary}

Table~\ref{tab:assumptions} collects all assumptions. Core assumptions (A1)--(A3) support existence. Optional extensions and context-specific conditions apply to individual results.

\begin{table}[htbp]
\centering
\caption{Summary of Assumptions}
\label{tab:assumptions}
\small
\begin{tabular}{@{}llp{7.5cm}l@{}}
\toprule
\textbf{Label} & \textbf{Name} & \textbf{Statement} & \textbf{Required for} \\
\midrule
\multicolumn{4}{@{}l}{\textit{Core Assumptions}} \\[0.5ex]
(A1) & Regularity & Strategy spaces $S_i$ are finite; type spaces $T_i$, $\Theta_j$ are compact and convex; payoffs are continuous & All results \\[0.5ex]
(A2) & Attribution Continuity & $\phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)$ is continuous in $(\theta_j, x_j)$ & Exogenous attrib.\ results \\[0.5ex]
(A2-beh) & Behavioral Attrib.\ Regularity & $\phi_i^{beh}$ continuous in $\sigma_j$; depends only on $\sigma_j$ & Behavioral attrib.\ results \\[0.5ex]
(A3) & Bounded Psychological Payoffs & $|\psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)})| \leq M$ for some $M < \infty$ & All results \\[1ex]
\midrule
\multicolumn{4}{@{}l}{\textit{Optional Extensions to (A2)}} \\[0.5ex]
(A2$'$) & Attribution Monotonicity & $\omega_i' > \omega_i \Rightarrow \tilde{h}_i^{(2,j)}(\omega_i') \geq \tilde{h}_i^{(2,j)}(\omega_i)$ & Props.\ 5--10$'$, Cors.\ 2--3 \\[0.5ex]
(A2$''$) & Attribution Non-Degeneracy & $\phi_i$ is not constant in $\omega_i$ when $\bar{h}_H > \underline{h}$ & Prop.\ 9$'$ \\[0.5ex]
(A2$'''$) & Signal Monotonicity & $\partial \tilde{h}_i^{(2,j)} / \partial x_j \geq 0$ for $\omega_i > 0$ & Props.\ 10, 10$'$, Cors.\ 2--3 \\[1ex]
\midrule
\multicolumn{4}{@{}l}{\textit{Context-Specific Conditions}} \\[0.5ex]
(G) & Guilt Dominance & $G \equiv \gamma_H \lambda_H^{GUILT} > 1$ & Props.\ 5, 8 \\[0.5ex]
(G$'$) & Positive Guilt Sensitivity & $G \equiv \gamma_H \lambda_H^{GUILT} > 0$ & Props.\ 9$'$, 10, 10$'$ \\[0.5ex]
(I) & Indignation Dominance & $\beta_i[(n_H - 1) + \lambda_i^{IND} n_A] > E(1 - m/n)$ & Props.\ 6, 8, 10, 10$'$ \\[0.5ex]
(E) & Cooperation Efficiency & $m > 1$ & Props.\ 8, 9, 9$'$, 10, 10$'$ \\[0.5ex]
(C) & Signal Clarity & $x_A > 0.5$ & Prop.\ 7 \\[0.5ex]
(T) & Temptation Dominance & $E(1 - m/n) > \beta_i(n_H - 1)$ & Props.\ 10, 10$'$ \\
\bottomrule
\end{tabular}
\end{table}
