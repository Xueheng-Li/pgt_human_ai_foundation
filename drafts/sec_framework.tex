% Section 2: The Formal Framework
\section{The Formal Framework}
\label{sec:framework}

This section presents the formal structure of asymmetric psychological games with human and AI players. We introduce the primitives, belief hierarchies, and the attribution function that captures how humans project mental states onto AI agents.

\subsection{Players and Types}

The population consists of two disjoint player sets: a set $N_H$ of \textbf{human players} with $|N_H| = n_H \geq 1$, and a set $N_A$ of \textbf{AI agents} with $|N_A| = n_A \geq 0$. The total player set is $N = N_H \cup N_A$ with $|N| = n = n_H + n_A$. Define the human population share as $\alpha = n_H / n \in (0, 1]$.

Each player $i \in N$ has a finite strategy set $S_i$, with mixed strategies $\sigma_i \in \Delta(S_i)$. The strategy profile space is $S = \prod_{i \in N} S_i$.

Players are characterised by type parameters. For humans $i \in N_H$, the type $t_i = (\beta_i, \gamma_i, \omega_i, \ldots) \in T_i$ encodes psychological characteristics: indignation sensitivity $\beta_i$, guilt sensitivity $\gamma_i$, and anthropomorphism tendency $\omega_i \in [0,1]$. For AI $j \in N_A$, the design parameters $\theta_j \in \Theta_j$ encode programmed objectives.

\subsection{Payoffs}

Payoffs decompose into material and psychological components. For all players, $\pi_i(s)$ denotes the material payoff given strategy profile $s$. For humans, total utility is
\begin{equation}
    U_i^H(s, h_i, \tilde{h}_i) = \pi_i(s) + \psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)}),
\end{equation}
where $\psi_i$ is the psychological payoff depending on second-order beliefs (defined formally in Definitions \ref{def:indignation}--\ref{def:guilt} below). We write $U_i^H(s_i, s_{-i}, h_i, \tilde{h}_i)$ when emphasizing individual strategy choice. AI utility is design-dependent with no psychological component:
\begin{equation}
    U_j^A(s; \theta_j) = f_j(s; \theta_j).
\end{equation}

Common AI specifications include materialist ($U_j^A = \pi_j(s)$), prosocial ($U_j^A = \pi_j(s) + \rho_j \sum_{k \in N} \pi_k(s)$), and conditional objectives.

\subsection{Belief Hierarchies}

Following \citet{mertens1985formulation} and \citet{battigalli2009dynamic}, we construct belief hierarchies recursively. For human $i \in N_H$:
\begin{align}
    h_i^{(0)} &= t_i \in T_i \quad \text{(type)} \\
    h_i^{(1)} &\in \Delta(S_{-i}) \quad \text{(first-order beliefs about others' play)} \\
    h_i^{(2,k)} &\in \Delta(\Delta(S_{-k})) \quad \text{(second-order beliefs: what } k \text{ expects from others)}
\end{align}
We use $h_i^{(n)}$ instead of the $\beta_i^{(n)}$ notation from \citet{battigalli2009dynamic} to avoid confusion with the indignation sensitivity parameter $\beta$. We write $h_i^{(1,k)}$ for the marginal of $h_i^{(1)}$ on player $k$'s strategy. For action $a \in S_k$, we write $h_i^{(2,k)}(a)$ for the probability that player $i$ believes player $k$ expected action $a$. We write $h_i^{(2)} = \{h_i^{(2,k)}\}_{k \neq i}$ for the collection of second-order beliefs.

\subsection{Attributed Beliefs}

The key departure from standard psychological game theory is the distinction between genuine and attributed beliefs.

\begin{defn}[Attributed Beliefs]
\label{def:attributed-beliefs}
For human $i \in N_H$ interacting with AI $j \in N_A$, the \textbf{attributed second-order belief} is:
\begin{equation}
    \tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i)
\end{equation}
where $\theta_j \in \Theta_j$ is the AI's design parameters, $x_j \in X$ is observable signals (interface design, behavioural cues), and $\omega_i \in [0,1]$ is human $i$'s anthropomorphism tendency.
\end{defn}

The attributed belief $\tilde{h}_i^{(2,j)} \in \Delta(S_i)$ represents ``what human $i$ believes AI $j$ expected human $i$ to do.'' This is the belief that triggers guilt or indignation when human $i$ disappoints the AI's perceived expectations.

The key distinction: \textbf{genuine beliefs} $h_i^{(2,k)}$ about other humans $k \in N_H$ are formed through observation and Bayesian updating; \textbf{attributed beliefs} $\tilde{h}_i^{(2,j)}$ about AI $j \in N_A$ are formed through psychological projection via the attribution function.

\subsection{The Attribution Function}

The attribution function is the novel primitive of ABE theory, formalising how humans project mental states onto AI agents.

\begin{defn}[Attribution Function]
\label{def:attribution-function}
For each human $i \in N_H$, the attribution function is a mapping
\begin{equation}
    \phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)
\end{equation}
that determines how human $i$ attributes expectations to AI $j$ based on AI's design parameters $\theta_j$, observable signals $x_j$, and the human's anthropomorphism tendency $\omega_i$.
\end{defn}

\begin{defn}[Zero-Anthropomorphism Benchmark]
\label{def:zero-anthro}
For human $i \in N_H$ interacting with AI $j \in N_A$, the \textbf{zero-anthropomorphism attributed belief} is:
\begin{equation}
    \tilde{h}_i^{(2,j),ZA} = \phi_i(\theta_j, x_j, 0)
\end{equation}
This represents the attributed belief when anthropomorphism is absent ($\omega_i = 0$). The zero-anthropomorphism benchmark is descriptive: it characterizes attribution without anthropomorphic bias, not a normative standard for ``correct'' beliefs.
\end{defn}

\begin{remark}[Interpretation]
\label{rem:zero-anthro-interp}
The zero-anthropomorphism benchmark isolates the role of AI design parameters $\theta_j$ and observable signals $x_j$ in shaping attributed beliefs, net of individual anthropomorphism. Under the linear specification (Example~\ref{ex:linear-attribution}), $\tilde{h}_i^{(2,j),ZA} = 0$: without anthropomorphism, humans attribute no expectations to AI. Alternative specifications may yield $\tilde{h}_i^{(2,j),ZA} > 0$ if attribution persists through non-anthropomorphic channels.
\end{remark}

The attribution function formalises the empirically established process of mind perception \citep{gray2007dimensions,epley2007seeing}. Its inputs correspond to well-established antecedents of intentionality attribution \citep{wiese2017robots}. Attribution intensity varies with human-like cues including voice \citep{schroeder2016voice}, gaze behaviour, and movement patterns, all of which can be captured in the signal vector $x_j$.

Three main approaches to specifying $\phi_i$ are:
\begin{enumerate}
    \item \textbf{Behavioural attribution}: $\phi_i^{beh}(\theta_j, x_j, \omega_i) = g(s_j^{obs}, \omega_i)$, depending on AI's observed behaviour
    \item \textbf{Signal-based attribution}: $\phi_i^{sig}(\theta_j, x_j, \omega_i) = g(x_j, \omega_i)$, depending primarily on observable signals
    \item \textbf{Dispositional attribution}: $\phi_i^{disp}(\theta_j, x_j, \omega_i) = \omega_i \cdot \bar{h} + (1 - \omega_i) \cdot \underline{h}$, depending primarily on the human's anthropomorphism tendency
\end{enumerate}

\begin{example}[Linear Attribution]
\label{ex:linear-attribution}
Linearity is tractable: first-order approximations yield closed-form equilibria and testable predictions. Mind perception research finds monotonic relationships between anthropomorphism and attributed mental states---linearity captures this without untested curvature. We adopt:
\begin{equation}
    \tilde{h}_i^{(2,j)}(C) = \omega_i \cdot \left( \rho_j + \eta \cdot x_j \right)
\end{equation}
The parameter $\omega_i \in [0,1]$ measures individual anthropomorphism tendency and multiplies the entire expression: when $\omega_i = 0$, no beliefs are attributed regardless of AI characteristics, representing the limiting case of purely mechanistic perception. Inside the bracket, $\rho_j \in [0,1]$ captures AI prosociality embedded in design, while $\eta \cdot x_j$ (with signal strength $x_j \in [0,1]$ and sensitivity $\eta > 0$) captures presentation effects from anthropomorphic cues. These terms are additive because they represent distinct attribution channels---what the AI is designed to do versus how human-like it appears. Section~\ref{sec:empirical-implementation} derives testable predictions; Section~\ref{sec:mind-perception} grounds this specification in established mind perception theory.
\end{example}

\subsubsection{Empirical Implementation and Testability}
\label{sec:empirical-implementation}

The attribution function $\phi_i$ is identified through experimental variation in its arguments. We describe the measurement strategy, illustrate with a trust game design, and derive testable predictions.

\paragraph{Identification Strategy.}

To identify $\phi_i$, we measure attributed beliefs $\tilde{h}_i^{(2,j)}$ while varying AI signals $x_j$ and measuring individual anthropomorphism $\omega_i$. Three components enable identification.

\emph{Belief elicitation.} We elicit attributed beliefs using incentive-compatible mechanisms. Quadratic scoring rules \citep{gachter2010peer} reward accuracy: subjects report their belief that the AI expects cooperation, and payment increases quadratically in the accuracy of that belief. To address hedging---subjects adjusting stated beliefs to insure against game outcomes---we follow \citet{blanco2010belief} in paying either the belief elicitation task or the strategic game, never both. This eliminates the incentive to hedge beliefs against strategic payoffs.

\emph{Strategy method.} Complete mapping of the attribution function requires observing $\tilde{h}_i^{(2,j)}$ across the full domain of $(x_j, \omega_i)$. The strategy method \citep{brandts2011strategy} elicits beliefs for each possible signal level within-subject, recovering the full $\phi_i$ mapping without relying on between-subject variation alone. Subjects state their attributed belief conditional on each interface treatment before learning which treatment applies.

\emph{Anthropomorphism measurement.} Individual anthropomorphism $\omega_i$ is measured using the Individual Differences in Anthropomorphism Questionnaire (IDAQ; \citealp{waytz2010who}). The IDAQ contains 15 items measuring attribution of mental states to non-human entities (e.g., ``To what extent does a robot have intentions?''). We use the technology subscale, which correlates with trust in automated systems \citep{waytz2014trusting}.

\paragraph{Application: Trust Game with Interface Manipulation.}

We illustrate the identification strategy with a trust game where an AI trustor sends $x \in [0, E]$ to a human trustee, who returns $y \in [0, 3x]$. The human's attributed belief $\tilde{h}_H^{(2,A)}$ represents what the human believes the AI expected her to return.

\emph{Experimental design.} Between-subject manipulation varies interface anthropomorphism $x_A$:
\begin{itemize}
    \item Treatment R (Robotic): Minimalist interface displaying only numerical information; $x_A^R = 0$
    \item Treatment H (Humanoid): Avatar with name, voice feedback, and natural language; $x_A^H = 1$
    \item Treatment C (Control): Standard interface without anthropomorphic or de-anthropomorphic cues; $x_A^C = 0.5$
\end{itemize}

The AI's prosociality parameter $\rho_A$ and programmed behavior remain constant across treatments. Only the interface presentation varies.

\emph{Measurement sequence.} For each treatment:
\begin{enumerate}
    \item Subjects complete the IDAQ questionnaire (measures $\omega_i$)
    \item Subjects observe the AI interface and its transfer decision $x_A$
    \item Subjects report $\tilde{h}_i^{(2,A)}$: ``What return amount do you believe the AI expected you to send?'' (incentivized via quadratic scoring)
    \item Subjects choose return amount $y_i$
    \item One of tasks (3) or (4) is randomly selected for payment
\end{enumerate}

The within-subject strategy method extension elicits beliefs for all three interface conditions before random assignment, providing three observations per subject.

\paragraph{Parameter Recovery.}

Under the linear attribution specification from Example 1:
\begin{equation}
    \tilde{h}_i^{(2,A)} = \omega_i \cdot \left( \rho_A + \eta \cdot x_A \right)
\end{equation}
treatment variation identifies $\eta$; $\rho_A$ is assumed known from AI design. The reduced-form regression is:
\begin{equation}
    \tilde{h}_{it}^{(2,A)} = \beta_0 + \beta_1 \omega_i + \beta_2 x_{At} + \beta_3 \omega_i \cdot x_{At} + \varepsilon_{it}
    \label{eq:attribution-regression}
\end{equation}
where $t \in \{R, H, C\}$ indexes treatments.

The theoretical model generates testable restrictions:
\begin{enumerate}
    \item $\beta_2 = 0$: Signal has no direct effect on attributed beliefs; the effect operates through anthropomorphism
    \item $\beta_3 > 0$: Anthropomorphism moderates signal effect; high-$\omega_i$ subjects respond more to interface manipulation
\end{enumerate}

The interaction coefficient $\beta_3$ directly estimates the signal sensitivity $\eta$. Given $\rho_A$ known from design, the baseline effect $\beta_1$ identifies anthropomorphism scaling.

The key testable prediction is the interaction effect $\beta_3 > 0$. Under the null hypothesis that attributed beliefs do not depend on anthropomorphism ($\omega_i$ irrelevant), $\beta_3 = 0$ and interface manipulation affects all subjects equally. A positive interaction indicates that anthropomorphic signals operate through the attribution mechanism specified in the model.

\paragraph{Alternative Functional Forms.}

The linear specification in Example 1 serves as a tractable benchmark. Two departures merit consideration.

\emph{Saturation.} Attributed beliefs are bounded in $[0,1]$, but the linear form $\omega_i(\rho_A + \eta x_A)$ may exceed unity for high parameter values. A logistic specification:
\begin{equation}
    \tilde{h}_i^{(2,A)} = \frac{1}{1 + \exp(-\omega_i(\rho_A + \eta x_A - \mu))}
\end{equation}
bounds attributed beliefs while preserving the interaction structure. The shift parameter $\mu$ centers the logistic at the midpoint of the belief scale.

\emph{Thresholds.} The uncanny valley phenomenon \citep{mori1970uncanny,gray2012feeling} suggests non-monotonicity: extremely human-like AI may trigger discomfort that reduces attribution. A threshold model:
\begin{equation}
    \tilde{h}_i^{(2,A)} = \omega_i \cdot \left( \rho_A + \eta x_A \right) \cdot \mathbf{1}_{x_A < \bar{x}}
\end{equation}
captures attribution collapse above threshold $\bar{x}$.

These alternatives generate distinct predictions. Saturation predicts diminishing marginal effects of $x_A$ at high levels; thresholds predict a discrete drop in $\tilde{h}_i^{(2,A)}$ above $\bar{x}$. Both can be tested by including quadratic or spline terms in regression (\ref{eq:attribution-regression}). In our trust game design, Treatment H ($x_A = 1$) tests whether extreme anthropomorphism triggers uncanny valley effects.

For applications in this paper, we maintain the linear specification. Empirical evidence on anthropomorphism-trust relationships \citep{blut2021understanding} is consistent with linearity over the range of interface manipulations typical in economic experiments. The linear form also yields closed-form equilibria enabling comparative statics.

\subsubsection{Psychological Foundations: Mind Perception Theory}
\label{sec:mind-perception}

The attribution function $\phi_i$ in Definition~\ref{def:attribution-function} formalizes how humans project mental states onto AI agents. This formalization rests on mind perception psychology. We map this literature to model parameters.

The SEEK framework \citep{epley2007seeing} identifies three determinants of anthropomorphism: \emph{Sociality} (the drive for social connection), \emph{Effectance} (the motivation to understand and predict behavior), and \emph{Elicited agent knowledge} (accessibility of human-related concepts). Sociality and effectance are dispositional, corresponding to individual variation in $\omega_i$. Elicited agent knowledge is situational: humanlike features such as voice, name, or conversational style activate human schemas, increasing attribution weight. When AI uses natural language, anthropocentric knowledge becomes accessible, amplifying the effect of $x_j$ on $\phi_i$.

A second body of work decomposes mind perception into two orthogonal dimensions \citep{gray2007dimensions,waytz2010causes}. The \emph{Agency} dimension captures perceived capacity for planning, self-control, and intentional action; the \emph{Experience} dimension captures perceived capacity for feeling, suffering, and subjective states. AI agents score high on agency but low on experience. This asymmetry maps to our two mechanisms. Attribution ($\phi_i$) is triggered by perceived agency: when AI appears to deliberate and choose, humans infer beliefs and intentions. Attenuation ($\lambda_i^{GUILT}, \lambda_i^{IND}$) is driven by low perceived experience: knowing that AI cannot truly suffer reduces guilt sensitivity. The two dimensions operate independently, explaining why the same human can both attribute beliefs to AI and feel reduced guilt exploiting it.

The SEEK framework incorporates a dual-process interpretation. Anthropomorphism operates as a System~1 response: humans automatically use self-knowledge as a template for reasoning about any agent. System~2 corrects this inference when people have time, motivation, and explicit knowledge that the target differs from humans. Correction is effortful and incomplete \citep{epley2007seeing}. The attribution function captures this: $\omega_i$ reflects the strength of automatic anthropomorphism, while signals $x_j$ modulate the degree of correction. Under cognitive load, correction diminishes and $\phi_i$ increases. Even users who know AI lacks mental states exhibit residual attribution because System~2 correction is incomplete.

This psychological foundation grounds our functional form choices. The linear specification in Example~\ref{ex:linear-attribution} reflects the SEEK prediction that attribution increases monotonically in both dispositional tendency ($\omega_i$) and situational triggers ($x_j$). The two-dimensional structure of mind perception justifies treating attribution and attenuation as separate mechanisms with distinct antecedents. These foundations link the model to empirical predictions in Section~\ref{sec:empirical-implementation}: regression estimates of $\omega_i$ correspond to validated individual difference measures \citep{waytz2010who}, and experimental variation in $x_j$ manipulates the situational triggers that SEEK identifies as determinants of attribution.

\subsection{Psychological Payoffs}

Two main psychological mechanisms are relevant: indignation and guilt.

\begin{defn}[Indignation with Attenuation]
\label{def:indignation}
The indignation payoff captures disutility from disappointing others' expectations:
\begin{equation}
    \psi_i^{IND}(s, h_i^{(2)}, \tilde{h}_i^{(2)}) = -\beta_i \cdot \mathbf{1}_{s_i = D} \cdot \left[ \sum_{k \in N_H} h_i^{(2,k)}(C) + \lambda_i^{IND} \sum_{j \in N_A} \tilde{h}_i^{(2,j)}(C) \right]
\end{equation}
where $\beta_i > 0$ is indignation sensitivity, $h_i^{(2,k)}(C)$ is the probability that human $i$ believes human $k$ expected cooperation, $\tilde{h}_i^{(2,j)}(C)$ is the attributed probability that AI $j$ expected cooperation, $\lambda_i^{IND} \in [0,1]$ is the indignation attenuation factor toward AI, and $\mathbf{1}_{s_i = D}$ is the indicator for defection (adapted to context-specific actions in applications).
\end{defn}

The attenuation of indignation toward AI reflects intent attribution. Indignation requires perceiving malicious intent, but humans perceive AI behaviour as data-driven rather than prejudice-driven \citep{bigman2023people}.

\begin{defn}[Guilt Aversion with Attenuation]
\label{def:guilt}
The guilt payoff captures disutility from falling short of perceived obligations:
\begin{equation}
    \psi_i^{GUILT}(s, h_i^{(2)}, \tilde{h}_i^{(2)}) = -\gamma_i \cdot \left[ \sum_{k \in N_H} \max\{0, h_i^{(2,k)} - \pi_k(s)\} + \lambda_i^{GUILT} \sum_{j \in N_A} \max\{0, \tilde{h}_i^{(2,j)} - \pi_j(s)\} \right]
\end{equation}
where $\gamma_i > 0$ is guilt sensitivity and $\lambda_i^{GUILT} \in [0,1]$ is the guilt attenuation factor toward AI.
\end{defn}

\citet{demelo2017people} find that participants feel ``considerably less guilt'' when exploiting machines than humans, suggesting guilt requires attribution of experience (capacity to suffer) that humans do not readily grant to AI. Cross-cultural evidence reveals heterogeneity: Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation \citep{karpus2025cross}.

\subsection{Welfare Measures}
\label{sec:welfare-measures}

Two welfare concepts arise naturally from the payoff decomposition. We define each, establish normative foundations, and characterize divergence.

\begin{defn}[Material Welfare]
\label{def:material-welfare}
For strategy profile $s \in S$, \textbf{material welfare} is the sum of material payoffs:
\begin{equation}
    W(s) = \sum_{i \in N} \pi_i(s).
\end{equation}
\end{defn}

Material welfare counts only outcomes that are transferable, verifiable, and comparable across agents---monetary payoffs, consumption, or quantities.

\begin{defn}[Extended Welfare]
\label{def:extended-welfare}
For strategy profile $s \in S$ with belief profile $(h, \tilde{h})$, \textbf{extended welfare} is:
\begin{equation}
    W^{ext}(s, h, \tilde{h}) = W(s) + \sum_{i \in N_H} \psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)}).
\end{equation}
\end{defn}

Extended welfare includes the psychological payoffs $\psi_i$ that humans experience. AI agents contribute no psychological terms since they lack subjective experience.

\subsubsection{Normative Justification}
\label{sec:welfare-normative}

The choice between welfare measures reflects a prior normative position on the status of psychological payoffs.

\citet{kahneman1997back} distinguish \emph{decision utility}---revealed preference over choices---from \emph{experienced utility}---hedonic quality of outcomes. Material welfare $W$ aligns with decision utility: it counts what agents choose to maximize, abstracting from the subjective states accompanying those choices. Extended welfare $W^{ext}$ aligns with experienced utility: it incorporates hedonic content, including guilt and disappointment.

When should each apply? The answer depends on whether psychological payoffs have intrinsic or instrumental value.

\paragraph{Instrumental view.} If guilt and disappointment matter only because they distort decisions, then $W$ is the appropriate measure. Psychological disutility is a means to material ends---it motivates behavior but has no standing in the social objective. Under this view, a planner seeks to maximize material outcomes, treating psychological costs as constraints that shape equilibrium but not as ends in themselves.

\paragraph{Intrinsic view.} If subjective experience has inherent value, then $W^{ext}$ applies. Guilt reduces human well-being directly, not merely through its behavioral consequences. A planner who values experienced happiness includes $\psi_i$ in the objective.

Standard welfare economics adopts the instrumental view: consumer surplus and Pareto efficiency concern material allocations. Yet behavioral welfare economics increasingly recognizes that experienced utility may diverge from decision utility \citep{kahneman1997back}. In human-AI interaction, this divergence takes a specific form: humans experience guilt toward agents that cannot reciprocate the experience.

\subsubsection{When Measures Diverge}
\label{sec:welfare-divergence}

With prosocial AI ($\rho_j > 0$), the two measures typically agree. The AI's objective incorporates human welfare, so attributed beliefs reflect genuine concern. Guilt from disappointing a prosocial AI corresponds to disappointing an entity that valued cooperation---the psychological cost has a material counterpart in the AI's design.

With materialist AI ($\rho_j = 0$), the measures can diverge. The AI maximizes its own material payoff and holds no expectations about human cooperation. Yet humans may attribute such expectations through anthropomorphism. The resulting guilt is real---humans experience the disutility---but corresponds to no genuine expectation.

\begin{example}[Phantom Expectations]
\label{ex:phantom-expectations}
Consider human $i$ interacting with materialist AI $j$. The AI's objective is $U_j^A = \pi_j(s)$, containing no term that depends on $i$'s cooperation. The AI holds no expectation about $i$'s behavior.

If $\omega_i > 0$, human $i$ attributes beliefs via $\tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i) > 0$. Human $i$ believes the AI expected cooperation. When $i$ defects, guilt follows:
\begin{equation}
    \psi_i^{GUILT} = -\gamma_i \lambda_i^{GUILT} \max\{0, \tilde{h}_i^{(2,j)} - \pi_j(s)\} < 0.
\end{equation}
This guilt is phantom: it responds to attributed expectations that exist only in the human's model of the AI. Material welfare $W$ ignores this cost. Extended welfare $W^{ext}$ counts it.
\end{example}

The divergence has policy implications. If a designer maximizes $W$, phantom guilt is irrelevant---only material outcomes matter. If the designer maximizes $W^{ext}$, phantom guilt is costly even when AI is indifferent. The optimal design of AI interfaces may differ across objectives.

\subsubsection{The Belief Revision Puzzle}
\label{sec:belief-revision}

A natural question arises: if phantom expectations impose psychological costs, why don't humans revise their attributed beliefs to eliminate them? A rational agent who learns that AI lacks genuine expectations should update $\tilde{h}_i^{(2,j)} \to 0$, avoiding guilt entirely.

Three mechanisms prevent full revision.

\paragraph{Automaticity of attribution.} Humans do not consciously choose attributed beliefs. Perceiving AI as agent-like automatically activates anthropomorphic inferences---without reflection and without deliberation \citep{epley2007seeing}. The attribution function $\phi_i$ operates as System~1 cognition: fast, automatic, and resistant to conscious override.

\paragraph{Limited correction capacity.} Belief revision requires System~2 cognition: effortful, deliberate, and slow. In interactive settings, real-time decisions prevent sustained correction. Under cognitive load, correction diminishes. The game-theoretic context---where humans must choose strategies while processing beliefs---imposes precisely these demands.

\paragraph{Incomplete correction residue.} Even with time and motivation, correction is characteristically insufficient. Users who know AI lacks mental states still exhibit residual anthropomorphism \citep{epley2007seeing}. The egocentric anchor---using self-knowledge as template for reasoning about agents---cannot be fully suppressed. Some attributed belief persists.

Together, these mechanisms explain the persistence of phantom expectations. Attributed beliefs are inferred rather than chosen, and deliberate correction (System~2) cannot fully override automatic inference (System~1). The resulting welfare losses are not irrational in the sense of violating preference axioms; they reflect the architecture of human social cognition applied to novel agents.

\subsection{Game Definition}

\begin{defn}[Asymmetric Human-AI Psychological Game]
\label{def:game}
An asymmetric psychological game is a tuple
\begin{equation}
    \Gamma = (N_H, N_A, \{T_i\}_{i \in N_H}, \{\Theta_j\}_{j \in N_A}, \{S_i\}_{i \in N}, \{U_i^H\}_{i \in N_H}, \{U_j^A\}_{j \in N_A}, \phi, p)
\end{equation}
where $N_H, N_A$ are human and AI player sets, $T_i$ and $\Theta_j$ are type spaces, $S_i$ are strategy sets, $U_i^H$ and $U_j^A$ are utility functions, $\phi = \{\phi_i\}_{i \in N_H}$ are attribution functions, and $p$ is the common prior over types.
\end{defn}

\subsection{Assumptions}

We impose the following regularity conditions:

\begin{assumption}[Regularity]
\label{ass:regularity}
(A1) Strategy spaces $S_i$ are non-empty and finite. Type spaces $T_i$ and $\Theta_j$ are non-empty, convex, and compact. Payoff functions are continuous.
\end{assumption}

\begin{assumption}[Attribution Continuity]
\label{ass:attribution}
(A2) The attribution function $\phi_i: \Theta_j \times X \times \Omega_i \to \Delta(S_i)$ is continuous in $(\theta_j, x_j)$ for fixed $\omega_i$.
\end{assumption}

\begin{assumption}[Bounded Psychological Payoffs]
\label{ass:bounded}
(A3) There exists $M < \infty$ such that $|\psi_i(s, h_i^{(2)}, \tilde{h}_i^{(2)})| \leq M$ for all $i \in N_H$, all $s \in S$, and all beliefs.
\end{assumption}
