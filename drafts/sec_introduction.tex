% Section 1: Introduction
% Final revised draft applying all Priority 1 and Priority 2 fixes
% Generated: January 14, 2026

\section{Introduction}

Artificial intelligence transforms economic and social life \citep{kaplan2020scaling, maslej2025aiindex}. Humans increasingly interact with AI agents---as collaborators, counterparties, and competitors \citep{acemoglu2024simple, rahwan2019machine}. Two asymmetries characterize this interaction. First, \textit{attribution}: humans attribute beliefs, intentions, and expectations to AI agents that lack mental states \citep{nass2000machines, gray2007dimensions, epley2007seeing}. A meta-analysis of 97 effect sizes confirms that anthropomorphism increases trust and cooperation in human-AI contexts \citep{blut2021understanding}. Second, \textit{attenuation}: moral emotions toward AI are weaker than toward humans. Humans feel less guilt exploiting machines \citep{demelo2017people} and less outrage when algorithms discriminate \citep{bigman2023people}. This dual psychological asymmetry---attribution and attenuation---lies beyond existing psychological game theory \citep{battigalli2022belief}, which lacks tools to analyze how humans form beliefs about AI mental states.

Three behavioral regularities sharpen this asymmetry. Guilt from disappointing expectations \citep{charness2006promises,sugden_Robert_2000}, reciprocity from perceived intentions \citep{rabin_Incorporating_1993,dufwenberg2004theory}, indignation from violated trust \citep{li_Indignation_2026}---all require beliefs about beliefs that AI cannot hold. Meanwhile, attenuation varies culturally: Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation, exploiting robotic partners at twice the rate \citep{karpus2025cross}. Attenuation is also design-dependent: AI expressing emotional distress partially restores guilt.

We introduce Attributed Belief Equilibrium (ABE) to address this dual asymmetry. ABE extends psychological game theory \citep{geanakoplos1989psychological, battigalli2009dynamic} to games where humans experience belief-dependent payoffs while AI optimize programmed objectives. The central insight: humans attribute mental states to AI, forming beliefs about what AI ``expect'' or ``believe.'' These attributed beliefs trigger guilt from disappointing attributed expectations and indignation from perceived violations---mirroring genuine human-to-human guilt.

The attribution function captures how humans form these beliefs. Three inputs determine attributed expectations: AI design parameters (prosociality level), observable signals (interface, behavior), and individual anthropomorphism tendency. A chatbot designed to express concern (``I understand your frustration'') generates higher attributed expectations than one providing neutral information. Attenuation parameters scale emotional intensity toward AI, from no attenuation (emotions equal human-directed) to full attenuation (no emotion toward AI). Attributed beliefs need not correspond to any actual AI mental state---only to what the human projects.

When attributed beliefs exceed feasible returns, humans experience \textit{phantom expectations}: guilt from disappointing agents holding no such expectations. Unlike guilt toward humans, which transfers welfare, phantom guilt is pure loss benefiting no one.

This structure reflects mind perception theory \citep{gray2007dimensions}. Humans perceive AI as having agency---capacity to act and form intentions---but lacking experience---capacity to feel and suffer. Agency drives attribution; low perceived experience drives attenuation. The asymmetry between perceived agency and perceived experience explains why attribution and attenuation coexist.

\paragraph*{Results.} We establish five clusters of results.

First, \textit{existence}: under regularity conditions on strategy spaces, utilities, and attribution functions, ABE exists (Theorem~1). The proof extends fixed-point arguments to dual belief structures---genuine beliefs about humans, attributed beliefs about AI.

Second, \textit{nesting}: ABE reduces to standard frameworks as special cases. When all players are human, ABE coincides with psychological Nash equilibrium (Proposition~1). When psychological payoffs vanish, ABE strategies coincide with Nash equilibria (Proposition~2). When attribution is rational---humans correctly anticipate AI behavior given design parameters---ABE reduces to Bayes-Nash equilibrium with type uncertainty (Proposition~3). The framework introduces a zero-anthropomorphism benchmark and Rational Attribution Equilibrium (RAE) to distinguish descriptive and normative baselines.

Third, \textit{multiplicity}: different attribution functions sustain different equilibria in the same material game (Proposition~5). In a trust game with AI trustor and human trustee, anthropomorphism above the cooperation threshold elevates attributed expectations, increasing guilt and equilibrium returns. Below this threshold, attributed beliefs attenuate, reducing psychological pressure. This multiplicity is consistent with absent betrayal aversion toward computers documented in prior experimental work. Interface design serves as an equilibrium selection device operating through the attribution channel: the same game produces different outcomes depending on AI presentation. The testable prediction: returns to AI trustees should increase with anthropomorphism measures, holding AI behavior constant, and high-anthropomorphism individuals should respond more strongly to interface manipulation than low-anthropomorphism individuals.

Fourth, \textit{applications}: the framework generates testable predictions in canonical games (Propositions~6--8). In trust games, anthropomorphism determines equilibrium returns: the human trustee returns either the attributed expectation or the maximum feasible return, whichever is lower. Returns increase through the guilt channel, not material incentives. In public goods, increasing AI population share has dual effects operating through distinct channels. The material channel: diluted material returns reduce cooperation incentives. The psychological channel: elevated attributed expectations from AI partners increase indignation costs of defection. The net effect depends on indignation attenuation, which varies cross-culturally---Japanese participants exhibit attenuation factors approximately half those of Western participants. This predicts non-monotonic effects of AI population share, with direction depending on cultural context. In coordination games, AI serves as focal point through expectation conformity: humans experience psychological pressure to match attributed expectations, resolving equilibrium multiplicity. AI provides a constructed focal point substituting for cultural or historical coordination devices.

Fifth, \textit{welfare and design}: anthropomorphism has asymmetric welfare effects depending on AI objectives (Propositions~9--10). With prosocial AI, higher anthropomorphism weakly increases welfare by elevating cooperation-inducing expectations. Elevated anthropomorphism---attributed beliefs exceeding the zero-anthropomorphism benchmark---improves welfare further when it triggers a regime switch from defection to cooperation.

With materialist AI, anthropomorphism creates phantom expectations. When these exceed feasible returns, humans incur guilt from disappointing agents holding no such expectations---a pure welfare loss benefiting no one.

These welfare effects yield design principles (Propositions~11--12). Prosocial AI should use minimal anthropomorphic signaling sufficient to induce cooperation---a threshold-finding objective. Higher cooperation efficiency reduces the required threshold because cooperation becomes easier to sustain. Materialist AI should use mechanical presentation---any positive anthropomorphic signal creates phantom expectations that reduce welfare. Mixed objectives face a tradeoff: intermediate signaling balances cooperation benefits against guilt costs---a marginal-balancing objective distinct from threshold-finding.

Material and extended welfare measures agree when attributed expectations stay below feasibility but diverge when phantom expectations arise (Corollaries~2--3). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs to users. This divergence justifies transparency regulation.

\paragraph*{Related Literature.} This paper contributes to four literatures.

\textit{Psychological game theory.} \citet{geanakoplos1989psychological} introduced psychological games, where payoffs depend on beliefs about beliefs. \citet{battigalli2009dynamic} extended this to dynamic settings; \citet{battigalli2019incorporating} develop a comprehensive framework for incorporating belief-dependent motivation; \citet{battigalli2007guilt} model guilt from disappointing expectations. Unlike outcome-based models \citep{fehr1999theory,bolton2000erc}, belief-dependent motivations require second-order beliefs about what others expect. We depart from standard PGT by accommodating asymmetric player types: humans with belief-dependent preferences, AI with design-dependent objectives. The attribution function replaces belief consistency for human beliefs about AI. ABE nests standard frameworks (Propositions~1--3): Psychological Nash Equilibrium when all players are human, Nash equilibrium when psychological payoffs vanish, Bayes-Nash equilibrium when attribution is rational.

\textit{Reciprocity.} \citet{rabin1993incorporating} introduced kindness-based reciprocity; \citet{dufwenberg2004theory} extended it to sequential games; \citet{battigalli2019frustration} model frustration and anger in leader-follower settings. \citet{falk2006theory} combine reciprocity with distributional concerns. These models require both players to form genuine kindness beliefs. With AI counterparties, humans attribute kindness to agents that have none---attributed AI kindness triggers reciprocity regardless of AI intent.

\textit{AI and strategic behavior.} \citet{mei2024turing} find large language models behave similarly to humans in economic games but are more prosocial. \citet{schniter2020trust} compare trust games with human versus robot trustees: investment levels are similar, but guilt, gratitude, and anger are attenuated toward robots. This finding directly supports ABE's attenuation parameters. \citet{bryant2024mental} show mental state attributions to LLMs affect trust---validating the attribution function mechanism. These studies document empirical relevance; ABE provides equilibrium foundations.

\textit{Anthropomorphism and mind perception.} \citet{epley2007seeing}'s SEEK framework identifies three determinants of anthropomorphism---sociality motivation, effectance motivation, and elicited agent knowledge---mapping to individual tendency $\omega_i$ and observable signals $x_j$ in our model. \citet{gray2007dimensions} decompose mind perception into agency and experience; this distinction explains why attribution and attenuation coexist. Japanese participants exhibit guilt toward robots comparable to guilt toward humans; Western participants show attenuation, exploiting robotic partners at twice the rate \citep{karpus2025cross}.

\paragraph*{Outline.} Section~\ref{sec:framework} presents the formal framework: asymmetric psychological games, the attribution function, and attenuation parameters. Section~\ref{sec:equilibrium} defines ABE, establishes existence, proves nesting results, and demonstrates attribution-dependent multiplicity. Section~\ref{sec:applications} analyzes trust, public goods, and coordination games, generating testable predictions. Section~\ref{sec:welfare} examines welfare effects of anthropomorphism and derives optimal AI presentation strategies. Section~\ref{sec:conclusion} concludes.
