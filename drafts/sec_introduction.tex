% Section 1: Introduction

\section{Introduction}

Artificial intelligence transforms economic and social life \citep{kaplan2020scaling, maslej2025aiindex}. Humans increasingly interact with AI agents as collaborators, counterparties, and competitors \citep{acemoglu2024simple, rahwan2019machine}. These interactions require new game-theoretic tools \citep{jackson2025ai}. The challenge is a \textit{dual psychological asymmetry}: humans experience belief-dependent preferences---guilt \citep{charness2006promises,sugden_Robert_2000}, reciprocity \citep{rabin_Incorporating_1993,dufwenberg2004theory}, indignation \citep{li_Indignation_2026}---while AI have design-dependent objectives. Existing psychological game theory \citep{battigalli2022belief} assumes symmetric belief-dependent mental states and cannot accommodate this heterogeneity.

Two empirical regularities complicate matters. First, \textit{anthropomorphism}: humans systematically attribute mental states---beliefs, intentions, expectations---to non-human agents \citep{nass2000machines, gray2007dimensions}. This projection is driven by activated agent knowledge, effectance motivation, and sociality needs \citep{epley2007seeing}. A meta-analysis of 97 effect sizes shows anthropomorphism increases trust and cooperation in human-AI contexts \citep{blut2021understanding}. Second, \textit{attenuation}: moral emotions are weaker toward AI than toward humans. Humans experience less guilt when exploiting machines \citep{demelo2017people} and less moral outrage when algorithms discriminate \citep{bigman2023people}. Attenuation is not fixed: Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation \citep{karpus2025cross}. Design features conveying emotional capacity---expressions of distress---can partially restore guilt.

We introduce \textit{Attributed Belief Equilibrium} (ABE) to address this dual asymmetry. ABE extends psychological game theory \citep{geanakoplos1989psychological, battigalli2009dynamic} to games where humans experience psychological payoffs from beliefs about beliefs, while AI optimize programmed objectives. The key insight is that humans \textit{attribute} mental states to AI: they form beliefs about what AI ``expect'' or ``believe,'' and these attributed beliefs trigger psychological responses---guilt from disappointing, indignation from violated expectations---that operate in purely human interaction.\footnote{\citet{epley2007seeing} is the standard reference on anthropomorphism; \citet{waytz2014mind} documents mind perception toward robots and AI.}

The innovation is formalizing attribution and attenuation through functions that capture anthropomorphism and emotional response. When human $i$ interacts with AI $j$, attributed second-order beliefs are
\[
\tilde{h}_i^{(2,j)} = \phi_i(\theta_j, x_j, \omega_i),
\]
where $\theta_j$ denotes AI design parameters (prosociality level $\rho_A$), $x_j$ denotes observable signals (interface, behavioral cues), and $\omega_i$ captures the human's anthropomorphism tendency. Psychological payoffs incorporate attenuation parameters $\lambda^{\text{GUILT}}, \lambda^{\text{IND}} \in [0,1]$ that scale emotional intensity toward AI. These attributed beliefs and attenuated emotions enter human utility through standard psychological mechanisms but satisfy different consistency conditions than genuine beliefs: they need not correspond to any actual AI mental state, only to what the human projects given AI characteristics. The attribution function formalizes the empirically established process of mind perception \citep{gray2007dimensions}, with attribution intensity varying with human-like cues including voice \citep{schroeder2016voice}, gaze behavior, and movement patterns \citep{wiese2017robots}.

\subsection*{Results}

We establish five clusters of results.

First, \textit{existence}: under regularity conditions on strategy spaces, utilities, and attribution functions, ABE exists (Theorem~1). The proof extends fixed-point arguments to dual belief structures---genuine beliefs about humans, attributed beliefs about AI.

Second, \textit{nesting}: ABE reduces to standard frameworks as special cases. When all players are human ($N_A = \emptyset$), ABE coincides with psychological Nash equilibrium (Proposition~1). When psychological payoffs vanish ($\psi_i \equiv 0$), ABE strategies coincide with Nash equilibria (Proposition~2). When attribution is rational---humans correctly anticipate AI behavior given design parameters---ABE reduces to Bayes-Nash equilibrium with type uncertainty (Proposition~3). These reductions establish ABE as a proper generalization. A \textit{zero-anthropomorphism benchmark}---$\tilde{h}_i^{(2,j),\text{ZA}} = \phi_i(\theta_j, x_j, 0)$---identifies when humans correctly anticipate AI behavior: the baseline absent anthropomorphic bias. This is distinct from the Rational Attribution Equilibrium concept (Proposition~3), where attribution correctly projects equilibrium play.

Third, \textit{multiplicity}: different attribution functions sustain different equilibria in the same material game (Proposition~4). In a trust game with AI trustor and human trustee, anthropomorphism above the cooperation threshold elevates attributed expectations, increasing guilt and equilibrium returns. Below this threshold, attributed beliefs attenuate, reducing psychological pressure. This is consistent with absent betrayal aversion toward computers \citep{aimone2014neural}. Interface design and behavioral presentation serve as equilibrium selection devices through the attribution channel.

Fourth, \textit{applications}: the framework generates testable predictions in canonical games (Propositions~5--7). In trust games, anthropomorphism determines equilibrium returns: $y^* = \min\{\tilde{h}_H^{(2,A)}, 3x\}$. In public goods, increasing AI population share has dual effects: diluted material returns reduce cooperation; elevated attributed expectations increase it. The net effect depends on indignation attenuation $\lambda_i^{\text{IND}}$, which varies cross-culturally. In coordination games, AI serves as focal point through expectation conformity: humans experience psychological pressure to match attributed expectations, resolving equilibrium multiplicity.

Moral emotions are \textit{attenuated} toward AI---a regularity shaping all subsequent welfare implications. Humans experience less guilt exploiting machines than humans \citep{demelo2017people} and less moral outrage when algorithms discriminate \citep{bigman2023people}. Attenuation varies culturally---Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation \citep{karpus2025cross}. The magnitude is substantial: Western participants exploit robotic partners at roughly twice the rate of Japanese participants, implying guilt attenuation factors $\lambda^{\text{GUILT}}$ approximately half those in Japanese populations. Attenuation is also design-dependent: AI expressing emotional distress partially restores guilt. Attenuation cuts both ways: it protects against phantom expectations from materialist AI but limits cooperation gains from prosocial AI.

Fifth, \textit{welfare and design}: anthropomorphism has asymmetric welfare effects depending on AI objectives. With prosocial AI ($\rho_A > 0$), higher anthropomorphism weakly increases welfare: attributed expectations favor cooperation (Proposition~8). Elevated anthropomorphism---$\tilde{h}_i^{(2,j)} > \tilde{h}_i^{(2,j),\text{ZA}}$---improves welfare further when it induces a regime switch from defection to cooperation (Proposition~9). With materialist AI ($\rho_A = 0$), anthropomorphism creates \textit{phantom expectations}: humans attribute expectations to agents that neither expect nor care. When phantom expectations exceed feasible returns, humans incur guilt from disappointing agents that hold no such expectations---a pure welfare loss benefiting no one (Proposition~9).

These welfare effects yield design principles (Proposition~10). Prosocial AI should use minimal anthropomorphic signaling sufficient to induce cooperation: $x^* = x_{\text{crit}}$. Higher cooperation efficiency reduces required signaling ($\partial x^*/\partial m < 0$) because the cooperation threshold is lower---a threshold-finding objective. Materialist AI should use mechanical presentation ($x^* = 0$)---any positive signal creates phantom expectations reducing extended welfare. Mixed AI ($\rho_A \in (0,1)$) faces a tradeoff: $x^* \in (0, \bar{x})$ balances cooperation benefits against guilt costs, with comparative statics $\partial x^*/\partial m > 0$, $\partial x^*/\partial G < 0$, $\partial x^*/\partial \omega < 0$---a marginal-balancing objective distinct from threshold-finding. Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs to users---a case for transparency regulation.

\subsection*{Related Literature}

This paper contributes to four literatures.

\textit{Psychological game theory.} \citet{geanakoplos1989psychological} introduced games where payoffs depend on beliefs about beliefs, enabling analysis of emotions tied to expectation violation. \citet{battigalli2009dynamic} extended this to dynamic settings, providing tools for guilt aversion \citep{charness2006promises} and sequential reciprocity \citep{dufwenberg2004theory}. Standard belief hierarchies follow the recursive construction of \citet{mertens1985formulation}. We depart by accommodating asymmetric player types: humans with genuine belief hierarchies and belief-dependent preferences, AI with design-dependent objectives but no beliefs. The attribution function replaces belief consistency for human beliefs about AI. Unlike standard psychological games that require ``cognizable'' beliefs on both sides, attributed beliefs need not be cognizable since AI lacks genuine mental states. The framework nests standard theories as special cases: when all players are human, ABE reduces to Psychological Nash Equilibrium \citep{geanakoplos1989psychological}; when psychological payoffs vanish, ABE reduces to Nash equilibrium (Propositions~1--2). When attribution is rational, ABE reduces to Bayesian games \citep{harsanyi1967games} with type uncertainty (Proposition~3). The framework deviates non-trivially from existing theory when attribution is biased by anthropomorphism. ABE complements behavioral game theory by formalizing one systematic departure from standard equilibrium: anthropomorphic belief formation. Rather than modeling noisy best responses as in quantal response equilibrium, ABE models systematic belief biases arising from cognitive tendencies to attribute mental states to non-human agents.

\textit{AI and strategic behavior.} \citet{mei2024turing} find that large language models behave similarly to humans in economic games but are more prosocial. \citet{rahwan2019machine} argue for treating AI as social actors; \citet{horton2023large} shows language models can simulate human experimental responses. This literature documents the empirical relevance of human-AI interaction but lacks game-theoretic foundations. We provide equilibrium concepts that generate testable predictions about how AI design affects human behavior. The phantom expectations problem (Proposition~\ref{prop:over-anthro}) connects to AI alignment concerns: humans optimize on attributed expectations that may not reflect AI objectives, creating welfare losses when beliefs and design diverge. This misalignment---humans targeting projected rather than actual AI goals---parallels specification problems in AI safety, where systems optimize proxies rather than true objectives.

\textit{Anthropomorphism and mind perception.} \citet{epley2007seeing} identify determinants of anthropomorphism: elicited agent knowledge, effectance motivation, sociality motivation. \citet{nass2000machines} established the foundational ``computers as social actors'' paradigm, showing humans apply social rules---politeness, reciprocity, gender stereotypes---to computers even when they know they are interacting with machines. \citet{gray2007dimensions} decompose mind perception into two dimensions: agency (capacity to act) and experience (capacity to feel). AI is typically attributed high agency but low experience, explaining why attributed beliefs form (agency-based) but moral emotions are attenuated (experience-based). Attenuation is not fixed: \citet{karpus2025cross} find Japanese participants exhibit guilt toward robots comparable to humans, while Western participants show strong attenuation. Design features conveying emotional capacity can partially restore guilt. We formalize this through attenuation parameters $\lambda_i^{\text{GUILT}}, \lambda_i^{\text{IND}}$ varying across individuals and contexts. \citet{zlotowski2015anthropomorphism} document effects in human-robot interaction. \citet{waytz2014mind} show that anthropomorphized agents receive greater moral consideration, affecting judgments about harm and obligations. \citet{joo2024mind} demonstrate that perceiving human-like mental qualities in AI increases moral blame. We formalize these findings, showing how anthropomorphism---via the attribution function---shapes strategic outcomes.

\textit{Anthropomorphic design.} \citet{schroeder2016voice} show that voice increases mind attribution; \citet{wiese2017robots} document effects of gaze and movement; \citet{waytz2014trusting} demonstrate that naming and voice increase trust. These design effects operate through the attribution channel we formalize. Optimal presentation depends on alignment between AI objectives and attributed mental states (Proposition~10): prosocial AI benefits from anthropomorphic features that elevate cooperation; materialist AI should avoid anthropomorphic presentation that creates phantom expectations---psychological costs from disappointing agents that neither expect nor care. Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs and creating a case for transparency regulation. \citet{bigman2023people} find algorithmic discrimination triggers less moral outrage than human discrimination because algorithms are perceived as data-driven rather than prejudice-driven. \citet{xu2022punishment} examine punishment patterns toward AI norm violators. \citet{demelo2017people} demonstrate that participants feel less guilt exploiting machines than humans, even though they feel comparable envy. This asymmetry suggests guilt requires attribution of experience (capacity to suffer) that humans do not readily grant to AI. Our framework incorporates these findings through attenuation parameters in psychological payoff functions.

\textit{Evolutionary game theory in heterogeneous populations.} \citet{young1993evolution} and \citet{sandholm2010orders} analyze stochastic stability with heterogeneous learning rules. Cross-cultural variation in anthropomorphism \citep{karpus2025cross} suggests these traits may be culturally evolved: anthropomorphism is adaptive in human-only environments but creates vulnerability to phantom expectations when AI is materialist. In companion work \citep{li2026pgt}, we extend ABE to evolutionary settings, analyzing how cooperation norms adapt in mixed populations. The present paper provides static equilibrium foundations for that dynamic analysis.

\subsection*{Outline}

Section~\ref{sec:framework} presents the formal framework: asymmetric psychological games, the attribution function, and attenuation parameters. Section~\ref{sec:equilibrium} defines ABE, establishes existence, proves that ABE nests standard frameworks (psychological games, Nash equilibrium, Bayesian games), and demonstrates attribution-dependent multiplicity. Section~\ref{sec:applications} analyzes trust, public goods, and coordination games, generating testable predictions about betrayal aversion, cooperation patterns, and focal point provision. Section~\ref{sec:welfare} examines welfare effects of anthropomorphism and derives optimal AI presentation strategies for prosocial, materialist, and mixed AI designs. Section~\ref{sec:conclusion} concludes.
