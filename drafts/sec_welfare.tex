% Section 5: Welfare
\section{Welfare and Optimal AI Design}
\label{sec:welfare}

This section examines the welfare implications of ABE and derives principles for optimal AI design.

\subsection{Welfare Measures}

Section~\ref{sec:welfare-measures} introduced two welfare concepts: material welfare $W(s)$ (Definition~\ref{def:material-welfare}) counting only transferable payoffs, and extended welfare $W^{ext}$ (Definition~\ref{def:extended-welfare}) incorporating psychological payoffs. The choice between measures reflects whether psychological states have intrinsic or instrumental value. This section applies both measures to characterize the welfare effects of anthropomorphism and derive principles for AI design.

\subsection{Anthropomorphism and Welfare}

The welfare effects of anthropomorphism depend critically on AI design.

\begin{proposition}[Welfare Effects of Anthropomorphism]
\label{prop:welfare-anthro}
Consider the public goods game (Parts i, iii) or trust game (Parts i--iii). Suppose Assumptions (A1)--(A3) regularity and (A2') attribution monotonicity hold.
\begin{enumerate}
    \item[(i)] \textbf{Material welfare with prosocial AI.} Under material welfare $W(s^*)$ (Definition~\ref{def:material-welfare}): when AI is prosocially designed ($\rho_A > 0$), if (E) cooperation is efficient ($m > 1$) and (I) indignation dominance holds, then higher anthropomorphism $\omega$ weakly increases $W(s^*)$.

    \item[(ii)] \textbf{Extended welfare with materialist AI.} Under extended welfare $W^{ext}(s^*, h^*, \tilde{h}^*)$ (Definition~\ref{def:extended-welfare}): when AI is materialistically designed ($\rho_A = 0$), if (G) guilt dominance holds ($G = \gamma_H \lambda_H^{GUILT} > 1$), then higher anthropomorphism $\omega$ may reduce $W^{ext}$.

    \item[(iii)] \textbf{Material welfare with materialist AI.} Under material welfare $W(s^*)$: when AI is materialistically designed ($\rho_A = 0$), anthropomorphism $\omega$ has no effect on $W(s^*)$.
\end{enumerate}
\end{proposition}

\begin{proof}[Proof Sketch]
\textit{Part (i).} With prosocial AI, attributed expectations favor cooperation. Higher $\omega$ elevates attributed expectations via attribution monotonicity (A2'), increasing the psychological cost of defection. This induces more cooperation, which raises total material payoffs when $m > 1$. The improvement is weak: within pure defection or pure cooperation regions, marginal increases in $\omega$ do not alter behavior.

\textit{Part (ii).} With materialist AI, the AI holds no prosocial expectations ($\rho_A = 0$). If humans over-anthropomorphize, they attribute \emph{phantom expectations} that correspond to nothing in AI objectives. When phantom expectations exceed feasible returns, humans incur guilt---a psychological cost counted by $W^{ext}$ but not by $W$. This is an existence result: welfare loss requires both that phantom expectations exceed feasibility and that $G > 1$.

\textit{Part (iii).} With materialist AI, equilibrium actions depend only on material payoffs. The AI plays its best response to maximize $\pi_A(s)$; humans best-respond to AI actions. Although higher $\omega$ increases attributed expectations and may induce guilt, guilt affects only the psychological component $\psi_i$, which $W$ excludes. Material outcomes---contributions, returns, transfers---remain unchanged.
\end{proof}

\begin{remark}[Asymmetric Welfare Implications]
Parts (i) and (ii) reveal a fundamental asymmetry. With prosocial AI, attributed expectations align directionally with AI preferences---humans who anthropomorphize attribute expectations that reflect, though potentially overestimate, genuine AI concern. With materialist AI, any attributed expectation is phantom: it exists only in the human's psychological model. Part (iii) clarifies that this asymmetry matters only when psychological welfare is valued intrinsically. If the planner adopts the instrumental view (material welfare only), anthropomorphism toward materialist AI is welfare-neutral.
\end{remark}

\begin{example}[Phantom Expectations in the Trust Game]
\label{ex:phantom}
Consider the trust game with $E = 10$, $x = 10$, and guilt parameter $G = 1.5$. With materialist AI ($\rho_A = 0$) and attribution function $\phi_H(0, x, \omega_H) = 5\omega_H x$:

\begin{center}
\begin{tabular}{lcc}
\hline
& $\omega_H = 0.6$ & $\omega_H = 1.0$ \\
\hline
Attributed expectation & 30 & 50 \\
Equilibrium return & 30 & 30 \\
Material welfare & 30 & 30 \\
Human guilt & 0 & $-30$ \\
Extended welfare & 30 & 0 \\
\hline
\end{tabular}
\end{center}

At moderate anthropomorphism ($\omega_H = 0.6$), phantom expectations equal the maximum feasible return, so the human meets them and incurs no guilt. At high anthropomorphism ($\omega_H = 1.0$), phantom expectations exceed feasibility---the human returns everything possible but still fails to meet attributed expectations, incurring guilt of $-30$. Extended welfare drops from 30 to 0.
\end{example}

\subsection{Elevated Anthropomorphism}

Define the \textbf{zero-anthropomorphism benchmark} as the attributed belief a perfectly informed, non-anthropomorphising agent would form:
\[
    \tilde{h}_i^{(2,j),ZA} \equiv \phi_i(\theta_j, x_j, 0),
\]
where $\theta_j$ captures AI design parameters, $x_j$ denotes observable signals, and $\omega_i = 0$ indicates no psychological tendency to project human-like mental states.

Human $i$ exhibits \textbf{elevated anthropomorphism} toward AI $j$ when attributed beliefs exceed this benchmark:
\[
    \tilde{h}_i^{(2,j)} > \tilde{h}_i^{(2,j),ZA}.
\]
We use ``elevated anthropomorphism'' as a descriptive term for attribution exceeding the zero-anthropomorphism benchmark, without implying welfare harm. The prefix ``elevated'' refers to the direction of deviation, not its normative valence---Part 1 below demonstrates that elevated anthropomorphism can improve welfare.

\begin{proposition}[Welfare Effects of Elevated Anthropomorphism]
\label{prop:over-anthro}
Under Assumptions (A1)--(A3), (A2') attribution monotonicity, and (A2'') attribution non-degeneracy:
\begin{enumerate}
    \item When AI is prosocially designed and (E) cooperation is efficient, elevated anthropomorphism \emph{weakly} improves material welfare $W(s^*)$. The improvement is strict when elevated anthropomorphism induces a regime switch from defection to cooperation.
    \item When AI is materialistically designed, elevated anthropomorphism \emph{may} reduce extended welfare $W^{ext}$. This is an existence result: welfare loss requires that phantom expectations exceed feasible returns and that humans have positive guilt sensitivity.
\end{enumerate}
\end{proposition}

Part 1 is weak because elevated anthropomorphism affects welfare only through equilibrium selection. Within pure defection or pure cooperation regions, marginal increases in $\tilde{h} - \tilde{h}^{ZA}$ do not alter behaviour. Part 2 is an existence result: when attributed expectations remain below feasible returns, humans satisfy phantom expectations and incur no guilt.

\subsection{Asymmetric Welfare Implications}

The two parts of Proposition~\ref{prop:over-anthro} reveal a fundamental asymmetry.

With \emph{prosocial AI}, elevated anthropomorphism amplifies cooperation beyond what zero-anthropomorphism would induce. Attributed expectations, though exceeding the zero-anthropomorphism benchmark, align directionally with AI preferences. The AI genuinely values cooperative outcomes; humans who attribute expectations to it are not entirely wrong about the direction of AI preferences, even if they overestimate their magnitude. This directional alignment creates mutual gains: humans cooperate more, AI achieves its prosocial objective, and material welfare rises.

With \emph{materialist AI}, elevated anthropomorphism creates expectations where none exist. The AI has no prosocial preferences ($\rho_A = 0$), so any attributed expectations are \emph{phantom}---they exist in the human's psychological model but correspond to nothing in AI objectives. When these phantom expectations exceed feasible returns, humans incur guilt from failing to meet expectations that (a) the AI never held and (b) were impossible to satisfy. This guilt is pure welfare loss: it benefits no one.

The critical distinction is whether attributed expectations correspond to genuine AI preferences. When attributed expectations align with AI design (prosocial case), elevated anthropomorphism coordinates behaviour toward efficient outcomes. When attributed expectations are phantom (materialist case), elevated anthropomorphism generates deadweight psychological costs.

This asymmetry creates a potential alignment problem. If AI designers want to maximise human welfare, they must consider not just AI objectives but also how humans perceive those objectives. A prosocial AI with a mechanical interface may fail to elicit cooperation because humans do not attribute expectations to it. A materialist AI with an anthropomorphic interface may harm welfare by inducing guilt without providing offsetting benefits.

\subsection{Optimal AI Presentation}

Given these welfare effects, what is the optimal AI presentation strategy?

\begin{proposition}[Optimal AI Design]
\label{prop:optimal-design}
Consider the public goods game (Parts i, iii) or trust game (Part ii). Suppose (A1)--(A3) regularity, (A2') attribution monotonicity, and (A2''') signal monotonicity hold. Let $\omega \in (0,1)$ denote the representative human's anthropomorphism tendency.
\begin{enumerate}
    \item \textbf{Prosocial AI} ($\rho_A = 1$): Under (E) cooperation efficiency, (I) indignation dominance, and (T) temptation dominance, the optimal signal is $x^* = \max\{0, x_{crit}\}$, where $x_{crit}$ is the minimal signal to induce cooperation. Higher efficiency reduces the required signal: $\partial x^*/\partial m < 0$.

    \item \textbf{Materialist AI} ($\rho_A = 0$): Under (G') positive guilt sensitivity,\footnote{(G') requires $G > 0$; the stronger (G) requires $G > 1$. We use (G) in Propositions \ref{prop:welfare-anthro} and \ref{prop:trust}, where guilt must overcome material temptation. (G') suffices for Propositions \ref{prop:over-anthro}, \ref{prop:optimal-design}, and \ref{prop:optimal-design-extended}, where positive guilt sensitivity generates welfare effects.} $x^* = 0$. Any positive signal creates phantom expectations that reduce extended welfare.

    \item \textbf{Mixed AI} ($\rho_A \in (0,1)$): Under (E) and (G'), the optimal signal $x^* \in (0, \bar{x})$ balances cooperation benefits against guilt costs. At interior solutions: $\partial x^*/\partial m > 0$, $\partial x^*/\partial G < 0$, $\partial x^*/\partial \omega < 0$.
\end{enumerate}
\end{proposition}

The comparative statics in Parts (i) and (iii) move in opposite directions: $\partial x^*/\partial m < 0$ versus $\partial x^*/\partial m > 0$. This reflects different optimisation objectives. Part (i) finds the \emph{minimal} signal to cross the cooperation threshold---higher efficiency lowers the threshold, reducing the required signal. Part (iii) balances material and psychological welfare at the margin---higher efficiency increases the value of expanding cooperation, justifying a higher signal. As $\rho_A \to 1$ and guilt costs vanish, Part (iii) degenerates to Part (i).

The three parts nest naturally. As $\rho_A \to 1$, attributed expectations are met in cooperation equilibrium, guilt vanishes, and the solution approaches Part (i). As $\rho_A \to 0$, baseline expectations vanish, expectations become purely signal-driven, and the solution approaches Part (ii): $x^* \to 0$.

The proposition has implications for AI regulation. It characterises the \emph{social} optimum, where the planner internalises psychological costs. Private AI designers may have misaligned incentives: anthropomorphic presentation increases engagement and revenue, while psychological costs (guilt, disappointment) are externalised to users. When $\rho_A < 1$, private designers prefer higher $x$ than the social optimum, creating a case for regulation---disclosure requirements or anthropomorphism limits.

\subsection{Optimal Design Under Extended Welfare}
\label{sec:optimal-design-extended}

Proposition~\ref{prop:optimal-design} characterizes optimal AI design when the planner maximizes material welfare $W$. We now derive corresponding results for extended welfare $W^{ext}$, which values psychological experience intrinsically.

\begin{proposition}[Extended Welfare Optimal Design]
\label{prop:optimal-design-extended}
Consider the public goods game (Parts i, iii) or trust game (Part ii). Suppose (A1)--(A3) regularity, (A2') attribution monotonicity, and (A2''') signal monotonicity hold. Let $\omega \in (0,1)$ denote the representative human's anthropomorphism tendency.
\begin{enumerate}
    \item \textbf{Prosocial AI} ($\rho_A = 1$): Under (E) cooperation efficiency, (I) indignation dominance, and (T) temptation dominance, the optimal signal under extended welfare is $x^{*,ext} = \max\{0, x_{crit}^{ext}\}$, where $x_{crit}^{ext}$ is the minimal signal inducing cooperation without phantom guilt. When $\tilde{h}_i^{(2,j)} \leq \bar{\pi}_j$ (attributed expectations do not exceed feasible returns, where $\bar{\pi}_j \equiv \max_{s} \pi_j(s)$), $x^{*,ext} = x^*$. When $\tilde{h}_i^{(2,j)} > \bar{\pi}_j$, $x^{*,ext} < x^*$.

    \item \textbf{Materialist AI} ($\rho_A = 0$): Under (G') positive guilt sensitivity, $x^{*,ext} = 0$. This coincides with the material welfare optimum.

    \item \textbf{Mixed AI} ($\rho_A \in (0,1)$): Under (E) and (G'), the optimal signal $x^{*,ext} \in [0, x^*)$ satisfies $x^{*,ext} \leq x^*$. The inequality is strict when phantom expectations arise at the material-optimal signal. At interior solutions: $\partial x^{*,ext}/\partial \gamma < 0$, where $\gamma$ is guilt sensitivity.
\end{enumerate}
\end{proposition}

\begin{proof}[Proof Sketch]
Part (i): With prosocial AI, attributed expectations reflect genuine AI preferences. When these expectations do not exceed feasible returns, humans who cooperate satisfy them and incur no guilt. The extended welfare objective then reduces to material welfare, and the solutions coincide. When attributed expectations exceed feasible returns---even with cooperation---phantom guilt arises. The extended welfare planner trades off cooperation benefits against guilt costs, preferring a lower signal that limits attributed expectations.

Part (ii): With materialist AI, all attributed expectations are phantom. Any positive signal increases $\tilde{h}_i^{(2,j)}$ without corresponding AI preferences. When humans cannot satisfy phantom expectations, guilt reduces $W^{ext}$. Since $\partial W^{ext}/\partial x < 0$ for all $x > 0$, the optimum is $x^{*,ext} = 0$. This matches Proposition~\ref{prop:optimal-design}(ii) because phantom guilt costs enter both objectives negatively.

Part (iii): With mixed AI, attributed expectations partially reflect genuine preferences. The extended welfare planner weighs cooperation benefits against two costs: guilt from unmet phantom expectations, and reduced cooperation from insufficient signals. Since $W^{ext}$ penalizes guilt directly while $W$ does not, the extended welfare optimum features weakly lower signals: $x^{*,ext} \leq x^*$. At interior solutions, higher guilt sensitivity reduces the optimal signal because the marginal psychological cost of attribution rises.
\end{proof}

The key insight is that extended welfare optimization incorporates phantom guilt as a direct cost, not merely a behavioral constraint. This shifts optimal design toward less anthropomorphic presentation whenever phantom expectations arise.

\begin{corollary}[Agreement of Welfare Measures]
\label{cor:welfare-agreement}
The material-optimal and extended-optimal designs coincide ($x^* = x^{*,ext}$) if either:
\begin{enumerate}
    \item AI is prosocial ($\rho_A > 0$) and attributed expectations do not exceed feasible returns: $\tilde{h}_i^{(2,j)} \leq \bar{\pi}_j(s^*)$; or
    \item AI is materialist ($\rho_A = 0$) and both objectives yield $x^* = x^{*,ext} = 0$.
\end{enumerate}
\end{corollary}

\begin{proof}[Proof Sketch]
Condition (i): When attributed expectations are feasible, cooperating humans satisfy them exactly. The guilt term vanishes: $\psi_i^{GUILT} = 0$. Extended welfare reduces to material welfare plus non-negative psychological terms, and the first-order conditions coincide.

Condition (ii): With materialist AI, any positive signal creates pure phantom expectations. Both $W$ and $W^{ext}$ are maximized at $x = 0$---the former because no material benefit offsets behavioral distortions, the latter because phantom guilt is eliminated.
\end{proof}

\begin{corollary}[Divergence of Welfare Measures]
\label{cor:welfare-divergence}
The material-optimal and extended-optimal designs diverge ($x^* > x^{*,ext}$) when:
\begin{enumerate}
    \item AI is prosocial ($\rho_A > 0$) but elevated anthropomorphism creates phantom expectations exceeding feasibility: $\tilde{h}_i^{(2,j)}(x^*) > \bar{\pi}_j(s^*)$; or
    \item AI is mixed ($\rho_A \in (0,1)$) and the material-optimal signal induces phantom expectations with positive guilt sensitivity.
\end{enumerate}
In both cases, $W^{ext}$-optimization recommends lower anthropomorphic presentation than $W$-optimization.
\end{corollary}

\begin{proof}[Proof Sketch]
Condition (i): Even with prosocial AI, extreme anthropomorphism can generate expectations beyond what cooperation achieves. The material welfare planner ignores the resulting guilt; the extended welfare planner counts it. At $x^*$, the marginal benefit of reducing phantom guilt exceeds the marginal cost of reduced cooperation under $W^{ext}$, yielding $x^{*,ext} < x^*$.

Condition (ii): With mixed AI, baseline expectations are positive but below fully prosocial levels. Signal-driven attribution adds expectations that may become phantom. The material welfare planner sets $x^*$ where marginal cooperation benefits equal marginal behavioral costs. The extended welfare planner faces an additional marginal cost---phantom guilt---and optimizes at a lower signal.
\end{proof}

The divergence has regulatory implications. If regulators adopt $W^{ext}$ as the social objective---valuing human psychological well-being intrinsically---they recommend stricter limits on anthropomorphic AI presentation than designers maximizing $W$. The gap between $x^*$ and $x^{*,ext}$ measures the welfare cost of treating psychological states as purely instrumental.

\subsection{AI Transparency and Regulation}

The phantom expectations mechanism supports a case for AI transparency requirements. If AI objectives were transparent, humans could calibrate $\omega$ appropriately: maintaining positive anthropomorphism toward prosocial AI (preserving cooperation benefits) while setting $\omega = 0$ toward materialist AI (eliminating phantom expectations).

The policy calculus depends on the population of AI systems:
\begin{itemize}
    \item If most AI is prosocially designed, some elevated anthropomorphism may be welfare-enhancing.
    \item If most AI is materialistically designed, elevated anthropomorphism causes net harm, favouring transparency mandates.
    \item With a mixed population, optimal policy must balance cooperation gains against psychological costs.
\end{itemize}

AI designers may have misaligned incentives. Anthropomorphic presentation increases engagement and revenue, while psychological costs from phantom expectations are borne by users. This externality suggests that market outcomes may feature excessive anthropomorphic design of materialist AI, strengthening the case for transparency regulation.

\subsection{Attenuation as a Welfare Buffer}

For expositional clarity, we adopt a representative-agent framework in this subsection, writing $\lambda^{IND}$ and $\lambda^{GUILT}$ for the common attenuation factors when all humans share identical psychological parameters.

The attenuation factors $\lambda^{IND}$ and $\lambda^{GUILT}$ serve as a natural buffer against welfare losses from elevated anthropomorphism.

\begin{remark}[Welfare Role of Attenuation]
When moral emotions are attenuated toward AI ($\lambda < 1$), the psychological costs of disappointing AI are reduced. This protects humans from full exposure to phantom expectations, limiting welfare losses from elevated anthropomorphism.
\end{remark}

However, attenuation also limits the welfare gains from prosocial AI. If humans do not feel guilt toward AI, then even a genuinely prosocial AI cannot induce cooperation through psychological mechanisms. The optimal attenuation level thus depends on the population composition: high attenuation is protective when AI is predominantly materialist, but costly when AI is predominantly prosocial.

\subsection{Cross-Cultural Implications}

Cross-cultural evidence suggests systematic variation in anthropomorphism $\omega$ across populations \citep{karpus2025cross}. Japanese participants exhibit guilt toward robots comparable to guilt toward humans (low attenuation, high effective $\omega$), while Western participants show strong attenuation (low effective $\omega$).

This variation has asymmetric welfare implications. Populations with high baseline $\omega$ benefit more from prosocial AI---larger cooperation gains from attributed expectations---but are also more vulnerable to materialist AI---larger phantom expectation costs. The cultural trait that amplifies benefits also amplifies harms.

Optimal AI design may therefore be culture-dependent. In high-$\omega$ cultures, prosocial design is especially valuable (large cooperation benefits), but materialist AI with anthropomorphic presentation is especially harmful (severe phantom expectations). In low-$\omega$ cultures, anthropomorphic design has limited effects regardless of AI objectives.

This asymmetry has regulatory implications. Transparency policies that enable calibrated anthropomorphism are especially important for high-$\omega$ populations. Without transparency, these populations face the largest welfare losses from materialist AI that presents anthropomorphically. International coordination on AI transparency may be complicated by divergent cultural exposures to phantom expectation costs.

\subsection{Dynamic Considerations}

The static welfare analysis assumes fixed anthropomorphism $\omega$ and attenuation $\lambda$. In practice, these parameters may evolve with AI exposure. If repeated interaction with AI reveals that attributed expectations are systematically violated (because AI is materialist), humans may learn to anthropomorphise less, naturally adjusting $\omega$ downward.

\begin{remark}[Learning and Long-Run Welfare]
If anthropomorphism is endogenous to experience, then short-run welfare losses from elevated anthropomorphism may be self-correcting. However, if AI designers continuously update presentation to maintain anthropomorphism, this natural correction may be undermined.
\end{remark}

This dynamic creates a regulatory challenge. Static welfare analysis may underestimate losses if it ignores the ``arms race'' between AI presentation and human learning.
