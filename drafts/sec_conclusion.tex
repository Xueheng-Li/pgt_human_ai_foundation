% Section 6: Conclusion
\section{Conclusion}
\label{sec:conclusion}

This paper introduces Attributed Belief Equilibrium to analyze strategic interaction between humans and artificial agents. The framework addresses a fundamental asymmetry: humans have belief-dependent preferences---guilt, reciprocity, indignation---while AI have design-dependent objectives with no genuine mental states. Standard psychological game theory cannot accommodate this heterogeneity because it assumes symmetric belief-dependent preferences across all players. ABE resolves this tension through the attribution function, which captures how humans project mental states onto AI even when those states do not exist.

\subsection{Summary of Contributions}

ABE's central innovation is its dual consistency requirement. Genuine beliefs about other humans satisfy standard Bayesian consistency as in \citet{battigalli2009dynamic}: humans correctly anticipate other humans' equilibrium strategies and beliefs. Attributed beliefs about AI satisfy a different condition---attribution consistency---determined by the attribution function $\phi$ given AI design parameters, observable signals, and the human's anthropomorphism tendency. This asymmetric treatment captures the empirical reality that humans form beliefs about AI ``expectations'' even when AI lack genuine mental states.

Existence of ABE follows from a Kakutani fixed-point argument under mild regularity conditions. The proof extends the recursive approach of \citet{battigalli2009dynamic} to dual belief structures, constructing attributed beliefs via $\phi$ and genuine beliefs via Bayesian consistency, then applying best responses to obtain a fixed point. Three nesting results establish ABE as a generalization of existing theory. ABE reduces to Psychological Nash Equilibrium \citep{geanakoplos1989psychological}---which coincides with Sequential Psychological Equilibrium for static games---when no AI agents are present. ABE reduces to Nash Equilibrium when psychological payoffs vanish. Under rational attribution, ABE corresponds bijectively to Bayes-Nash equilibria of an equivalent Bayesian game with type uncertainty about AI objectives. Rational attribution is itself a fixed-point requirement: $\phi$ projects equilibrium play, and the strategy profile is an equilibrium given $\phi$. When types are common knowledge, this reduces further to Nash equilibrium of a game with type-dependent payoffs (Corollary~\ref{cor:complete-info}).

A distinctive feature of ABE is attribution-dependent multiplicity. Different attribution functions sustain different equilibria in the same material game. The mechanism is a feed-forward chain: $\phi \to \tilde{h}^{(2)} \to U^H \to s^*$. Attributed beliefs are pinned down by $\phi$, so changing $\phi$ directly changes utility and thereby equilibrium. This contrasts with standard PGT multiplicity, which arises from feedback between strategies and belief consistency. The trust game shows how anthropomorphic presentation doubles equilibrium returns by elevating attributed expectations and thereby guilt from defection. The same logic extends to public goods provision, where high anthropomorphism sustains cooperation despite defection being materially dominant; the psychological cost of defecting on attributed expectations exceeds the material gain. Coordination games reveal a third mechanism: AI resolves equilibrium multiplicity by serving as a constructed focal point, creating psychological pressure to conform to the AI-favored equilibrium. These applications establish that interface design, framing, and behavioral presentation affect equilibrium selection even with fixed material payoffs.

The welfare analysis reveals an asymmetry between prosocial and materialist AI design. We distinguish material welfare---the sum of payoffs---from extended welfare that includes psychological states. Under conditions (E), (I), and (G) on cooperation efficiency and psychological sensitivity, this asymmetry takes a sharp form. When AI is prosocially designed, elevated anthropomorphism weakly improves material welfare, strictly so when it triggers cooperation. Elevated attributed expectations induce cooperation, and those expectations align with genuine AI preferences. When AI is materialistically designed, elevated anthropomorphism reduces extended welfare through phantom expectations. Humans attribute expectations that AI never held, and when these phantom expectations exceed feasible returns, humans incur guilt---a deadweight loss. Attenuation factors serve as a natural buffer: while they limit cooperation gains from prosocial AI, they protect humans from phantom expectation costs when AI is materialist.

Proposition~\ref{prop:optimal-design} characterizes optimal AI presentation. Prosocial AI should signal expectations at the minimal level needed to induce cooperation; higher efficiency reduces the required signal. Materialist AI should minimize signaling to avoid phantom expectations. Mixed AI with partially prosocial objectives requires balancing cooperation benefits against guilt costs. The comparative statics exhibit opposite signs: prosocial AI benefits from reduced signaling as efficiency rises, while mixed AI requires increased signaling to offset higher guilt costs.

\subsection{Relation to Existing Theory}

ABE nests standard equilibrium concepts as special cases. Without AI agents, ABE2 and ABE4 hold vacuously, and ABE1 and ABE3 reduce exactly to the optimality and belief consistency conditions of psychological equilibrium. Without psychological payoffs, beliefs become strategically irrelevant, and ABE strategy profiles coincide with Nash equilibria of the material game. This nesting ensures that ABE agrees with established theory in the domains where that theory applies.

The connection to Bayesian games illuminates when ABE generates new predictions. Under rational attribution, ABE is equivalent to a Bayesian game with type uncertainty about AI design parameters. ABE departs from Bayesian predictions precisely when attribution is systematically biased. Elevated anthropomorphism elevates attributed expectations above zero-anthropomorphism benchmarks; under-anthropomorphism attenuates them. These biases create predictable deviations from Bayesian outcomes, with welfare implications that depend on AI objectives.

\subsection{Limitations and Open Questions}

We have presented multiple approaches to specifying the attribution function---behavioral, signal-based, and dispositional---but have not resolved which is empirically appropriate in different contexts. The attribution function is the key primitive that distinguishes ABE from standard game theory, yet its empirical content remains to be established. Experimental work measuring how attributed beliefs respond to AI design features would enable calibration of $\phi$ and sharpen predictions.

The framework admits extensions we have not pursued. The current analysis stops at second-order attributed beliefs---what human $i$ attributes to AI $j$ about $i$'s behavior. Higher-order attributed beliefs may matter when humans reason about what AI ``thinks'' humans expect AI to expect. We treat anthropomorphism $\omega$ and attenuation $\lambda$ as fixed parameters, but humans likely update these based on experience with AI. A dynamic extension would analyze how attribution evolves through repeated interaction. Short-run welfare losses from phantom expectations may be self-correcting as humans learn from experience, though this correction could be undermined if AI designers continuously update presentation. The framework assumes humans observe AI design parameters $\theta_j$, but when AI types are uncertain, attributed beliefs depend on inferences about objectives, complicating the equilibrium analysis.

\subsection{Empirical Agenda}
\label{sec:empirical-agenda}

ABE generates testable predictions that distinguish it from standard game theory. The central prediction is that anthropomorphism and interface signals interact: in regression (\ref{eq:attribution-regression}), the coefficient $\beta_3 > 0$ indicates that high-$\omega_i$ subjects respond more strongly to anthropomorphic cues than low-$\omega_i$ subjects. Standard theory predicts $\beta_3 = 0$---interface design affects behavior uniformly or not at all. Three comparative statics sharpen the prediction. First, trust game returns $y^*$ increase in attributed beliefs $\tilde{h}_H^{(2,A)}$ through guilt aversion rather than material incentives. Second, public goods contributions exhibit threshold effects: cooperation emerges when $\omega_i \geq \bar{\omega}_i$, with the threshold depending on attenuation factor $\lambda_i^{IND}$. Third, AI population share has non-monotonic effects on cooperation---adding AI can sustain or erode cooperation depending on whether the psychological channel (attributed expectations) dominates the material channel (diluted payoffs).

Section~\ref{sec:empirical-implementation} details the experimental design: interface manipulation across robotic, humanoid, and control conditions; belief elicitation via quadratic scoring rules with hedging elimination; IDAQ measurement of individual anthropomorphism \citep{waytz2010who}. Two design choices require attention. Identifying phantom expectations---attributed beliefs about AI ``expectations'' despite AI lacking mental states---requires treatments where AI objectives $\rho_A$ are transparent versus hidden, to compare guilt when expectations are genuine versus projected. Cross-cultural samples are essential: \citet{karpus2025cross} document that Japanese participants exhibit guilt toward robots comparable to humans while Western participants show strong attenuation, implying that $\lambda_i^{GUILT}$ varies systematically across populations.

Measurement challenges remain. Distinguishing attributed beliefs from post-hoc rationalization requires careful timing of belief elicitation relative to strategic choice. The IDAQ captures general anthropomorphism tendency, but transferability to specific AI agents and game contexts is untested. Welfare predictions require measuring psychological costs inaccessible to introspection. Learning dynamics---whether phantom expectations persist or correct with experience---require repeated interaction designs with extended time horizons. Addressing these challenges will require combining laboratory precision with field studies of AI adoption.

\subsection{Implications for AI Development}

Design choices affect equilibrium behavior, not just user experience. Anthropomorphic design features---voice, naming, emotional expression, gaze behavior---enter equilibrium through the attribution function. The same material payoffs support different equilibrium behavior depending solely on how anthropomorphic the AI appears. A prosocial AI signaling expectations through humanlike cues induces more cooperation than an equally prosocial AI with a mechanical interface.

Beyond this direct effect, optimal presentation depends on the match between AI objectives and human projections. Prosocial AI benefits from anthropomorphic presentation because elevated attributed expectations align with genuine AI preferences and induce cooperation. Materialist AI harms welfare through anthropomorphic presentation because it creates phantom expectations that impose psychological costs without offsetting benefits. The analysis identifies a core design failure: anthropomorphic presentation of materialist AI. This combination creates phantom expectations that reduce welfare without offsetting gains. If AI designers have incentives to over-anthropomorphize to increase engagement, and if this creates phantom expectations that reduce user welfare, disclosure requirements or anthropomorphism limits may be warranted.

The cultural heterogeneity documented in \citet{karpus2025cross}---with Japanese participants exhibiting guilt toward robots comparable to humans while Western participants show strong attenuation---implies that optimal design varies across populations. High-anthropomorphism populations benefit more from prosocial AI but are more vulnerable to phantom expectations from materialist AI. The cultural trait that amplifies benefits also amplifies harms, complicating international coordination on AI design standards.

\subsection{Future Directions}

In companion work, we extend ABE to evolutionary settings to analyze how cooperation norms emerge when populations of humans and AI interact over time. The approach combines ABE with stochastic stability analysis to characterize long-run equilibrium selection under mutation and learning. Team collaboration, market competition, and AI-mediated negotiation all involve the asymmetric belief structures that ABE addresses. In teams, complementary expertise creates coordination gains that depend on attributed beliefs about AI contributions. In markets, attributed beliefs about AI strategies affect entry and pricing decisions. In negotiations, AI can serve as commitment devices whose effectiveness depends on whether humans attribute genuine expectations. Each setting extends the framework to new domains where the growing presence of artificial agents reshapes strategic interaction.
