% Section 6: Conclusion
\section{Conclusion}
\label{sec:conclusion}

This paper introduces Attributed Belief Equilibrium to analyze strategic interaction between humans and artificial agents. The framework addresses a fundamental asymmetry: humans have belief-dependent preferences---guilt, reciprocity, indignation---while AI have design-dependent objectives with no genuine mental states. Standard psychological game theory cannot accommodate this heterogeneity because it assumes symmetric belief-dependent preferences across all players. ABE resolves this tension through the attribution function, which captures how humans project mental states onto AI even when those states do not exist.

\paragraph*{Summary of Contributions.} ABE addresses the fundamental asymmetry between human belief-dependent preferences and AI design-dependent objectives through the attribution function $\phi$, which captures how humans project mental states onto AI. Three nesting results establish ABE as a generalization: reduction to Psychological Nash Equilibrium without AI agents, to Nash Equilibrium without psychological payoffs, and to Bayes-Nash equilibria under rational attribution.

A distinctive feature is attribution-dependent multiplicity: different attribution functions sustain different equilibria in the same material game through a feed-forward chain ($\phi \to \tilde{h}^{(2)} \to U^H \to s^*$). Applications demonstrate that interface design affects equilibrium selection with fixed material payoffs---the trust game shows anthropomorphic presentation doubles equilibrium returns through elevated attributed expectations.

The welfare analysis reveals an asymmetry: elevated anthropomorphism weakly improves welfare with prosocial AI (by inducing cooperation that aligns with AI preferences) but reduces extended welfare with materialist AI through phantom expectations---attributed beliefs about expectations AI never held. Proposition~\ref{prop:optimal-design} characterizes optimal presentation: prosocial AI should signal at the minimal cooperation-inducing level; materialist AI should minimize signaling.

\paragraph*{Relation to Existing Theory.} ABE nests standard equilibrium concepts as special cases. Without AI agents, ABE2 and ABE4 hold vacuously, and ABE1 and ABE3 reduce exactly to the optimality and belief consistency conditions of psychological equilibrium. Without psychological payoffs, beliefs become strategically irrelevant, and ABE strategy profiles coincide with Nash equilibria of the material game. This nesting ensures that ABE agrees with established theory in the domains where that theory applies.

The connection to Bayesian games illuminates when ABE generates new predictions. Under rational attribution, ABE is equivalent to a Bayesian game with type uncertainty about AI design parameters. ABE departs from Bayesian predictions precisely when attribution is systematically biased. Elevated anthropomorphism elevates attributed expectations above zero-anthropomorphism benchmarks; under-anthropomorphism attenuates them. These biases create predictable deviations from Bayesian outcomes, with welfare implications that depend on AI objectives.

\paragraph*{Limitations and Open Questions.} We have presented multiple approaches to specifying the attribution function---behavioral, signal-based, and dispositional---but have not resolved which is empirically appropriate in different contexts. The attribution function is the key primitive that distinguishes ABE from standard game theory, yet its empirical content remains to be established. Experimental work measuring how attributed beliefs respond to AI design features would enable calibration of $\phi$ and sharpen predictions.

The framework admits extensions we have not pursued. The current analysis stops at second-order attributed beliefs---what human $i$ attributes to AI $j$ about $i$'s behavior. Higher-order attributed beliefs may matter when humans reason about what AI ``thinks'' humans expect AI to expect. We treat anthropomorphism $\omega$ and attenuation $\lambda$ as fixed parameters, but humans likely update these based on experience with AI. A dynamic extension would analyze how attribution evolves through repeated interaction. The framework assumes humans observe AI design parameters $\theta_j$, but when AI types are uncertain, attributed beliefs depend on inferences about objectives, complicating the equilibrium analysis.

\paragraph*{Empirical Agenda.} ABE predicts that anthropomorphism and interface signals interact: high-$\omega_i$ subjects respond more strongly to anthropomorphic cues ($\beta_3 > 0$ in regression~\ref{eq:attribution-regression}), while standard theory predicts uniform effects ($\beta_3 = 0$). Section~\ref{sec:empirical-implementation} details the experimental design.

Key challenges include distinguishing attributed beliefs from rationalization, measuring psychological costs, and testing whether phantom expectations persist with experience. Cross-cultural samples are essential: \citet{karpus2025cross} document systematic variation in attenuation across populations.

\paragraph*{Implications for AI Development.} Design choices affect equilibrium behavior through the attribution function. The analysis identifies a core design failure: anthropomorphic presentation of materialist AI, which creates welfare-reducing phantom expectations. If designers over-anthropomorphize to increase engagement while users bear psychological costs, disclosure requirements or anthropomorphism limits may be warranted.

Cross-cultural variation \citep{karpus2025cross} implies that optimal design varies across populations. High-anthropomorphism populations benefit more from prosocial AI but are more vulnerable to materialist AI, complicating international coordination on AI standards.

\paragraph*{Future Directions.} Several extensions merit exploration. Evolutionary analysis can characterize long-run equilibrium selection when populations of humans and AI interact over time. The optimal presentation problem connects to mechanism design: choosing signals to implement outcomes subject to incentive constraints from attributed beliefs. Applications to team collaboration, market competition, and AI-mediated negotiation each involve asymmetric belief structures where attributed expectations shape strategic interaction.
