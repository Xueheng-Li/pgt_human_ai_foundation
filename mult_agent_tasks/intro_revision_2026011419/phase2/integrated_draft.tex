% Section 1: Introduction
% Integrated draft combining Opening, Results, Literature, and Outline sections
% Generated: January 14, 2026

\section{Introduction}

Artificial intelligence transforms economic and social life \citep{kaplan2020scaling, maslej2025aiindex}. Humans increasingly interact with AI agents---as collaborators, counterparties, and competitors \citep{acemoglu2024simple, rahwan2019machine}---creating a dual psychological asymmetry. Humans experience belief-dependent emotions: guilt from disappointing expectations \citep{charness2006promises,sugden_Robert_2000}, reciprocity from perceived intentions \citep{rabin_Incorporating_1993,dufwenberg2004theory}, indignation from violated trust \citep{li_Indignation_2026}. AI optimize programmed objectives without mental states. Existing psychological game theory \citep{battigalli2022belief} assumes symmetric belief-dependent agents and cannot accommodate this heterogeneity.

Two empirical regularities complicate matters. First, anthropomorphism: humans attribute beliefs, intentions, and expectations to non-human agents \citep{nass2000machines, gray2007dimensions, epley2007seeing}. A meta-analysis of 97 effect sizes shows anthropomorphism increases trust and cooperation in human-AI contexts \citep{blut2021understanding}. Second, attenuation: moral emotions are weaker toward AI than toward humans. Humans feel less guilt exploiting machines \citep{demelo2017people} and less moral outrage when algorithms discriminate \citep{bigman2023people}. Attenuation varies culturally---Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation \citep{karpus2025cross}. The magnitude is substantial: Western participants exploit robotic partners at roughly twice the rate of Japanese participants. Attenuation is also design-dependent: AI expressing emotional distress partially restores guilt.

We introduce Attributed Belief Equilibrium (ABE) to address this dual asymmetry. ABE extends psychological game theory \citep{geanakoplos1989psychological, battigalli2009dynamic} to games where humans experience psychological payoffs from beliefs about beliefs, while AI optimize programmed objectives. The central insight is that humans attribute mental states to AI: they form beliefs about what AI ``expect'' or ``believe,'' and these attributed beliefs trigger psychological responses---guilt from disappointing attributed expectations, indignation from perceived violations---that parallel purely human interaction.

The attribution function captures how humans form beliefs about AI expectations based on three inputs: AI design parameters such as prosociality level, observable signals including interface and behavioral cues, and the human's anthropomorphism tendency. Attenuation parameters scale emotional intensity toward AI, ranging from no attenuation---emotions equal to human-directed---to full attenuation---no emotion toward AI. These attributed beliefs and attenuated emotions enter human utility through standard psychological mechanisms but satisfy different consistency conditions than genuine beliefs: they need not correspond to any actual AI mental state, only to what the human projects given AI characteristics.

This structure reflects mind perception theory \citep{gray2007dimensions}. Humans perceive AI as having agency---capacity to act and form intentions---but lacking experience---capacity to feel and suffer. Agency drives attribution: humans form beliefs about what AI expects. Low perceived experience drives attenuation: moral emotions weaken when the target lacks capacity to suffer. The asymmetry between perceived agency and perceived experience explains why attribution and attenuation coexist.

\subsection*{Results}

We establish five clusters of results.

First, \textit{existence}: under regularity conditions on strategy spaces, utilities, and attribution functions, ABE exists (Theorem~1). The proof extends fixed-point arguments to dual belief structures---genuine beliefs about humans, attributed beliefs about AI. This existence result establishes ABE as a tractable framework for analyzing human-AI interaction under psychological heterogeneity.

Second, \textit{nesting}: ABE reduces to standard frameworks as special cases. When all players are human, ABE coincides with psychological Nash equilibrium (Proposition~1). When psychological payoffs vanish, ABE strategies coincide with Nash equilibria (Proposition~2). When attribution is rational---humans correctly anticipate AI behavior given design parameters---ABE reduces to Bayes-Nash equilibrium with type uncertainty (Proposition~3). These reductions establish ABE as a proper generalization of existing theory. The framework introduces two distinct benchmarks: the \textit{zero-anthropomorphism benchmark} identifies what humans would attribute absent anthropomorphic bias---a descriptive baseline. The \textit{Rational Attribution Equilibrium} (RAE) refines this by requiring attributed beliefs to equal equilibrium strategies---an equilibrium concept. These are distinct: zero-anthropomorphism describes attribution at a parameter value; rational attribution requires a fixed-point condition.

Third, \textit{multiplicity}: different attribution functions sustain different equilibria in the same material game (Proposition~4). In a trust game with AI trustor and human trustee, anthropomorphism above the cooperation threshold elevates attributed expectations, increasing guilt and equilibrium returns. Below this threshold, attributed beliefs attenuate, reducing psychological pressure. This multiplicity is consistent with absent betrayal aversion toward computers documented in prior experimental work. Interface design and behavioral presentation serve as equilibrium selection devices operating through the attribution channel: the same underlying game produces different outcomes depending on how AI is presented. The testable prediction is an interaction effect---high-anthropomorphism individuals respond more strongly to interface manipulation than low-anthropomorphism individuals.

Fourth, \textit{applications}: the framework generates testable predictions in canonical games (Propositions~5--7). In trust games, anthropomorphism determines equilibrium returns: the human trustee returns either the attributed expectation or the maximum feasible return, whichever is lower. Returns increase through the guilt channel, not material incentives. In public goods, increasing AI population share has dual effects operating through distinct channels. The material channel: diluted material returns reduce cooperation incentives. The psychological channel: elevated attributed expectations from AI partners increase indignation costs of defection. The net effect depends on indignation attenuation, which varies cross-culturally---Japanese participants exhibit attenuation factors approximately half those of Western participants. This predicts non-monotonic effects of AI population share, with direction depending on cultural context. In coordination games, AI serves as focal point through expectation conformity: humans experience psychological pressure to match attributed expectations, resolving equilibrium multiplicity. AI provides a constructed focal point that can substitute for cultural or historical coordination devices.

Fifth, \textit{welfare and design}: anthropomorphism has asymmetric welfare effects depending on AI objectives (Propositions~8--9). With prosocial AI, higher anthropomorphism weakly increases welfare by elevating cooperation-inducing expectations. Elevated anthropomorphism---attributed beliefs exceeding the zero-anthropomorphism benchmark---improves welfare further when it triggers a regime switch from defection to cooperation. With materialist AI, anthropomorphism creates \textit{phantom expectations}: humans attribute expectations to agents that neither expect nor care. When phantom expectations exceed feasible returns, humans incur guilt from disappointing agents holding no such expectations---a pure welfare loss benefiting no one.

These welfare effects yield design principles (Propositions~10 and 10'). Prosocial AI should use minimal anthropomorphic signaling sufficient to induce cooperation---a threshold-finding objective. Higher cooperation efficiency reduces the required signaling threshold because cooperation becomes easier to sustain. Materialist AI should use mechanical presentation---any positive anthropomorphic signal creates phantom expectations that reduce welfare. Mixed objectives face a tradeoff: intermediate signaling balances cooperation benefits against guilt costs---a marginal-balancing objective distinct from threshold-finding. Material and extended welfare measures agree when attributed expectations stay below feasibility but diverge when phantom expectations arise (Corollaries~2--3). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs to users. This divergence between private incentives and social welfare provides a case for transparency regulation requiring disclosure of AI objectives.

\subsection*{Related Literature}

This paper contributes to four literatures.

\textit{Psychological game theory.} \citet{geanakoplos1989psychological} introduced games where payoffs depend on beliefs about beliefs, enabling analysis of guilt and reciprocity tied to expectation violation. \citet{battigalli2009dynamic} extended this to dynamic settings with applications to guilt aversion \citep{charness2006promises} and sequential reciprocity \citep{dufwenberg2004theory}. We depart by accommodating asymmetric player types: humans with belief-dependent preferences, AI with design-dependent objectives. The attribution function replaces belief consistency for human beliefs about AI. Unlike standard psychological games requiring cognizable beliefs on both sides, attributed beliefs need not be cognizable since AI lacks genuine mental states. The framework nests standard theories: when all players are human, ABE reduces to Psychological Nash Equilibrium; when psychological payoffs vanish, to Nash equilibrium; when attribution is rational, to Bayesian games with type uncertainty (Propositions~1--3). ABE deviates from existing theory when attribution is biased by anthropomorphism.

\textit{AI and strategic behavior.} \citet{mei2024turing} find that large language models behave similarly to humans in economic games but are more prosocial. \citet{rahwan2019machine} argue for treating AI as social actors; \citet{horton2023large} show language models simulate human experimental responses. This literature documents the empirical relevance of human-AI interaction but lacks game-theoretic foundations. We provide equilibrium concepts generating testable predictions about how AI design affects human behavior. The phantom expectations problem connects to AI alignment: humans optimize on attributed expectations that may not reflect AI objectives, creating welfare losses when beliefs and design diverge.

\textit{Anthropomorphism and mind perception.} \citet{epley2007seeing} identify three determinants of anthropomorphism in the SEEK framework: sociality motivation (the need for social connection), effectance motivation (the drive to understand and predict agents), and elicited agent knowledge (cues triggering humanlike mental models). These map to our model: sociality and effectance shape individual anthropomorphism tendency; agent knowledge corresponds to observable design signals. \citet{nass2000machines} established that humans apply social rules---politeness, reciprocity---to computers even when they know they interact with machines.

\citet{gray2007dimensions} decompose mind perception into two dimensions: agency (capacity to act and form intentions) and experience (capacity to feel and suffer). This distinction explains why attribution and attenuation coexist: AI is attributed high agency but low experience. Agency drives attribution---humans form beliefs about AI expectations. Low perceived experience drives attenuation---moral emotions weaken when the target lacks capacity to suffer. \citet{karpus2025cross} find Japanese participants exhibit guilt toward robots comparable to humans, while Western participants show strong attenuation---indicating attenuation is culturally variable. Design features conveying emotional capacity partially restore guilt. We formalize these patterns through attenuation parameters varying across individuals and contexts.

\textit{Anthropomorphic design.} \citet{schroeder2016voice} show that voice increases mind attribution; \citet{wiese2017robots} document effects of gaze and movement; \citet{waytz2014trusting} demonstrate that naming and voice increase trust. These design effects operate through the attribution channel we formalize. Our optimal design principles derive from this connection: prosocial AI benefits from anthropomorphic features that elevate cooperation-inducing expectations; materialist AI should avoid anthropomorphic presentation that creates phantom expectations---psychological costs from disappointing agents that neither expect nor care (Proposition~10). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs---a case for transparency regulation.

\subsection*{Outline}

Section~\ref{sec:framework} presents the formal framework: asymmetric psychological games, the attribution function, and attenuation parameters. Section~\ref{sec:equilibrium} defines ABE, establishes existence, proves that ABE nests standard frameworks (psychological games, Nash equilibrium, Bayesian games), and demonstrates attribution-dependent multiplicity. Section~\ref{sec:applications} analyzes trust, public goods, and coordination games, generating testable predictions about betrayal aversion, cooperation patterns, and focal point provision. Section~\ref{sec:welfare} examines welfare effects of anthropomorphism and derives optimal AI presentation strategies for prosocial, materialist, and mixed AI designs. Section~\ref{sec:conclusion} concludes.
