\subsection*{Related Literature}

This paper contributes to four literatures.

\textit{Psychological game theory.} \citet{geanakoplos1989psychological} introduced games where payoffs depend on beliefs about beliefs, enabling analysis of guilt and reciprocity tied to expectation violation. \citet{battigalli2009dynamic} extended this to dynamic settings with applications to guilt aversion \citep{charness2006promises} and sequential reciprocity \citep{dufwenberg2004theory}. We depart by accommodating asymmetric player types: humans with belief-dependent preferences, AI with design-dependent objectives. The attribution function replaces belief consistency for human beliefs about AI. Unlike standard psychological games requiring cognizable beliefs on both sides, attributed beliefs need not be cognizable since AI lacks genuine mental states. The framework nests standard theories: when all players are human, ABE reduces to Psychological Nash Equilibrium; when psychological payoffs vanish, to Nash equilibrium; when attribution is rational, to Bayesian games with type uncertainty (Propositions~1--3). ABE deviates from existing theory when attribution is biased by anthropomorphism.

\textit{AI and strategic behavior.} \citet{mei2024turing} find that large language models behave similarly to humans in economic games but are more prosocial. \citet{rahwan2019machine} argue for treating AI as social actors; \citet{horton2023large} show language models simulate human experimental responses. This literature documents the empirical relevance of human-AI interaction but lacks game-theoretic foundations. We provide equilibrium concepts generating testable predictions about how AI design affects human behavior. The phantom expectations problem connects to AI alignment: humans optimize on attributed expectations that may not reflect AI objectives, creating welfare losses when beliefs and design diverge.

\textit{Anthropomorphism and mind perception.} \citet{epley2007seeing} identify three determinants of anthropomorphism in the SEEK framework: sociality motivation (the need for social connection), effectance motivation (the drive to understand and predict agents), and elicited agent knowledge (cues triggering humanlike mental models). These map to our model: sociality and effectance shape individual anthropomorphism tendency; agent knowledge corresponds to observable design signals. \citet{nass2000machines} established that humans apply social rules---politeness, reciprocity---to computers even when they know they interact with machines.

\citet{gray2007dimensions} decompose mind perception into two dimensions: agency (capacity to act and form intentions) and experience (capacity to feel and suffer). This distinction explains why attribution and attenuation coexist: AI is attributed high agency but low experience. Agency drives attribution---humans form beliefs about AI expectations. Low perceived experience drives attenuation---moral emotions weaken when the target lacks capacity to suffer. \citet{karpus2025cross} find Japanese participants exhibit guilt toward robots comparable to humans, while Western participants show strong attenuation---indicating attenuation is culturally variable. Design features conveying emotional capacity partially restore guilt. We formalize these patterns through attenuation parameters varying across individuals and contexts.

\textit{Anthropomorphic design.} \citet{schroeder2016voice} show that voice increases mind attribution; \citet{wiese2017robots} document effects of gaze and movement; \citet{waytz2014trusting} demonstrate that naming and voice increase trust. These design effects operate through the attribution channel we formalize. Our optimal design principles derive from this connection: prosocial AI benefits from anthropomorphic features that elevate cooperation-inducing expectations; materialist AI should avoid anthropomorphic presentation that creates phantom expectations---psychological costs from disappointing agents that neither expect nor care (Proposition~10). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs---a case for transparency regulation.
