% Results Preview Section - Draft
% Target: ~650 words, 5 clusters
% NO equations - verbal descriptions only

\subsection*{Results}

We establish five clusters of results.

First, \textit{existence}: under regularity conditions on strategy spaces, utilities, and attribution functions, ABE exists (Theorem~1). The proof extends fixed-point arguments to dual belief structures---genuine beliefs about humans, attributed beliefs about AI. This existence result establishes ABE as a tractable framework for analyzing human-AI interaction under psychological heterogeneity.

Second, \textit{nesting}: ABE reduces to standard frameworks as special cases. When all players are human, ABE coincides with psychological Nash equilibrium (Proposition~1). When psychological payoffs vanish, ABE strategies coincide with Nash equilibria (Proposition~2). When attribution is rational---humans correctly anticipate AI behavior given design parameters---ABE reduces to Bayes-Nash equilibrium with type uncertainty (Proposition~3). These reductions establish ABE as a proper generalization of existing theory. The framework introduces two distinct benchmarks: the \textit{zero-anthropomorphism benchmark} identifies what humans would attribute absent anthropomorphic bias---a descriptive baseline. The \textit{Rational Attribution Equilibrium} (RAE) refines this by requiring attributed beliefs to equal equilibrium strategies---an equilibrium concept. These are distinct: zero-anthropomorphism describes attribution at a parameter value; rational attribution requires a fixed-point condition.

Third, \textit{multiplicity}: different attribution functions sustain different equilibria in the same material game (Proposition~4). In a trust game with AI trustor and human trustee, anthropomorphism above the cooperation threshold elevates attributed expectations, increasing guilt and equilibrium returns. Below this threshold, attributed beliefs attenuate, reducing psychological pressure. This multiplicity is consistent with absent betrayal aversion toward computers documented in prior experimental work. Interface design and behavioral presentation serve as equilibrium selection devices operating through the attribution channel: the same underlying game produces different outcomes depending on how AI is presented. The testable prediction is an interaction effect---high-anthropomorphism individuals respond more strongly to interface manipulation than low-anthropomorphism individuals.

Fourth, \textit{applications}: the framework generates testable predictions in canonical games (Propositions~5--7). In trust games, anthropomorphism determines equilibrium returns: the human trustee returns either the attributed expectation or the maximum feasible return, whichever is lower. Returns increase through the guilt channel, not material incentives. In public goods, increasing AI population share has dual effects operating through distinct channels. The material channel: diluted material returns reduce cooperation incentives. The psychological channel: elevated attributed expectations from AI partners increase indignation costs of defection. The net effect depends on indignation attenuation, which varies cross-culturally---Japanese participants exhibit attenuation factors approximately half those of Western participants. This predicts non-monotonic effects of AI population share, with direction depending on cultural context. In coordination games, AI serves as focal point through expectation conformity: humans experience psychological pressure to match attributed expectations, resolving equilibrium multiplicity. AI provides a constructed focal point that can substitute for cultural or historical coordination devices.

Fifth, \textit{welfare and design}: anthropomorphism has asymmetric welfare effects depending on AI objectives (Propositions~8--9). With prosocial AI, higher anthropomorphism weakly increases welfare by elevating cooperation-inducing expectations. Elevated anthropomorphism---attributed beliefs exceeding the zero-anthropomorphism benchmark---improves welfare further when it triggers a regime switch from defection to cooperation. With materialist AI, anthropomorphism creates \textit{phantom expectations}: humans attribute expectations to agents that neither expect nor care. When phantom expectations exceed feasible returns, humans incur guilt from disappointing agents holding no such expectations---a pure welfare loss benefiting no one.

These welfare effects yield design principles (Propositions~10 and 10'). Prosocial AI should use minimal anthropomorphic signaling sufficient to induce cooperation---a threshold-finding objective. Higher cooperation efficiency reduces the required signaling threshold because cooperation becomes easier to sustain. Materialist AI should use mechanical presentation---any positive anthropomorphic signal creates phantom expectations that reduce welfare. Mixed objectives face a tradeoff: intermediate signaling balances cooperation benefits against guilt costs---a marginal-balancing objective distinct from threshold-finding. Material and extended welfare measures agree when attributed expectations stay below feasibility but diverge when phantom expectations arise (Corollaries~2--3). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs to users. This divergence between private incentives and social welfare provides a case for transparency regulation requiring disclosure of AI objectives.
