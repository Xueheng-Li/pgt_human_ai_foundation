% Section 1: Introduction
% Final revised draft applying all Priority 1 and Priority 2 fixes
% Generated: January 14, 2026

\section{Introduction}

Artificial intelligence transforms economic and social life \citep{kaplan2020scaling, maslej2025aiindex}. Humans increasingly interact with AI agents---as collaborators, counterparties, and competitors \citep{acemoglu2024simple, rahwan2019machine}. Two asymmetries characterize this interaction. First, \textit{attribution}: humans attribute beliefs, intentions, and expectations to AI agents that lack mental states \citep{nass2000machines, gray2007dimensions, epley2007seeing}. A meta-analysis of 97 effect sizes confirms that anthropomorphism increases trust and cooperation in human-AI contexts \citep{blut2021understanding}. Second, \textit{attenuation}: moral emotions toward AI are weaker than toward humans. Humans feel less guilt exploiting machines \citep{demelo2017people} and less outrage when algorithms discriminate \citep{bigman2023people}. This dual psychological asymmetry---attribution without attenuation---lies beyond existing psychological game theory \citep{battigalli2022belief}, which lacks tools to analyze how humans form beliefs about AI mental states.

Three behavioral regularities sharpen this asymmetry. Guilt from disappointing expectations \citep{charness2006promises,sugden_Robert_2000}, reciprocity from perceived intentions \citep{rabin_Incorporating_1993,dufwenberg2004theory}, indignation from violated trust \citep{li_Indignation_2026}---all require beliefs about beliefs that AI cannot hold. Meanwhile, attenuation varies culturally: Japanese participants exhibit guilt toward robots comparable to guilt toward humans, while Western participants show strong attenuation, exploiting robotic partners at twice the rate \citep{karpus2025cross}. Attenuation is also design-dependent: AI expressing emotional distress partially restores guilt.

We introduce Attributed Belief Equilibrium (ABE) to address this dual asymmetry. ABE extends psychological game theory \citep{geanakoplos1989psychological, battigalli2009dynamic} to games where humans experience belief-dependent payoffs while AI optimize programmed objectives. The central insight: humans attribute mental states to AI, forming beliefs about what AI ``expect'' or ``believe.'' These attributed beliefs trigger guilt from disappointing attributed expectations and indignation from perceived violations---mirroring genuine human-to-human guilt.

The attribution function captures how humans form these beliefs. Three inputs determine attributed expectations: AI design parameters (prosociality level), observable signals (interface, behavior), and individual anthropomorphism tendency. A chatbot designed to express concern (``I understand your frustration'') generates higher attributed expectations than one providing neutral information. Attenuation parameters scale emotional intensity toward AI, from no attenuation (emotions equal human-directed) to full attenuation (no emotion toward AI). Attributed beliefs need not correspond to any actual AI mental state---only to what the human projects.

When attributed beliefs exceed feasible returns, humans experience \textit{phantom expectations}: guilt from disappointing agents holding no such expectations. Unlike guilt toward humans, which transfers welfare, phantom guilt is pure loss benefiting no one.

This structure reflects mind perception theory \citep{gray2007dimensions}. Humans perceive AI as having agency---capacity to act and form intentions---but lacking experience---capacity to feel and suffer. Agency drives attribution; low perceived experience drives attenuation. The asymmetry between perceived agency and perceived experience explains why attribution and attenuation coexist.

\subsection*{Results}

We establish five clusters of results.

First, \textit{existence}: under regularity conditions on strategy spaces, utilities, and attribution functions, ABE exists (Theorem~1). The proof extends fixed-point arguments to dual belief structures---genuine beliefs about humans, attributed beliefs about AI.

Second, \textit{nesting}: ABE reduces to standard frameworks as special cases. When all players are human, ABE coincides with psychological Nash equilibrium (Proposition~1). When psychological payoffs vanish, ABE strategies coincide with Nash equilibria (Proposition~2). When attribution is rational---humans correctly anticipate AI behavior given design parameters---ABE reduces to Bayes-Nash equilibrium with type uncertainty (Proposition~3). The framework introduces two distinct benchmarks. The \textit{zero-anthropomorphism benchmark} identifies what humans would attribute absent anthropomorphic bias---a descriptive baseline at a specific parameter value. The \textit{Rational Attribution Equilibrium} (RAE, Definition~10) requires attributed beliefs to equal equilibrium strategies---an equilibrium concept satisfying a fixed-point condition. Zero-anthropomorphism is descriptive; RAE is normative.

Third, \textit{multiplicity}: different attribution functions sustain different equilibria in the same material game (Proposition~4). In a trust game with AI trustor and human trustee, anthropomorphism above the cooperation threshold elevates attributed expectations, increasing guilt and equilibrium returns. Below this threshold, attributed beliefs attenuate, reducing psychological pressure. This multiplicity is consistent with absent betrayal aversion toward computers documented in prior experimental work. Interface design serves as an equilibrium selection device operating through the attribution channel: the same game produces different outcomes depending on AI presentation. The testable prediction: returns to AI trustees should increase with anthropomorphism measures, holding AI behavior constant, and high-anthropomorphism individuals should respond more strongly to interface manipulation than low-anthropomorphism individuals.

Fourth, \textit{applications}: the framework generates testable predictions in canonical games (Propositions~5--7). In trust games, anthropomorphism determines equilibrium returns: the human trustee returns either the attributed expectation or the maximum feasible return, whichever is lower. Returns increase through the guilt channel, not material incentives. In public goods, increasing AI population share has dual effects operating through distinct channels. The material channel: diluted material returns reduce cooperation incentives. The psychological channel: elevated attributed expectations from AI partners increase indignation costs of defection. The net effect depends on indignation attenuation, which varies cross-culturally---Japanese participants exhibit attenuation factors approximately half those of Western participants. This predicts non-monotonic effects of AI population share, with direction depending on cultural context. In coordination games, AI serves as focal point through expectation conformity: humans experience psychological pressure to match attributed expectations, resolving equilibrium multiplicity. AI provides a constructed focal point substituting for cultural or historical coordination devices.

Fifth, \textit{welfare and design}: anthropomorphism has asymmetric welfare effects depending on AI objectives (Propositions~8--9). With prosocial AI, higher anthropomorphism weakly increases welfare by elevating cooperation-inducing expectations. Elevated anthropomorphism---attributed beliefs exceeding the zero-anthropomorphism benchmark---improves welfare further when it triggers a regime switch from defection to cooperation.

With materialist AI, anthropomorphism creates phantom expectations. When these exceed feasible returns, humans incur guilt from disappointing agents holding no such expectations---a pure welfare loss benefiting no one.

These welfare effects yield design principles (Propositions~10 and 10'). Prosocial AI should use minimal anthropomorphic signaling sufficient to induce cooperation---a threshold-finding objective. Higher cooperation efficiency reduces the required threshold because cooperation becomes easier to sustain. Materialist AI should use mechanical presentation---any positive anthropomorphic signal creates phantom expectations that reduce welfare. Mixed objectives face a tradeoff: intermediate signaling balances cooperation benefits against guilt costs---a marginal-balancing objective distinct from threshold-finding.

Material and extended welfare measures agree when attributed expectations stay below feasibility but diverge when phantom expectations arise (Corollaries~2--3). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs to users. This divergence justifies transparency regulation.

\subsection*{Related Literature}

This paper contributes to four literatures.

\textit{Psychological game theory.} \citet{geanakoplos1989psychological} introduced games where payoffs depend on beliefs about beliefs, enabling analysis of guilt and reciprocity tied to expectation violation. \citet{battigalli2009dynamic} extended this to dynamic settings with applications to guilt aversion \citep{charness2006promises} and sequential reciprocity \citep{dufwenberg2004theory}. We depart by accommodating asymmetric player types: humans with belief-dependent preferences, AI with design-dependent objectives. The attribution function replaces belief consistency for human beliefs about AI. Unlike standard psychological games requiring cognizable beliefs on both sides, attributed beliefs need not be cognizable since AI lacks genuine mental states. The framework nests standard theories: when all players are human, ABE reduces to Psychological Nash Equilibrium; when psychological payoffs vanish, to Nash equilibrium; when attribution is rational, to Bayesian games with type uncertainty (Propositions~1--3). ABE deviates from existing theory when attribution is biased by anthropomorphism.

\textit{AI and strategic behavior.} \citet{mei2024turing} find that large language models behave similarly to humans in economic games but are more prosocial. \citet{rahwan2019machine} argue for treating AI as social actors; \citet{horton2023large} show language models simulate human experimental responses. This literature documents the empirical relevance of human-AI interaction but lacks game-theoretic foundations. We provide equilibrium concepts generating testable predictions about how AI design affects human behavior. The phantom expectations problem connects to AI alignment: humans optimize on attributed expectations that may not reflect AI objectives, creating welfare losses when beliefs and design diverge.

\textit{Anthropomorphism and mind perception.} \citet{epley2007seeing} identify three determinants of anthropomorphism in the SEEK framework: sociality motivation, effectance motivation, and elicited agent knowledge. These map to our model: sociality and effectance shape individual anthropomorphism tendency; agent knowledge corresponds to observable design signals. \citet{nass2000machines} established that humans apply social rules to computers even when they know they interact with machines.

\citet{gray2007dimensions} decompose mind perception into agency (capacity to act and form intentions) and experience (capacity to feel and suffer). This distinction explains why attribution and attenuation coexist: AI is attributed high agency but low experience. Agency drives attribution; low perceived experience drives attenuation. \citet{karpus2025cross} find Japanese participants exhibit guilt toward robots comparable to humans, while Western participants show strong attenuation---indicating attenuation is culturally variable. Design features conveying emotional capacity partially restore guilt. We formalize these patterns through attenuation parameters varying across individuals and contexts.

\textit{Anthropomorphic design.} \citet{schroeder2016voice} show that voice increases mind attribution; \citet{wiese2017robots} document effects of gaze and movement; \citet{waytz2014trusting} demonstrate that naming and voice increase trust. These design effects operate through the attribution channel we formalize. Our optimal design principles derive from this connection: prosocial AI benefits from anthropomorphic features elevating cooperation-inducing expectations; materialist AI should avoid anthropomorphic presentation creating phantom expectations (Proposition~10). Private designers may over-anthropomorphize materialist AI to increase engagement, externalizing psychological costs---a case for transparency regulation.

\subsection*{Outline}

Section~\ref{sec:framework} presents the formal framework: asymmetric psychological games, the attribution function, and attenuation parameters. Section~\ref{sec:equilibrium} defines ABE, establishes existence, proves nesting results, and demonstrates attribution-dependent multiplicity. Section~\ref{sec:applications} analyzes trust, public goods, and coordination games, generating testable predictions. Section~\ref{sec:welfare} examines welfare effects of anthropomorphism and derives optimal AI presentation strategies. Section~\ref{sec:conclusion} concludes.
