{"result":"# Dynamic psychological games\n\n**Type:** journalArticle\n\n**Item Key:** 33W8M5G4\n\n**Date:** 2009\n\n**Authors:** Battigalli, Pierpaolo; Dufwenberg, Martin\n\n**Journal:** Journal of Economic Theory, Volume 144, Issue April, Pages 1-35\n\n**DOI:** http://dx.doi.org/10.1016/j.jet.2008.01.004\n\n**URL:** internal-pdf://196.0.0.1/2009Dynamic psychological games.pdf http://www.sciencedirect.com/science/article/pii/S0022053108000379 internal-pdf://196.0.0.1/2009Dynamic psychological games.pdf\n\n\n\n## Extra\n\nNumber: April\nCitation Key: battigalli_Dynamic_2009\n\n**Citation Key (from Extra):** battigalli_Dynamic_2009\n\n**Tags:** `Psychological games Belief-dependent motivation Ex`\n\n\n\n## Abstract\n\nThe motivation of decision makers who care for various emotions, intentions-based reciprocity, or the opinions of others may depend directly on beliefs (about choices, beliefs, or information). Geanakoplos, Pearce and Stacchetti [J. Geanakoplos, D. Pearce, E. Stacchetti, Psychological games and sequential rationality, Games Econ. Behav. 1 (1989) 60–79] point out that traditional game theory is ill-equipped to address such matters, and they pioneer a new framework which does. However, their toolbox – psychological game theory – incorporates several restrictions that rule out plausible forms of belief-dependent motivation. Building on recent work on dynamic interactive epistemology, we propose a more general framework. Updated higher-order beliefs, beliefs of others, and plans of action may influence motivation, and we can capture dynamic psychological effects (such as sequential reciprocity, psychological forward induction, and regret) that were previously ruled out. We develop solution concepts, provide examples, explore properties, and suggest avenues for future research.\n\n**Collections:** 4 collections\n\n**Notes/Attachments:** 2\n\n---\n\n## Full Text\n\nJournal of Economic Theory 144 (2009) 1–35\n\nwww.elsevier.com/locate/jet\n\nDynamic psychological games\n\nPierpaolo Battigalli a,∗, Martin Dufwenberg b\n\na Bocconi University, Via Sarfatti 25, 20136 Milan, Italy\nb University of Arizona, Tucson, AZ, USA\n\nReceived 30 November 2007; accepted 31 January 2008\n\nAvailable online 17 May 2008\n\nAbstract\n\nThe motivation of decision makers who care for various emotions, intentions-based reciprocity, or the\nopinions of others may depend directly on beliefs (about choices, beliefs, or information). Geanakoplos,\nPearce and Stacchetti [J. Geanakoplos, D. Pearce, E. Stacchetti, Psychological games and sequential ratio-\nnality, Games Econ. Behav. 1 (1989) 60–79] point out that traditional game theory is ill-equipped to address\nsuch matters, and they pioneer a new framework which does. However, their toolbox – psychological game\ntheory – incorporates several restrictions that rule out plausible forms of belief-dependent motivation. Build-\ning on recent work on dynamic interactive epistemology, we propose a more general framework. Updated\nhigher-order beliefs, beliefs of others, and plans of action may inﬂuence motivation, and we can capture\ndynamic psychological effects (such as sequential reciprocity, psychological forward induction, and regret)\nthat were previously ruled out. We develop solution concepts, provide examples, explore properties, and\nsuggest avenues for future research.\n© 2008 Elsevier Inc. All rights reserved.\n\nJEL classiﬁcation: C72; C73\n\nKeywords: Psychological games; Belief-dependent motivation; Extensive-form solution concepts; Dynamic interactive\n\nepistemology\n\n* Corresponding author. Fax: +39 02 5836 3332.\n\nE-mail address: pierpaolo.battigalli@unibocconi.it (P. Battigalli).\n\n0022-0531/$ – see front matter © 2008 Elsevier Inc. All rights reserved.\ndoi:10.1016/j.jet.2008.01.004\n\n\f2\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n1. Introduction\n\nWe develop a framework for analyzing strategic interaction when players have ‘belief-\ndependent’ motivations, generalizing the theory of extensive form psychological games pro-\nposed by Geanakoplos, Pearce and Stacchetti [36] (hereafter GPS, see also Gilboa and Schmei-\ndler [37]). The rest of this Introduction motivates in more detail.\n\nTraditional game theory is not a rich enough toolbox to adequately describe many psycho-\nlogical or social aspects of motivation and behavior. The traditional approach assumes utilities\ndepend only on which actions are chosen, but if decision makers are emotional or care for the\nintentions, opinions, or emotions of others utilities may depend also on which beliefs (about\nchoices, beliefs, or information) players harbor. The following examples illustrate:\n\n1. When Abi takes a taxi ride she tips as much as she expects that the driver (Ben) expects to\n\nget. She suffers from guilt if she tips less.\n\n2. Cleo suddenly pushes Dan over. Should Dan splash a bucket of water over Cleo in return?\nMaybe she actually tried to hug him? If so, Dan would rather forgive (maybe even hug) Cleo.\n3. Eva is unemployed. Her neighbor, Fred, observes the effort with which she tries to get a job.\nFred’s taxes pay for Eva’s unemployment beneﬁts, so Eva’s choice has externalities the size\nof which depends on her talent translating effort to probability of getting a job (low effort\nis costlier to Fred if Eva is talented and could have gotten a job had she tried harder). Eva’s\ntalent is known only to her, but Fred makes inferences observing her effort. This determines\nthe social respect he bestows on Eva, and since she cares about respect this inﬂuences her\neffort.\n\n4. Gwen is anxious about her health. Hal, her doctor, has diagnosed a serious illness. He is con-\ncerned with Gwen’s health and anxiety. Should he prescribe the most appropriate treatment\nand thus reveal to Gwen how bad is her situation?\n\nAbi’s tip, Dan’s hug/soak choice, Eva’s effort, and Hal’s prescription each pins down an out-\n\ncome. Yet the preferred choice depends on a belief.1\n\nThe point that belief-dependent motivation may be important for strategic decision making\nis made by GPS, who present several intriguing examples involving various emotions. They\nshow the inadequacy of traditional methods to represent the involved preferences, and develop\nan extension (in the normal as well as in the extensive form) of traditional game theory to deal\nwith the matter. A literature has emerged which either draws on or which can be related to GPS’\nframework. Contributions include applications to speciﬁc economic problems,2 models of certain\nforms of belief-dependent preferences (like reciprocity or guilt),3 and experimental studies that\n\n1 Abi’s preference depends on her belief of Ben’s belief about the tip; Dan’s on his assessment of Cleo’s intentions;\nEva’s on Fred’s inferences on her talent; Hal’s on his belief about Gwen’s anxiety – a belief-induced psychological state\n– and on his belief about Gwen’s sensitivity to anxiety.\n2 See Huang and Wu [42], Dufwenberg [24,25], Geanakoplos [35], Rufﬂe [71], Dufwenberg and Kirchsteiger [28],\nHuck and Kübler [43], Caplin and Eliaz [20], Caplin and Leahy [22], Li [55], K˝oszegi [48], K˝oszegi and Rabin [49,50],\nSebald [72]. Bernheim [16] and Dufwenberg and Lundholm [30] also consider belief-dependent motivations although\nthe authors do not draw a connection to psychological games.\n3 See Rabin [67], Dufwenberg and Kirchsteiger [29], Falk and Fischbacher [31], Battigalli and Dufwenberg [8], Segal\nand Sobel [73]; see Sobel [75] for some related discussion.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n3\n\nhave provided support for such models.4 By now a quite large set of economists argue that belief-\ndependent motivation is relevant to economic behavior.5\n\nWhile GPS’ paper is a source of inspiration for all work on belief-dependent motivation,\nand an applicable toolbox for some work, a careful scrutiny reveals that their approach is too\nrestrictive to handle many plausible forms of belief-dependent motivation (this is acknowledged\nby GPS themselves; see pp. 70, 78–79). There are several reasons including the following four:\n\nR1 (updated beliefs): GPS only allow initial beliefs to enter the domain of a player’s utility,\nwhile many seemingly important forms of belief-dependent motivation require updated be-\nliefs to matter.\n\nR2 (others’ beliefs): GPS only allow a player’s own beliefs to enter the domain of his utility\nfunction, while there are conceptual and technical reasons to let others’ beliefs matter.\nR3 (dependence on plans): GPS follow the traditional extensive games approach of letting\nstrategies inﬂuence utilities only insofar as they inﬂuence terminal histories, but many forms\nof belief-dependent motivation become compelling in particular in conjunction with prefer-\nences that depend on strategies in ways not captured by terminal histories.\n\nR4 (non-equilibrium analysis): GPS restrict attention to equilibrium analysis, but in many strate-\ngic situations there is little compelling reason to expect players to coordinate on an equilib-\nrium and one may wish to explore alternative assumptions.\n\nThis list deserves backup by examples, but we postpone this until the next section. Here we\njust note that items in the list have lead some researchers to deviate from GPS’ framework, in\ndeveloping speciﬁc examples or models with belief-dependent motivation. However almost no\npapers are concerned with developing the overall framework of psychological game theory.6 We\nattempt to ﬁll this gap, using R1–R4 as guiding principles.\n\nOur approach crucially draws on Battigalli and Siniscalchi’s [10] work on how to represent\nhierarchies of conditional beliefs. This is essential for R1, and ﬁgures in the background of\nR2–R4 which are all related to updated beliefs. We deﬁne a large class of psychological games,\nwhich contains (in a particular sense) GPS’ games and traditional games as special cases. Our\nmain goal is to develop this basic framework, and to illustrate some solution concepts that can\nbe meaningfully developed for it. While one could imagine a variety of interesting solution con-\ncepts, we choose to extend two basic concepts of classical game theory to our setting: sequential\nequilibrium and (extensive form) rationalizability. We prove related theorems, and illustrate how\nthe concepts work in examples.\n\n4 See Dufwenberg and Gneezy [27], Guerra and Zizzo [39], Bacharach, Guerra and Zizzo [5], Charness and Dufwen-\nberg [23], Bouckaert and Dhaene [17], Dufwenberg, Gaechter and Hennig-Schmidt [26], Tadelis [77], as well as the\nsurvey Attanasi and Nagel [3].\n5 Add some (not many) decision-theorists to this list: Machina [58,59] presents examples which concern belief-\ndependent forms of disappointment. Robin Pope has written extensively (since the early 80’s) about how conventional\ndecision theory excludes various forms of belief-dependent motivation; Pope [66] expounds her program and give further\nreferences and Albers, Pope, Selten and Vogt [1] report on a related experiment. Bell [13], Loomes and Sugden [57],\nKarni [44], Karni and Schlee [45], and Caplin and Leahy [21] develop single decision-maker models in which utility\nmay depend directly on beliefs.\n6 Kolpin [47] explores an alternative route to GPS’ games, where players ‘choose beliefs.’ Segal and Sobel [73] analyze\nsimultaneous move games, and assume preferences over material consequences depend on the equilibrium probability\ndistribution over actions. They observe that their approach can be regarded as a reformulation of GPS’ normal form\ngames.\n\n\f4\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nR1–R4 do not exhaust the good reasons to generalize GPS, but in the name of pedagogical\nclarity we only deal with R1–R4 in most sections of this paper (2–5). Section 2 surveys these con-\nceptual issues. Section 3 develops the general framework, up to the deﬁnition of a psychological\ngame. Section 4 concerns sequential equilibrium. Section 5 concerns interactive epistemology\nand rationalizability. Section 6 contains a discussion as well as extensions beyond R1–R4. We\ncompare our approach to that of GPS in more depth, consider imperfect information, chance\nmoves, asymmetric information, own-plan dependence, dynamic inconsistency, and multi-self\nutility, and ﬁnally offer remarks regarding solutions concept we did not develop. Appendix A\ncollects most of the proofs.\n\n2. Overview of some conceptual issues\n\nThis section surveys the conceptual issues that motivate us. We ﬁrst describe what GPS’ do,\nand why this is ‘non-standard’ vis-a-vis traditional game theory (2.1). We then explain what is\nour own contribution, going through R1–R4 in more detail (2.2). The style is ‘semi-technical,’\nwe introduce some notation, but postpone proper treatment of details for later.\n\n2.1. What GPS do\n\nThe traditional approach to analyzing extensive games (with complete information) describes\n\na player’s preferences using a utility function of the form\n\nui : Z → R\n\nwhere Z is the set of terminal histories (endnodes).\n\nPsychological games capture richer motivations than traditional games, and the payoff func-\ntions have richer domains. GPS deﬁne a set of i’s initial (pre-play) beliefs about others’ strategies\nand initial beliefs, here referred to as Mi , which does not rule out any hierarchy of initial beliefs.\nEach element of Mi is a sequence μi = (μ1\ni , μ2\ni , . . .) where μ1\ni represents i’s beliefs about the\nopponents’ strategies, or ﬁrst-order beliefs, μ2\ni represents i’s joint beliefs about the opponents’\nstrategies and ﬁrst-order beliefs, and so on.7\n\nGPS model preferences using utility functions of the form\n\nui : Z × Mi → R.\n\nThis structure bears some superﬁcial similarities to games of incomplete information. It is worth\nclarifying the differences. In a game of incomplete information some payoff-relevant exogenous\nparameters (e.g. players’ abilities or tastes) are not commonly known. Let θ ∈ Θ denote the\nvector of such parameters. Players’ payoffs are represented by parametrized utility functions\nvi : Z × Θ → R. Note that θ does not specify strategic choices. A player has beliefs about θ\n(comprising her private information about θ ), beliefs about the beliefs of others concerning θ ,\netc. Following Harsanyi [41], such ﬁrst- and higher-order beliefs can be represented in an elegant,\nalbeit implicit, form by assuming that each player i is characterized by a ‘type’ ti ∈ Ti and\neach ti corresponds to a probability measure pti over the set of payoff-relevant parameters and\n∈ Δ(Θ × T−i). It can be shown that pti corresponds to an inﬁnite\nopponents’ types, i.e. pti\n\n7 More formally, ﬁrst-order beliefs are elements of Δ(S−i ) (where S−i is the set of strategy proﬁles of i’s co-players),\nsecond-order beliefs elements of Δ(S−i ×\nj (cid:5)=i Δ(S−j )), etc. (see [18] and [61]). Upper-bars distinguish initial beliefs\nfrom systems of conditional beliefs, the main object of our analysis. We will be more precise in Section 3.\n\n(cid:2)\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n5\n\nhierarchy of beliefs (p1\nis a joint\nti\nbelief about θ and the opponents’ beliefs about θ , and so on. Taking conditional expectations, the\npayoff functions of the incomplete information game can be represented as Vi : Z ×\nj Tj → R,\nwhere\n\n∈ Δ(Θ) is the marginal of pti on Θ, p2\nti\n(cid:2)\n\n, . . .) where p1\nti\n\n, p2\nti\n\n(cid:3)\n\nVi(z, ti, t−i) =\n\nvi(z, θ )pti (dθ |t−i).\n\nThus, both psychological games and incomplete information games can be described so that\npayoffs depend not only on how the game is played (z ∈ Z) but also on hierarchical beliefs.\nHowever, we are talking about different beliefs in the two cases. In psychological games payoffs\nat endnodes depend on beliefs about strategies, beliefs about such beliefs, and so on. The modeler\nexplains/predicts, such beliefs via some solution concept. Hence payoffs at a given endnode are\nendogenous. On the other hand, players’ hierarchical beliefs about the parameter vector θ are as\nexogenous as θ itself. Hence payoffs at a given terminal history of an incomplete information\ngame are exogenous as well.\n\n2.2. Extension of GPS\n\nGPS’ approach can capture interesting forms of belief-dependent motivation. Example 1 of\nthe Introduction, e.g., could be handled by assuming that Abi’s utility equals w − m − 2 max{μ −\nm, 0}, where w is her pre-tip wealth, m ∈ {0, 1, . . . , w} her tip, and μ her expectation of Ben’s\nexpectation of m, a function of her second-order belief. Abi maximizes her utility by choosing\nm = μ.\n\nHowever, the issues R1–R4 lead us to enrich the domain of utilities further. We consider\n\npayoff functions of the form\nui : Z × Mi ×\n\n(cid:4)\n\n(Mj × Sj ) → R\n\nj (cid:5)=i\n\nwhere Mj (with j = i or j (cid:5)= i) is the set of j ’s possible conditional beliefs about others’ strate-\ngies and conditional beliefs, Sj is the set of (pure) strategies of j . The conditioning in Mj is done\nfor every history, building on Battigalli and Siniscalchi [10] who show how to represent hierar-\nchies of conditional beliefs without ruling out any hierarchy. Mj is (isomorphic to) a subspace\nof Mj , so the payoff functions we consider are more general than those assumed by GPS.8\n\nIssues R1–R4 will be related to different arguments of ui as we go.\n\nR1: updated beliefs\nRabin’s [67] reciprocity theory, in which a player’s preferences over material payoff distribu-\ntions depends on the co-players intentions, is perhaps the most well-known application of GPS’\ntheory. Rabin works in the normal form. His goal is to highlight key qualitative features of reci-\nprocity, and he does not address issues of dynamic decision making although he points out that\nthis is important for applied work (p. 1296).9 Dufwenberg and Kirchsteiger [29] pick up from\nthere, and develop a reciprocity theory for extensive games. In motivating their exercise, they\nargue that it is necessary to deviate from GPS’ extensive form framework: GPS only allow ini-\ntial beliefs to enter the domain of a player’s utility, while the modeling of reciprocal response at\n\n8 For a more precise comparison between our framework and GPS see Subsection 6.1.\n9 Rabin [68] (see p. 23 and footnote 16) also provides a hilarious autobiographical sequential-move example reminis-\ncent of our Example 2 of the Introduction.\n\n\f6\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nFig. 1. Trust game Γ1 with material payoffs.\n\nFig. 2. Psychological trust game Γ2.\n\nvarious ventures of a game tree requires that (intentions-based) kindness be re-evaluated using\nupdated belief. The argument is an instance of R1.\n\nReciprocity theory does not provide the easiest route to illustrating the key issues involved\nthough. Instead, we consider the motivation of guilt aversion, applied to the trust game Γ1 in\nFig. 1.10 Payoffs are in dollars and do not necessarily represent preferences. Therefore, we call\nthem ‘material payoffs.’\n\nWe now modify Γ1 to incorporate a guilt sentiment of Bob’s: To make our point, let us ﬁrst\nspecify what it means that ‘Bob lets Ann down.’ Ann is let down if the material payoff she gets is\nless than what she expected. Let α be the probability that Ann (initially) assigns to Bob’s strategy\nShare if Trust. Bob suffers from guilt to the extent that he believes he lets Ann down. The higher\nis α the more let down she will be if he chooses Grab. Bob does not know what α is, as this\nbelief is in the mind of Ann. However, he has a belief about α. Let β be Bob’s expectation of α,\nconditional on Ann choosing Trust. We can model guilt aversion assuming that Bob’s utility at the\nterminal history (Trust, Grab) is decreasing in β. See Γ2 in Fig. 2. What appears at the terminal\nhistories should be thought of as utilities, not material payoffs although the notions coincide for\nall but one terminal histories.11\n\nΓ2 is not a psychological game in GPS’ class, because β (being an updated belief) is not\ncaptured by any element of Mi . This in itself illustrates R1. However, in order to appreciate the\n\n10 Battigalli and Dufwenberg [8] use the framework of the present paper to develop a general model of (two forms of)\nguilt aversion for extensive game forms. For the speciﬁc context of trust games, related sentiments have previously been\nconsidered by Huang and Wu [42], Dufwenberg [24,25], Dufwenberg and Gneezy [27], Guerra and Zizzo [39], Charness\nand Dufwenberg [23], and Bacharach, Guerra and Zizzo [5].\n11 There is no special signiﬁcance to the “5” in Fig. 2; we could have chosen many other numbers to make the upcoming\npoint. Similar remarks apply to all examples below.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n7\n\nFig. 3. Psychological Trust Game Γ3.\n\nsigniﬁcance of this issue, it is useful to note that one can draw compelling (we think) conclusions\nabout behavior that hinge crucially on the fact that β is an updated belief.\n\nFollowing Dufwenberg [24,25], consider the following (for the time being intuitive) ‘psycho-\nlogical forward induction’ argument: Suppose Ann chooses Trust. If she is rational, she must\nbelieves the probability that Bob would choose Share (after Trust) is at least 1\n2 . Since\nwe can ﬁgure this out, presumably Bob can too. Even if he is uncertain regarding the value of α,\n2 . Since 4 − 5β < 2 if β (cid:2) 1\nhe infers it is at least 1\n2 , he prefers Share. Since we can\nﬁgure this out, presumably Ann can too. Hence she chooses Trust, fully expecting Bob to Share\n(so α = 1). Bob ﬁgures this out (so that β = 1), which further reinforces his preference to Share.\nThe path (Trust, Share) is predicted!\n\n2 . Hence β (cid:2) 1\n\n2 , i.e., α (cid:2) 1\n\nThe argument depends on belief β being conditional on Ann choosing Trust. It cannot be\nrecast using GPS’ theory, since Mi contains only initial beliefs, but it can be captured in our\nframework, since Mi contains conditional beliefs.\n\nR2: others’ beliefs\nThere are two independent justiﬁcations for letting a player’s utility depend on others’ beliefs.\nFirst, this may be an adequate description of how certain social rewards operate. Refer back to\nExample 3 from the Introduction, where Eva’s preferences over effort depends on Fred’s infer-\nences. It is taken from Dufwenberg and Lundholm [30]. A related example is Bernheim’s [16]\nmodel of social conformity. Another example is Caplin and Leahy’s [22] story of an informa-\ntion providing doctor concerned about the belief-dependent anxiety of a patient.12 These authors\ndevelop models where a player’s utility depends on others’ beliefs (although only Caplin and\nLeahy explicitly refer to psychological games).13\n\nThe second justiﬁcation concerns convenience in modeling. Refer back to the discussion\nof Γ2, including the deﬁnition of α and β. We modeled Bob’s guilt feelings by letting his psycho-\nlogical payoff depend on β, an updated second order belief. It turns out that there is an equivalent\nmodeling choice. One can assume that Bob’s utility at ((cid:8), L) depends directly on α, rather than\non β, although Bob is uncertain about the true value of α. He uses probability assessments to\nweigh the different possibilities. We get Γ3 in Fig. 3.\n\n12 For related work see Caplin [19], Caplin and Eliaz [20], and K˝oszegi [48] from whom Example 4 of the Introduction\nis taken.\n13 The models can be interpreted as psychological games with asymmetric information where the utility of a player\ndepends on the terminal beliefs of another player (cf. Section 6.2).\n\n\f8\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nFig. 4. Modiﬁed Trust Game Γ4 with material payoffs.\n\nAfter Trust, when Bob has to make a choice he compares 2, the payoff of action Share, with\nthe conditional expected payoff of action Grab, that is E2[4 − 5α|Trust] = 4 − 5β; thus, we obtain\nthe same results as with Γ2.14\n\n(cid:2)\n\nThis illustrates an important point: some belief-dependent motivations can be modeled re-\nplacing a conditional own belief of a certain ‘order’ (meaning: how many layers of beliefs about\nbeliefs/choices are involved) with another object involving one degree lower order. This may\nallow one to work with utilities ui : Z ×\nj (cid:5)=i(Mj × Sj ) → R, where Mi is not a factor of the\ndomain. This has two advantages. First, it may seem easier to represent preferences with lower\norder beliefs (like α in Γ3 rather than β in Γ2). Second, and most importantly, one is lead to\nclearly distinguish between the carriers of utility (i.e., elements of Z ×\nj (cid:5)=i(Mj × Sj )) and\nhow a player deals with uncertainty by making updated probabilistic predictions (described by\n(cid:2)\nj (cid:5)=i(Mj × Sj ) ele-\nelements of Mi ). By contrast, when the domain of i’s utility is Z × Mi ×\nments of Mi serve both purposes.\n\n(cid:2)\n\nR3: dependence on plans\nMany forms of belief-dependent motivation require preferences to depend on overall strate-\ngies, beyond how strategies cause terminal histories when they are carried out.15 Consider Γ4 in\nFig. 4, a variation of Γ1 where Ann may ‘dissipate’ some payoff. The payoffs of Γ4 are material,\nnot necessarily reﬂecting utilities.\n\nRecall (from the discussion of R1) the terminology that ‘Ann is let down’ if the material\npayoff she gets is less than what she expects. In Γ4 what she expects to get does not only depend\non her beliefs about Bob, but also on how she plans to play. Suppose Ann plans to trust Bob\nand then keep the surplus. Then her subjectively expected material payoff is 2α, where α is the\nprobability Ann assigns to Bob’s strategy Share if Trust. But if she plans to trust Bob and then\ndissipate the surplus, her expected material payoff is zero independently of α.\n\n14 We do not suggest that Γ3 is interesting only in providing a convenient alternative way to analyzing Γ2; the emotion\nmodeled in Γ3 may make sense in its own right, as a primitive assumption about preferences (akin to Examples 3 and 4\nof the Introduction).\n15 This point has been anticipated by Caplin and Leahy [21]. Mariotti [60] also makes a similar point, but not in the\ncontext of psychological games.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n9\n\nAgain assume that Bob suffers from guilt to the extent that he believes he lets Ann down.\nA natural way to model this is to let his utility at (Trust, Grab) be 4 − 5α (as in Γ3) if Ann plans\nto Keep, but 4 if she plans to Dissipate.16\n\nThis example illustrates that psychological motivations may exhibit a concern for other play-\ners’ intentions. Intentions depend on beliefs as well as plans, and the latter dependence goes\nbeyond the endnodes implied by implementing such plans. Therefore, the domain of ui includes\n(conditional) beliefs and strategies of other players, on top of terminal histories and own beliefs.\n\nR4: non-equilibrium analysis\nR1–R3 concern features of players’ motivation one may wish to incorporate in a formal\nframework. The next step is to predict play. We propose a generalization of Kreps and Wilson’s\n[51] sequential equilibrium. We postpone illustrations until we formally introduce the concept in\nSection 4.\n\nWhile much of economic theory presumes that players coordinate on an equilibrium, it is\nnot always clear such an assumption is justiﬁed. For one thing, people may be quite rational,\nand conﬁdent in others’ rationality, even if they fail to coordinate. In conventional game theory,\nrelated matters have inspired work on the implications of common belief of rationality; see e.g.\nBernheim’s [15] and Pearce’s [65] work on rationalizability. This brings us to R4. There is little\nreason to assume that equilibrium coordination is easier in psychological games than in standard\ngames. In fact, since psychological games often seem more complicated, and since problems\nof equilibrium multiplicity may be enhanced, assuming equilibrium may be assuming too much\nespecially in psychological games.17\n\nGiving up the equilibrium assumption does not necessarily mean giving up on predictive\npower. Refer back to the psychological forward induction argument, presented for Γ2. Ann and\nBob perform deductive reasoning regarding one another’s behavior and beliefs, and a clear-cut\nprediction results despite that no presumption of equilibrium is made. However, the story told\nwas informal, and speciﬁc to Γ2 (or, equivalently, Γ3). It is natural to wonder about generally\napplicable formalizations. In Section 5, we develop a framework for analyzing interactive episte-\nmology in psychological games, without postulating equilibrium play. This is a relatively small\nstep because our very deﬁnition of psychological game already provides the necessary ingredi-\nents. Building on an epistemic theme due to Battigalli and Siniscalchi [11], we extend Pearce’s\n[65] classical notion of (extensive form) rationalizability to psychological games. The concept\ncaptures psychological forward induction in simple games like Γ2 and Γ3, and in more compli-\ncated games for which long chains of beliefs about beliefs are needed to get sharp predictions.\n\n3. Psychological games\n\nIn this section we introduce notation on extensive-forms (3.1), model a universal belief space\nthat accounts for updated beliefs (3.2), and put forth and illustrate our general deﬁnition of a\npsychological game (3.3).\n\n16 Note also that Ann’s anticipation of feeling let down might affect her initial decision. This can be modeled by letting\nAnn’s utility at (Trust, Grab) be affected by her initial beliefs and her own plan. We pursue this point and its ramiﬁcations\nin Section 6.\n17 For more on this, see Section 6.4.\n\n\f10\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n3.1. Extensive forms with observable actions\n\nWe ﬁrst restrict attention to ﬁnite multi-stage games with observable actions, no chance\nmoves, and complete information. These restrictions can be removed, at the cost of additional\nnotational complexity (see Section 6). We assume players move simultaneously at every stage.\nThis is without loss of generality, because the set of feasible actions of a player may depend\non actions chosen in previous stages and may be singleton. Simultaneous moves games, perfect\ninformation games, and repeated games are special cases (cf. Fudenberg and Tirole [34], §3.3,\nOsborne and Rubinstein [63], ch. 6). We use the following notation/terminology:\n\nAn extensive form with observable actions is a tuple (cid:6)N, H (cid:7) where N = {1, . . . , n} is the\nplayer set, and H is the ﬁnite set of feasible histories. A history of length (cid:8) is a sequence h =\n(a1, . . . , a(cid:8)) where each at = (at\nn) represents the proﬁle of actions chosen at stage t\n(1 (cid:3) t (cid:3) (cid:8)). We assume history h becomes public information as soon as it occurs. The empty\nhistory (of length 0), denoted h0, is an element of H . The set of feasible actions for player i at\nhistory h is denoted Ai(h) and may be singleton, meaning that i is not active at h. Ai(h) is empty\nif and only if h is a terminal history. Z denotes the set of terminal histories.\n\n1, . . . , at\n\nFor any given extensive form, we let Si denote the set of (pure) strategies of player i. A typical\nstrategy is denoted by si = (si,h)h∈H \\Z, where si,h is the action that would be selected by si if\ni∈N Si and S−i =\nhistory h occurred. Deﬁne S =\nj (cid:5)=i Sj . The set of i’s strategies that allow\nhistory h is denoted Si(h). Similar notation is used for strategy proﬁles: S(h) =\ni∈N Si(h);\nS−i(h) =\n\nj ∈N Sj (h). We let ζ (s) ∈ Z denote the terminal history induced by s = (si)i∈N .\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\n3.2. Conditional beliefs and inﬁnite hierarchies of beliefs\n\nHere we summarize the theory of hierarchies of conditional beliefs due to Battigalli and Sinis-\ncalchi [10], which should be consulted for proofs, details, and further references. Consider a\ndecision maker DM who is uncertain about which element in a set X is true. Assume X is a\ncompact Polish space.18 DM assigns probabilities to events E, F, . . . in the Borel sigma-algebra\nB of X according to some (countably additive) probability measure. Let Δ(X) denote the set\nof all such probability measures.19 As events unfold DM updates her beliefs. The actual and/or\npotential beliefs of DM are described by a conditional probability system (see Rênyi [70]). Let\nC ⊆ B denote the collection of potentially observable events (or conditioning events). DM holds\nprobabilistic beliefs conditional on each event F ∈ C.\n\nDeﬁnition 1. A conditional probability system (cps) on (X, B, C) is a function μ(·|·) : B × C →\n[0, 1] such that for all E ∈ B, F, F (cid:9) ∈ C\n\n(1) μ(·|F ) ∈ Δ(X),\n(2) μ(F |F ) = 1,\n(3) E ⊆ F (cid:9) ⊆ F implies μ(E|F ) = μ(E|F (cid:9))μ(F (cid:9)|F ).\n\n18 A topological space X is Polish if it admits a compatible metric d such that (X, d) is a complete and separable metric\nspace (see, e.g., [46], p. 13).\n19 Note that (X, B) is a standard Borel space (see, e.g., [46], Deﬁnition 12.5).\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n11\n\nWe regard the set of cps’ on (X, B, C) as a subset of the topological space [Δ(X)]C , where\nΔ(X) is endowed with the topology of weak convergence of measures and [Δ(X)]C is endowed\nwith the product topology.\n\nFrom now on DM is a player i; (X, B, C) is either X = S−i (a ﬁnite set) or X = S−i × Y\nwhere Y is a compact Polish space typically representing a set of opponents’ beliefs. The Borel\nsigma-algebra B is implicitly understood,20 and conditioning events corresponds to histories,\ni.e., C = {F ⊆ S−i × Y : F = S−i(h) × Y, h ∈ H } (or C = {F ⊆ S−i: F = S−i(h), h ∈ H } if\nX = S−i ). The set of cps’ is denoted ΔH (S−i × Y ) a subset of [Δ(S−i × Y )]H . If conditioning\nevent F corresponds to history h, then we abbreviate as μ(·|F ) = μ(·|h). Note that our speciﬁ-\ncation of the conditioning events relies on interpreting sj as an objective description of how j\nwould behave at each decision node. However, we will also interpret sj as a plan in the mind of\nplayer j . The implicit assumption underlying our analysis (as well as most papers on interactive\nepistemology in games) is that each player has correct beliefs, given by his plan of action, about\nhow he would choose at different histories (and there is common certainty of this). This is the\nreason why, like GPS (and Battigalli and Siniscalchi [11]), we do not explicitly model i’s beliefs\nabout his own behavior. We will come back to this in Section 6.\n\nThe following result shows that ΔH (S−i × Y ) is a compact Polish space, just like Y .21 It is key\nin our construction of hierarchical beliefs, implying that the domains of higher- and lower-order\nuncertainty have the same structural properties.\n\nLemma 2. ΔH (S−i) is a compact Polish space. Furthermore, if Y is a compact Polish space,\nalso ΔH (S−i × Y ) is a compact Polish space.\n\nHierarchies of cps’ are deﬁned recursively as follows:\n\n• X0\n−i\n• Xk\n−i\n\n= S−i (i ∈ N ),\n(cid:2)\n= Xk−1\n−i\n\n×\n\nj (cid:5)=i ΔH (Xk−1\n\n−j ) (i ∈ N ; k = 1, 2, . . .).\n\nBy repeated applications of Lemma 2, each Xk\n\n−i is a cross-product of compact Polish spaces,\n∈ ΔH (Xk−1\n−i ) is called k-order cps. For k > 1, μk\nhence compact Polish itself.22 A cps μk\ni is a\ni\njoint cps on the opponents’ strategies and (k − 1)-order cps’. A hierarchy of cps’ is a countably\n(cid:2)\nk>0 ΔH (Xk−1\ni , μ2\n−i ). μi is coherent if the cps’ of\ninﬁnite sequence of cps’ μi\ndistinct orders assign the same conditional probabilities to lower-order events:\n\ni , . . .) ∈\n\n= (μ1\n\nμk\ni (·|h) = margXk−1\n\n−i\n\nμk+1\ni\n\n(·|h)\n\n(k = 1, 2, . . . ; h ∈ H ).\n\nIt can be shown that a coherent hierarchy μi induces a cps νi on the cross-product of S−i with\nthe sets of hierarchies of cps’ of i’s opponents, a compact Polish space.\n\nHowever, νi may assign positive probability (conditional on some h) to opponents’ incoher-\nence. To rule this out, say that a coherent hierarchy μi satisﬁes belief in coherency of order 1\nif the induced cps νi is such that each νi(·|h) (h ∈ H ) assigns probability one to the opponents’\n\n20 B obtains from the product of the discrete topology on S−i and the topology of Y .\n21 This depends on two facts: (1) the collection of conditioning events for player i (corresponding to H ) is at most\ncountable (indeed ﬁnite), and (2) each conditioning event S−i (h) × Y (or S−i (h) if X = S−i ) is both closed and open,\nas we consider the discrete topology on S−i .\n22 The cross-product of countably many compact Polish spaces is also compact Polish.\n\n\f12\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\ncoherency; μi satisﬁes belief in coherency of order k if it satisﬁes belief in coherency of order\nk − 1 and the induced cps νi is such that each νi(·|h) (h ∈ H ) assigns probability one the op-\nponents’ coherency of order k − 1; μi is collectively coherent if it satisﬁes belief in coherency\nof order k for each positive integer k. The set of collectively coherent hierarchies of player i is\na compact Polish space, denoted by Mi . We let M k\ni denote the set of k-order beliefs consistent\nwith collective coherency, that is, the projection of Mi on ΔH (Xk−1\nj (cid:5)=i M k\n−i ), and let M k\nj ,\n−i\nM−i =\n\nj (cid:5)=i Mj , M =\n\nj ∈N Mj .\n\n(cid:2)\n\n(cid:2)\n\n(cid:2)\n\n=\n\nWe have now deﬁned all components of the domain of the utility functions. But is this enough\nfor the analysis of strategic reasoning? In order to decide on the best course of action, player i\nmay need to form (conditional) beliefs about the inﬁnite hierarchies of (conditional) beliefs of\nother players, either because they affect his payoff or because his assessment of the behavior and\nﬁnite-order beliefs of others is derived from assumptions, such as “common belief in rationality,”\ninvolving beliefs of inﬁnitely many orders. Does this mean that we need additional layers of\nbeliefs? No. The following result shows that the countably inﬁnite hierarchies of cps’ deﬁned\nabove are sufﬁcient for the strategic analysis; Mi is isomorphic to ΔH (S−i × M−i), so each\nμi\n\n∈ Mi corresponds to a cps on S−i × M−i :\n\nLemma 3. For each i ∈ N there is a 1-to-1 and onto continuous function\n\nfi = (fi,h)h∈H : Mi → ΔH (S−i × M−i)\n\nwhose inverse is also continuous. Furthermore, each coordinate function fi,h is such that for all\nμi\n\ni , . . .) ∈ Mi , k (cid:2) 1,\n\n= (μ1\n\ni , μ2\n\nμk\ni (·|h) = margS−i ×M 1\n\n−i\n\n×···×M k−1\n−i\n\nfi,h(μi).\n\n3.3. Psychological games\n\nWe are now ready to state our deﬁnition of a psychological game:\n\nDeﬁnition 4. A psychological game based on extensive form (cid:6)N, H (cid:7) is a structure Γ =\n(cid:6)N, H, (ui)i∈N (cid:7) where ui : Z × M × S−i → R is i’s (measurable and bounded) psychological\npayoff function.\n\n2(·|Trust); 23 in Γ3, u2 depends on z and 1’s initial ﬁrst-order belief, μ1\n\nThe numerical examples examined in Section 2 ﬁt this deﬁnition: in Γ2, u2 depends on z\n1(·|h0); ﬁnally, the\nand μ2\npsychological payoff function u2 proposed to analyze Γ4 (a game with material payoffs) depends\non z, μ1\n\n1(·|h0), and s1.\n\nIn all these examples, a psychological game is obtained from a material payoff game\n(cid:6)N, H, (πi : Z → R)i∈N (cid:7) according to some formula. We now illustrate a few such derivations,\nfocusing on two-player games.\n\n2(·|Trust) is the conditional second-order belief of player 2 used to compute the expectation β of the probability α\n\n23 μ2\ninitially assigned by 1 to the strategy ‘Share if Trust.’\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n13\n\nPlayer j is ‘let down’ if his actual material payoff, denoted ˆπj , is lower than the payoff he\ninitially expected to get, Ej [(cid:5)πj ]. This disappointment can be measured by the following expres-\nsion:24\n\n(cid:6)\n0,\n\n(cid:7)\nEj [(cid:5)πj ] − ˆπj\n\n(cid:8)(cid:9)\n\n.\n\nmax\n\nSuppose that i likes his material payoff but dislikes disappointing j . A simple way to model such\n“guilt” motivation is to assume that i wants to maximize the expected value of the following\nexpression:\n\n(cid:6)\nˆπi − θi max\n0,\n\n(cid:7)\nEj [(cid:5)πj ] − ˆπj\n\n(cid:8)(cid:9)\n.\n\nTaking explicitly into account the material payoff game, and that Ej [(cid:5)πj ] is a function of j ’s plan\nand his initial beliefs about i’s plan, we obtain the following utility function:\n\n(cid:10)\nui(z, μ, sj ) = πi(z) − θi max\n\n0,\n\n(cid:11)(cid:12)\n\n(cid:7)\n\n(cid:9)\ns\ni\n\n(cid:8)\n\n(cid:13)\n(cid:13)h0\n\nμ1\nj\n\n(cid:7)\n\nπj\n\nζ\n\n(cid:8)(cid:8)\n\n(cid:7)\n(cid:9)\nsj , s\ni\n\n− πj (z)\n\n(cid:14)(cid:15)\n\ns(cid:9)\ni\n\nwhere θi (cid:2) 0 is a psychological sensitivity parameter. For the special case of material payoff\ngame Γ1, we obtain psychological game Γ3 by letting θ1 = 0 and θ2 = 5\n\nAnother motivation is the willingness to give up some material payoff to avoid feeling regret\nex post. The regret of i can be captured by the distance between the actual material payoff and\nthe maximal expected payoff that could have been obtained ‘with the beneﬁt of hindsight,’ i.e.,\nusing the terminal beliefs. Formally, i’s regret when he gets material payoff ˆπi and has terminal\nbelief τi ∈ Δ(Sj ) equals\n(cid:12)\n(cid:7)\nζ\n\n(cid:8)(cid:8)\n\n− ˆπi.\n\n(cid:7)\n(cid:9)\ns\nj\n\n(cid:8)\nπi\n\nτi\n\n(cid:7)\n(cid:9)\nsi, s\nj\n\n2 .25\n\nmax\nsi\n\ns(cid:9)\nj\n\nTaking into account that the actual payoff obtains at a terminal history z, which is observed by\nplayer i with ﬁrst-order cps μ1\ni , we obtain the utility function\n(cid:11)\n(cid:13)\n(cid:8)(cid:8)\n(cid:13)z\n\nui(z, μ, sj ) = πi(z) − θi\n\n(cid:14)\n− πi(z)\n\n(cid:7)\nζ\n\n(cid:12)\n\nπi\n\n(cid:8)\n\n(cid:7)\n(cid:9)\nsi, s\nj\n\n(cid:7)\n(cid:9)\ns\nj\n\nμ1\ni\n\nmax\nsi\n\ns(cid:9)\nj\n\nwhere θi (cid:2) 0 is a psychological sensitivity parameter.26 This shows that it may be natural to let\nutility depend on what players believe at the end of the game (for other examples of this kind see\nSubsection 6.2).\n\nNote that while we allow for a general functional form with very complex arguments,\n\nui(z, μ, sj ), the utility functions used in speciﬁc applications can be relatively simple.\n\n24 Bell [13] and Loomes and Sugden [57] present decision-theoretic models of belief-dependent disappointment. What\nwe have here is a game-theoretic extension. In this section we use it as input to formulate guilt (see next paragraph).\nConcern for own disappointment may naturally depend on one’s own plan, which is excluded under Deﬁnition 4 but\nconsidered again in Section 6.3.\n25 The modeling choices here reﬂect the discussion of R2 in Section 2 and are in line with Battigalli and Dufwenberg’s\n[8] notion of simple guilt to whom we refer for more guilt examples (the mathematical formulation in that paper is\nslightly more complex, but leads to a utility with the same best response correspondence).\n26 Bell [12] and Loomes and Sugden [56] develop theories of regret, in which a decision maker’s experienced utility\ndepends on the post-choice revelation of a state-of-nature. Our formulation preserves that spirit, but extends it to belief-\ndependent motivation. This is natural in a strategic setting, where players cannot perfectly observe ex post the state of the\nworld, which includes what another player would have chosen.\n\n\f14\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nSo far we have illustrated three forms of belief-dependent motivation which all relied on\n(different forms of) ﬁrst-order beliefs (own initial, others’ initial, own and/or others’ terminal).\nOf course, Deﬁnition 4 allows also for higher-order belief-dependence. One example is Bat-\ntigalli and Dufwenberg’s [8] notion of “guilt from blame,” which involves dependence on the\nco-player’s third-order beliefs.27 The rest of this section leads up to another class of examples,\nwhich involve inﬁnite-order beliefs.\n\nAs we noted in Section 2, letting beliefs and strategies of others in ui may yield simpler\nfunctional forms and clarify the distinction between prediction and psychological motivation.\nYet, as reﬂected by Γ2 and Γ3 it is always possible to obtain an equivalent functional form where\na player’s utility depends only on the terminal history and his own conditional beliefs. Given\nui(z, μ, s−i) let\n\nˆui(z, μi) := Eμi\n\n[ui|z]\n\n[ui|z] is the conditional expectation operator given cps fi(μi) ∈ ΔH (S−i × M−i) (see\nwhere Eμi\nLemma 3). In words, ˆui(z, μi) is how much i values z after he has observed it. It can be shown\nthat ˆui and ui yield the same (sequential) best responses.28\n\nThe functional form ˆui has inﬁnite hierarchy μi as an argument, but this is just because\nwe want an abstract and general expression. If ui depends only on beliefs of order k, then ˆui\ndepends only on beliefs of order k + 1. An example that necessarily involves inﬁnite-order belief-\ndependence arises if belief-dependent motivation appears together with ‘interactive altruism.’\nSuppose that the utility of i is given by two terms as follows\n\nui = φi + θ uj\n(cid:7)\nz, μ1, sj\nφi = φi\n\n(0 < θ < 1),\n(cid:8)\n\n(φi could be a guilt or regret component). Then we can compute an explicit form for the expected\nutility conditional on z by repeated substitution, where the term that depends on (k + 2)-order\nbeliefs has exponentially decreasing weight θ k:\n\nˆui(z, μi) = Eμi\n= Eμ2\n= Eμ2\n= · · ·\n\ni\n\ni\n\n[φi + θ uj |z]\n[φi|z] + θ Eμi\n[φi|z] + θ Eμ3\n\ni\n\n[uj |z]\n(cid:16)\n\nE(cid:5)μ2\n\nj\n\n(cid:13)\n(cid:13)z\n[φj |z]\n\n(cid:17)\n\n+ θ 2Eμi\n\n(cid:16)\nE(cid:5)μj\n\n(cid:17)\n\n(cid:13)\n(cid:13)z\n[ui|z]\n\n(Eμk\n\ni\n\n[·|z] is the conditional expectation operator given cps μk\n\ni ).\n\n4. Equilibrium analysis\n\nKreps and Wilson’s sequential equilibrium has become a benchmark for the analysis of stan-\ndard games. We extend this concept to the class of psychological games deﬁned in Section 3.\n(The restriction to multi-stage game forms with complete information simpliﬁes but is not essen-\ntial; cf. Section 6.) We next deﬁne and interpret mixed strategies and assessments (4.1), give the\nmain deﬁnition and provide an existence theorem (4.2), and consider examples (4.3).\n\n27 To save space we refer to the original source for precise deﬁnitions and examples. The intuition is that i experiences\nguilt (judging by his terminal beliefs) to the extent that j ’s terminal beliefs indicate that i intended to disappoint j .\n28 See the ﬁrst part of Appendix A and Lemma 17.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n15\n\n4.1. Randomized strategies and consistent assessments\n\n(cid:2)\n\nThe equilibrium concept we develop refers to randomized choices. However, in our interpre-\ntation, we exclude actual randomization. Rather, we interpret a randomized choice of a given\nplayer i as the common ﬁrst-order belief of i’s opponents about i (cf. Aumann and Branden-\nburger [4]). This is akin to the following characterization of a Nash equilibrium in a standard\nsimultaneous-move game: (σ1, . . . , σn) ∈ Δ(A1) × · · · × Δ(An) is an equilibrium if, for each i,\neach action in the support of σi is a best reply to σ−i .\n\nWe focus on behavior strategies σi = (σi(·|h))h∈H \\Z ∈\n\nh∈H \\Z Δ(Ai(h)), interpreting σi as\nan array of common conditional ﬁrst-order beliefs held by i’s opponents. This interpretation is\npart of the notion of ‘consistency’ of proﬁles of strategies and hierarchical beliefs deﬁned below.\nKreps and Wilson [51] argue that an appropriate deﬁnition of equilibrium in extensive form\ngames must refer to ‘assessments’: proﬁles of (behavior) strategies and conditional (ﬁrst-order)\nbeliefs. They proceed in two steps: ﬁrst a ‘consistency’ condition for assessments is put for-\nward, and then sequential equilibrium is deﬁned as a consistent assessment satisfying sequential\nrationality. It turns out that consistency captures the assumptions that each player regards his\nopponents’ choices at different histories as stochastically independent, and any two players have\nthe same (prior and conditional) beliefs about any third player. In our framework an assessment\nis a proﬁle (σ, μ) =(σi, μi)i∈N where σ is a behavioral strategy proﬁle and μ ∈ M. We extend\nthe deﬁnition of consistency by adding a requirement concerning the higher-order beliefs that\nneed to be speciﬁed in psychological games.\n\nLet Prσj (·| ˆh) ∈ Δ(Sj ( ˆh)) denote the probability measure over j ’s strategies conditional on ˆh\n\nderived from behavior strategy σj under the assumption of independence across histories:\n\n(cid:4)\n\n∀sj ∈ Sj ( ˆh),\n\nPrσj (sj | ˆh) :=\n\nσj (sj,h|h)\n\n(h ⊀ ˆh means that h is not a predecessor, or preﬁx, of ˆh).29\n\nh∈H \\Z:h⊀ ˆh\n\ni )i∈N is derived from a behavioral strategy\n\nDeﬁnition 5. A proﬁle of ﬁrst-order cps’ μ1 = (μ1\nproﬁle σ = (σi)i∈N if for all i ∈ N , s−i ∈ S−i , ˆh ∈ H ,\nPrσj (sj | ˆh).\n\ni (s−i| ˆh) =\nμ1\n\n(cid:4)\n\n(1)\n\nj (cid:5)=i\n\nClearly, if μ1 is derived from σ then for any three players i, j , k, the beliefs of i and j about\n\nk coincide:\n\n∀ ˆh ∈ H, margSk μ1\n\nj (·| ˆh).\nWe are now ready for the main deﬁnition of this subsection.\n\ni (·| ˆh) = Prσk (·| ˆh) = margSk μ1\n\nDeﬁnition 6. Assessment (σ, μ) is consistent if\n\n(a) μ1 is derived from σ ,\n\n29 Cf. Kuhn [53]: Prσj (·|h) ∈ Δ(Sj (h)) is only one out of the many probability measures that induce the same out-\ncome probabilities starting from h, for all s−j ∈ S−j (h). But note that realization-equivalent beliefs may yield different\npsychological utilities!\n\n\f16\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n(b) higher-order beliefs in μ assign probability 1 to the lower-order beliefs:\n\n∀i ∈ N, ∀k > 1, ∀h ∈ H, μk\n\ni (·|h) = μk−1\n\n(·|h) × δμk−1\n\ni\nwhere × denotes the product of measures and δx is the Dirac measure assigning probability 1\nto singleton {x}.\n\n−i\n\nA justiﬁcation of the (strong) condition (b) comes from (i) the classical interpretation of equi-\nlibrium beliefs as the end-product of a transparent reasoning process by intelligent players, and\n(ii) the trembling hand assumption underlying the sequential equilibrium concept.30 (i) implies\nthat any two players must share the same initial ﬁrst-order beliefs about any other player, and\nevery player comes to a correct conclusion about the (hierarchical) beliefs of his opponents be-\ncause he is able to replicate their reasoning. (ii) implies that unexpected moves are explained\nas mistakes, not as the result of unexpected beliefs, therefore players never change their beliefs\nabout the conditional beliefs that the opponents would hold at each h. Of course, by observing\nthe actual play-path each player infers the current actual beliefs of his opponents, but interesting\nforms of learning about others’ beliefs are ruled out. We offer further comments on consistency\nand sequential equilibrium in Section 6.4.\n\nWe also note that (b) is analogous to a condition used by GPS to deﬁne psychological Nash\nequilibrium, requiring that players hold common, correct beliefs about each others’ beliefs. In-\ndeed, (b) is equivalent to the requirement that, for each player i and each history h, the conditional\nbelief on S−i × M−i induced by hierarchy μi assigns probability one to μ−i .31\n\n4.2. Sequential equilibrium assessments\n\nWe now move to the section’s main deﬁnition: a consistent assessment is a sequential equi-\nlibrium if it satisﬁes sequential rationality. Formally, ﬁx a hierarchy of cps’ μi a (non-terminal)\nhistory h and a strategy si consistent with h. The expectation of ui conditional on h, given si and\nμi is\n\n(cid:7)\nζ (si, s−i), μi, μ−i, s−i\n\n(cid:8)\nfi,h(μi)(ds−i, dμ−i).\n\nui\n\n(2)\n\n(cid:3)\n\nEsi ,μi\n\n[ui|h] :=\n\nS−i ×M−i\n\nDeﬁnition 7. An assessment (σ, μ) is a sequential equilibrium (SE) if it is consistent and for all\ni ∈ N , h ∈ H \\Z, s∗\ni\n(cid:7)\n∗\ns\ni\n\n∈ Si(h),\n∗\n> 0 ⇒ s\ni\n\n[ui|h].\n\n(cid:8)\n|h\n\nEsi ,μi\n\nPrσi\n\n(3)\n\n∈ arg max\nsi ∈Si (h)\n\nNote that, by consistency, σi represents the ﬁrst-order beliefs of i’s opponents about i, and\nfurthermore there is common certainty of the true belief proﬁle μ at every history; therefore the\nsequential rationality condition (3) can equivalently be written as\n\n∀j (cid:5)= i,\n\nsupp margSi μ1\n\nj (·|h) ⊆ arg max\nsi ∈Si (h)\n\ns−i ∈S−i (h)\n\n(cid:12)\n\nμ1\ni (s−i|h)ui\n\n(cid:7)\nζ (si, s−i), μ, s−i\n\n(cid:8)\n.\n\n(4)\n\n30 Kreps and Wilson [51] show that (in standard games) sequential equilibrium is generically equivalent to trembling\nhand perfect equilibrium.\n31 That is, (b) holds iff ∀i ∈ N , ∀h ∈ H , fi,h(μi )(S−i × {μ−i }) = 1.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n17\n\nThis clariﬁes that SE is a notion of equilibrium in beliefs. Indeed we could have given an equiv-\nalent deﬁnition of SE with no reference to behavioral strategies.\n\nWe can also take the point of view of an ‘agent’ (i, h) of player i, in charge of the move at\nhistory h, who seeks to maximize i’s conditional expected utility given the consistent assessment\n(σ, μ). The expected utility of i conditional on h and ai ∈ Ai(h) given (σ, μ) can be expressed\nas\n\n(cid:12)\n\n(cid:4)\n\n(cid:12)\n\nEσ,μ[ui|h, ai] :=\n\nPrσj (sj |h)\n\nPrσi (si|h, ai)ui\n\n(cid:7)\nζ (s), μ, s−i\n\n(cid:8)\n,\n\n(5)\n\ns−i ∈S−i (h)\n(cid:2)\n\nj (cid:5)=i\n\nsi ∈Si (h,ai )\n\nh(cid:9)∈H \\Z:h(cid:9)/(cid:2)h σi(si,h(cid:9) |h(cid:9)) (h(cid:9) /(cid:4) h means that h(cid:9) is not h or a predecessor\nwhere Prσi (si|h, ai) :=\nof h). This speciﬁcation presumes that (i, h) assesses the probabilities of actions by other agents\nof player i in the same way as each player j (cid:5)= i; that is using the behavioral strategy σi .32 It can\nbe shown that a version of the One-Shot-Deviation (or unimprovability) principle holds in our\nframework33:\n\nProposition 8. A consistent assessment (σ, μ) satisﬁes (3) and hence is an SE if and only if for\nall i ∈ N , h ∈ H \\Z,\n(cid:8)\n(cid:7)\nσi(·|h)\n\nEσ,μ[ui|h, ai].\n\n⊆ arg max\n\nsupp\n\n(6)\n\nai ∈Ai (h)\n\nProof. Available on request. (cid:2)\n\nWe obtain the following existence theorem:\n\nTheorem 9. If the psychological payoff functions are continuous, there exists at least one se-\nquential equilibrium assessment.\n\nThe proof relies on a “trembling hand” argument (cf. Selten [74]). We only provide\na sketch here (but details are available on request). Consider ε-perturbed games where\nthere is strictly positive minimal probability of choosing any action at any history, i.e. ε =\n(εi,h(ai, h)ai ∈Ai (h))i∈N,h∈H is a strictly positive vector such that\nai ∈Ai (h) ε(ai, h) < 1 for\neach h. For each behavior strategy proﬁle σ , let μ = β(σ ) denote the unique proﬁle of hierarchies\nof cps’ such that (σ, μ) is consistent.34 Deﬁne an (agent-form, psychological) ε-equilibrium as\nan ε-constrained behavior strategy proﬁle σε such that for each h and each i, a pure action ai\nthat does not maximize the expectation of ui (given h and β(σε)) is assigned the minimal prob-\nability ε(ai, h) > 0. It can be shown by standard compactness-continuity arguments that each\nε-perturbed game has an ε-equilibrium (cf. the existence proof for psychological Nash equilibria\nin GPS). Fix a sequence εk → 0 and a corresponding sequence σ k of εk-equilibrium assess-\nments. By compactness, σ k has an accumulation point σ ∗. By upper-hemicontinuity of the local\n\n(cid:18)\n\n32 Suppose that ui depends only on terminal histories and beliefs, not on s−i . Then we obtain the more familiar formula\n\nEσ,μ[ui |h, ai ] =\n\nPrσ (z|h, ai )ui (z, μ),\n\n(cid:12)\n\nz\n\nwhere Prσ (z|h, ai ) is the probability of terminal history z conditional on (h, ai ) determined by σ .\n33 See Section 6.3 for further discussion.\n34 (μ1\norder beliefs.\n\ni )i∈N is derived from σ via formula (1) and the inﬁnite hierarchies of cps’ are obtained by assuming correct higher\n\n\f18\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nbest response correspondences, for each (i, h), σ ∗\ni (·|h) assigns positive probability only to ac-\ntions that are best responses to (σ ∗, β(σ ∗)) at h. So, by Proposition 8 (σ ∗, β(σ ∗)) is a sequential\nequilibrium assessment.\n\nWe next show that the SE concept generalizes subgame perfect equilibrium for standard games\nwith observable actions (recall: sequential and subgame perfect equilibrium coincide in games\nwith observable actions). This is a corollary of a more general result for games where psycho-\nlogical utilities depend only on terminal nodes and beliefs: ui : Z × M → R. For any such game\nΓ = (cid:6)N, H, (ui)i∈N (cid:7) and any proﬁle of hierarchies of cps’ μ = (μi)i∈N , we can obtain a stan-\ndard game Γ μ = (cid:6)N, H, (vμ\n\ni )i∈N (cid:7) with payoff functions vμ\n\ni (z) = ui(z, μ).\n\nRemark 10. Suppose ∀i ∈ N , ui : Z × M → R. Then an assessment (σ, μ) is a SE if and only if it\nis consistent and σ is a subgame perfect (hence sequential) equilibrium of the standard game Γ μ.\n\n4.3. Examples\n\nWe illustrate the SE concept with three examples, ﬁrst a simultaneous move game illustrating\nhow we can reproduce the essence of a leading example of GPS, then two versions of the Trust\nGame connecting back to some of the key notions previously highlighted in Section 2.\n\n4.3.1. Equilibrium beliefs in the Bravery Game\n\nThe Bravery Game is a numerical example used in GPS (p. 66) to show that a psychological\ngame may have multiple, isolated mixed strategy equilibria even if there is only one active player,\nwhich is impossible in standard games. We consider a modiﬁed version to illustrate our deﬁnition\nof equilibrium in beliefs. Let A1 = {Wait}, A2 = {bold, timid}. Player 1 (Ann) is inactive so we\ncan ignore her payoffs, but her beliefs matter. Player 2 (Bob) is concerned about what Ann\nthinks about him. Acting boldly is dangerous, but worthwhile if Ann expects Bob to act boldly.\nGPS model the situation with a payoff function of the form u2 : A × M2 → R. Speciﬁcally, let\n1(bold|h0) denote Ann’s ﬁrst-order belief about Bob (a random variable from Bob’s point\nα := μ1\nof view), and let β :=\n1) denote (a feature of) the Bob’s second-order beliefs. GPS’\npayoff function is\n\n2(dμ1\n\nαμ2\n\n(cid:19)\n\nu2(a2, μ2) =\n\n2 − β,\n3(1 − β),\n\nif a2 = bold,\nif a2 = timid.\n\nWe modify u2, considering instead u2 : A × M → R deﬁned by\n\n(cid:10)\n\n(cid:10)\n\nu2(a2, μ) =\n\n2 − α,\n3(1 − α),\n\nif a2 = bold,\nif a2 = timid.\n\nClearly, the expectation of u2 given a2 and Bob’s second-order belief β is u2. There are three\nequilibria, with β = α = 1, β = α = 0 and β = α = 1\n\n2 .35\n\n4.3.2. Trust game with guilt aversion\n\nConsider Γ3 in Fig. 3 (or equivalently Γ2). Recall that α (a function of μ1\nAnn assigns to strategy ‘Share if Trust’ at the beginning of the game, and β =\n\n1) is the probability\n(cid:19)\n|Trust)\n2(dμ1\n1\n\nαμ2\n\n35 These are essentially the same equilibria as those obtained by GPS. But GPS allow for actual randomization; thus\nthe ﬁrst-order beliefs of Bob are degenerate on the equilibrium (mixed) strategy of Ann, and higher-order beliefs of each\nplayer are degenerate on the equilibrium lower-order beliefs of the other player.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n19\n\nFig. 5. Trust Game Γ5 with reciprocity payoffs.\n\nis the relevant feature of the conditional second-order beliefs of Bob. We let τ = μ1\n2(Trust|h0)\ndenote Bob’s initial ﬁrst-order belief. An assessment is summarized by (τ, α, β), where (τ, α)\ncorresponds to a behavior strategy proﬁle. The indifference condition for Bob is β = 2\n5 , the\n2 ; consistency yields α = β. The game has three SEs:\nindifference condition for Ann is α = 1\nτ = α = β = 1 (trust), τ = α = β = 0 (no trust), and τ = 0, α = β = 2\n5 (insufﬁcient trust). Note\nthat only the ﬁrst equilibrium is consistent with forward induction reasoning (as described in\nSection 2, and further elaborated on in Subsection 5.1 below).\n\n4.3.3. Trust game with reciprocity\n\nOur framework is adequate for modeling reciprocity in extensive games. To support this claim,\nwe show how the essence of Dufwenberg and Kirchsteiger’s theory can be captured in Γ1: Let\nα, β and τ be deﬁned as in the previous example. The key tenets of the theory concern player\ni’s kindness to player j (Kij ), and i’s belief in j ’s kindness to i ( ˆKij i ). At each history, player i\nmaximizes utility deﬁned by the sum of material payoffs (as in Γ1) and reciprocity payoffs equal\nto θi × Kij × ˆKij i , where θi is a constant measuring i’s sensitivity to reciprocity. Assume that\n3 . One can show that Kij and ˆKij i can be\nAnn’s and Bob’s sensitivities are θ1 = 0 and θ2 = 4\nreproduced in our framework and notation; in particular we need the following:\n\n– Bob’s kindness following Trust = −1 for Grab and = 1 for Share,\n– Bob’s belief in Ann’s kindness following Trust = 3\n2\n\n− β.\n\nΓ5 in Fig. 5 displays the relevant utilities as conceived by the players when they move (Bob is\nnot active at h0, so we put no utility for him following Don’t):36\n\nApplying Deﬁnition 7, Γ5 has a unique SE with τ = 1, α = β = 3\n\n4 . No ‘pure’ SE exists,37 just\n\nlike in Dufwenberg and Kirchsteiger’s theory (cf. 6.3 below).\n\n5. Interactive epistemology\n\nWe argued in Section 2 that alternatives to equilibrium analysis are worth exploring. The\ndeﬁnition of Mi provides us with all the ingredients to analyze strategic reasoning by means of\n\n36 As with Γ2 vs. Γ3, we can replace Bob’s conditional second-order belief β with Ann’s initial ﬁrst-order belief α in\nBob’s payoffs, and get analogous conclusions.\n37 In any SE we have α = β. With θ2 = 4\nshoots up, so Bob prefers Share to Grab, which in SE would imply α = β = 1, . . . a contradiction. If α = β > 3\nˆK212 goes down, so Bob prefers Grab to Share, implying α = β = 0, . . . another contradiction.\n\n3 , the indifference condition for Bob yields β = 3\n\n4 then ˆK212\n4 then\n\n4 . If α = β < 3\n\n\f20\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\ninteractive epistemology, i.e., assumptions about players’ rationality and what they believe about\neach other at any node. We show how to express such assumptions in the language of events and\nbelief operators (5.1), and then analyze a notion of extensive form rationalizability (5.2).\n\n5.1. States of the world, events, and belief operators\n\nA state of the world speciﬁes, for each player i and history h, what i would do and believe if\nh were reached. Note the subjunctive conditional: game-theoretic analysis does not only concern\nthe actual path of actions and beliefs, but also considers how players would react (in terms of\nchoices and beliefs) to histories that do not actually occur at the true state. The state of a player is\ntherefore given by his strategy and his hierarchy of cps’, (si, μi) (as explained in Subsection 3.2\nwe interpret si both as an objective description of i’s contingent behavior and as i’s plan of\naction). The set of states for player i is denoted by Ωi = Si × Mi , and the set of states of the\nworld is Ω =\nj (cid:5)=i Ωj denote the set of possible states of i’s opponents.\nWith a slight abuse of notation we often write Ω = Ωi ×Ω−i with typical element ω = (ωi, ω−i).\nAn event is a (Borel) subset E ⊆ Ω; its complement is denoted ¬E = Ω\\E. An event about i\nis any set of states E = Ei × Ω−i , where Ei is a Borel subset of Ωi . We let Ei denote the family\nof events about i. Events about the opponents of i are similarly deﬁned; the collection of such\nevents is denoted E−i .\n\n(cid:2)\nn\ni=1 Ωi . We let Ω−i =\n\n(cid:2)\n\nWe often use brackets to denote speciﬁc events. In particular, for any function (random vari-\nable) x : Ω → X and value x∗ ∈ X, we use the notation [x = x∗]: = {ω: x(ω) = x∗}. When x is\nunderstood, we simply write [x∗]. For example, [s∗\n} ∈ Ei is the event\ni\n“i plays s∗\ni ”; here it is understood that x is the projection function x(si, μi, ω−i) = si . Similarly,\n[h] =\ni∈N Si(h) × Mi is the event that history h occurs.\n\n] = {(si, μi, ω−i): si = s∗\ni\n\n(cid:2)\n\nRecall that we follow both GPS and Battigalli and Siniscalchi [11] in disregarding play-\ners’ beliefs about themselves. At state ω = (si, μi, ω−i), player i would believe event E =\nΩi × E−i ∈ E−i conditional on history h with probability fi,h(μi)(E−i) (cf. Section 3.2). Thus\n{(si, μi, ω−i): fi,h(μi)(E−i) = 1} is the event “i would believe E conditional on h.” E may\nconcern the beliefs of i’s opponents.\n\nWe use belief operators to represent events about interactive beliefs: a belief operator for\nplayer i is a mapping with domain E−i and range Ei . For any given history h ∈ H , the h-\nconditional belief operator for i is deﬁned as follows:\n\n∀E = Ωi × E−i ∈ E−i, Bi,h(E) =\n\n(cid:6)\n(si, μi, ω−i): fi,h(μi)(E−i) = 1\n\n(cid:9)\n.\n\nh may be counterfactual at ω, because strategies played at ω may not induce h; in this case “i\nwould believe E conditional on h” is a counterfactual statement about i’s beliefs at ω. Clearly,\nBi,h(E) ∈ Ei .38 Bi,h(·) satisﬁes monotonicity [E ⊆ F implies Bi,h(E) ⊆ Bi,h(F )] and conjunc-\ntion [Bi,h(E ∩ F ) = Bi,h(E) ∩ Bi,h(F )]. Furthermore Bi,h(E) = Bi,h(E ∩ [h]) because i always\nbelieves what he observes.\n\nThe basic event we are interested in is players’ rationality. We focus on a condition that\ndoes not distinguish between realization-equivalent strategies. Say that i is rational at state\ni , μi, ω−i) iff s∗\n(s∗\n[ui|h] (deﬁned in (2)) con-\n\ni maximizes i’s conditional expected utility Esi ,μi\n\n38 For any Borel set Ωi × E−i , Bi,h(Ωi × E−i ) is also a Borel set because the h-coordinate belief function fi,h is\ncontinuous (see Lemma 3).\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n21\n\nditional on each history h allowed by s∗\ndenote the set of non-terminal histories allowed by s∗\n\ni . More formally, let Hi(s∗\n\ni ; we require si ∈ r(μi) where\n\ni ) = {h ∈ H \\Z: s∗\n\ni\n\n∈ Si(h)}\n\n(cid:20)\n∗\ni : ∀h ∈ H (s\ns\n\n∗\n∗\ni ), s\ni\n\n(cid:21)\n[ui|h]\n\nri(μi) :=\n\n∈ arg max\nsi ∈Si (h)\nThe event “player i is rational” is Ri = {(si, μi, ω−i): si ∈ ri(μi)}. It can be shown that ri(μi)\ncan be obtained via a backward induction algorithm and that Ri is a well-deﬁned nonempty event\n(cf. proof of Lemma 14 in Appendix A).\n\nEsi ,μi\n\n(7)\n\n.\n\nTo illustrate how these concepts can be used, we re-examine two psychological versions of\nthe Trust Game. As regards notation, we have to distinguish the event “Bob shares,” which in the\nextensive form implies that “Ann trusts Bob,” from the event “Bob would share if Ann trusted\nBob” which is a subjunctive conditional, logically independent on whether Ann trusts Bob or\nnot. Similar considerations hold for the action Grab. We use bold letters to denote subjunctive\nconditionals (which in this case correspond to strategies of Bob), as in [Share] and [Grab].\n\nConsider the Trust Game with guilt aversion Γ3 in Fig. 3. The game can be solved by forward\ninduction reasoning: it is rational for Ann to trust Bob only if she assigns at least 50% probability\n2 , where α : M1 → R is the random variable deﬁned by\nto strategy Share, i.e. only if α (cid:2) 1\n1(Share|h0).39 If Bob believes in Ann’s rationality when he has to move (even if he is\nα(μ1) = μ1\n‘surprised’), he infers from Ann’s action Trust that α (cid:2) 1\n2 , where β : M2 → R\nis the random variable deﬁned by β(μ2) =\nα(μ1)f2,Trust(μ2)(dμ1). His rational response is to\nshare. If Ann anticipates Bob’s reasoning she trusts him.\n\n2 . Therefore β (cid:2) 1\n\n(cid:19)\n\nThe formal counterpart of this argument is as follows (the events listed are nonempty; we rely\n\non the monotonicity of the belief operators):\n\n(cid:10)\n\nR1 =\n\n(s1, μ1, ω2): α(μ1) >\n\n(cid:10)\n\nR2 =\n\n(ω1, s2, μ2): β(μ2) >\n\n1\n2\n2\n5\n\nR1 ∩ [Trust] ⊆\n\nB2,Trust(R1) = B2,Trust\n\n,\n\n(cid:14)\n\n(cid:11)\nα (cid:2) 1\n2\n(cid:7)\n(cid:8)\nR1 ∩ [Trust]\n(cid:14)\n\n⇒ s1 = Trust, α(μ1) <\n\n⇒ s2 = Share, β(μ2) <\n\n(cid:15)\n\n1\n2\n2\n5\n\n⇒ s1 = Don’t\n\n,\n(cid:15)\n⇒ s2 = Grab\n\n,\n\n(cid:22)(cid:11)\n\n⊆ B2,Trust\n\n(cid:14)(cid:23)\n\n(cid:11)\n\n⊆\n\n(cid:14)\n\n,\n\nβ (cid:2) 1\n2\n\nα (cid:2) 1\n2\n\n(cid:11)\nβ (cid:2) 1\nR2 ∩ B2,Trust(R1) ⊆ R2 ∩\n2\n(cid:7)\n(cid:8)\n⊆ R1 ∩ [α = 1] ⊆ [Trust].\nR2 ∩ B2,Trust(R1)\n\n⊆ [Share],\n\nR1 ∩ B1,h0\n\nNow consider the Trust Game with Reciprocity Γ5 in Fig. 5. Without an equilibrium suppo-\n3 , Bob’s best response depends on whether β\n4 . This cannot be resolved by forward induction reasoning, which\n\nsition, one is at loss for predictive power: if θ2 = 4\nis below or above ( 3\n2\nyields (as explained above) β (cid:2) 1\n2 .\n\nHowever, if one uses other values of θ2 one can draw clear conclusions merely using back-\n3 , Bob’s best response (given Trust) is Grab independently of β,\n\nward induction: if θ2 < 2\n\n− 1\nθ2\n\n) = 3\n\n39 In some formulas, we have to make explicit the dependence of random variable α on the state of the world. The same\nholds for random variable β.\n\n\f22\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nthus R2 ⊆ [Grab] and R1 ∩ B1,h0 (R2) ⊆ [Don’t]; on the other hand, if θ2 > 2, R2 ⊆ [Share]\nand R1 ∩ B1,h0 (R2) ⊆ [Trust]. Furthermore, a subtle issue arises when 2\n3 < θ2 < 1: back-\nward induction cannot pin down Bob’s best response, which is Grab only if β (cid:2) ( 3\n),\n2\nwhile forward induction yields β (cid:2) 1\n2 . This puts an upper bound on how kind Bob believes\n3 < θ2 < 1 the best response is Grab (formally, R2 ∩ B2,Trust(R1) ⊆ [Grab],\nAnn is.40 With 2\nB1,h0 (R2 ∩ B1,Trust(R1)) ⊆ [α = 0] and R1 ∩ B1,h0 (R2 ∩ B1,Trust(R1)) ⊆ [Don’t]).\n\n− 1\nθ2\n\nOne can show that the SE prediction implies 0 < α = β = ( 3\n2\n\n2 , τ = 0. Thus, SE and\nforward induction reasoning yield the same path, but very different predictions about how Bob\nwould revise his beliefs off that path.41\n\n− 1\nθ2\n\n) < 1\n\n5.2. Rationalizability\n\nThe basic rationalizability concept for standard games is equivalent to iterated strict domi-\nnance and is motivated by the assumption that players are rational and there is common belief\nin rationality. Several modiﬁcations of rationalizability have been proposed, to handle sequen-\ntial rationality and to reﬂect alternative epistemological assumptions.42 In psychological games\npayoffs are affected by hierarchical beliefs, so rationalizability has to be deﬁned as a property\nof a whole state of the world rather than of strategies. One could, e.g., stipulate that a state\nω = (si, μi)i∈N is rationalizable if at ω players are rational and there is common belief in ratio-\nnality at the beginning of the game.43\n\nOne could go on to examine an array of modiﬁcations. That is not our goal. Rather, we wish\nto indicate that the class of psychological games we have deﬁned is amenable to interactive epis-\ntemology analysis in principle, and to illustrate the potential cutting power of such an approach.\nWe provide tools to perform a particular forward-induction analysis. Following Battigalli and\nSiniscalchi [11], we ﬁrst deﬁne a ‘strong belief operator’ SBi as follows: SBi(∅) = ∅ and\n\n∀E ∈ E−i\\{∅},\n\nSBi(E) =\n\n(cid:24)\n\nBi,h(E).\n\n[h]∩E(cid:5)=∅\n\nIn words, SBi(E) is the event “player i would believe E conditional on every history that does\nnot contradict E”44; e.g., SBi([sj ]) is the event “player i would believe player j plays sj at each\nhistory h allowed by sj .”\n\nWe are interested in events of the form SBi(R−i ∩ E), where R−i =\n\nj (cid:5)=i Rj is the event that\ni’s opponents are rational and E is either Ω or some event concerning beliefs, and we consider\nassumptions like “everybody strongly believes that the opponents are rational.” To write this\nconcisely, we deﬁne a mutual strong belief operator. Let E denote the collection of events of the\nform E =\ni∈N Ei ∈ E, the\ni∈N Ei (Ei ∈ Ei ). For example, R =\n(cid:25)\nevent “mutual strong belief in E” is SB(E) =\n\ni∈N Ri ∈ E. For each E =\n\nj (cid:5)=i Ej ). Note that SB(E) ∈ E.\n\ni∈N SBi(\n\n(cid:25)\n\n(cid:25)\n\n(cid:25)\n\n(cid:25)\n\n(cid:25)\n\n40 The higher β, the more Bob believes that Ann’s choice to trust him is self-interested.\n41 This can happen in standard games too, but for different reasons and with more complex extensive forms (see, e.g.,\nReny, 1992, [69]). Furthermore, we will show in Section 6 that in some psychological games SE and forward induction\nyield different paths!\n42 See, e.g., Battigalli and Bonanno [6], Asheim [2], and references therein.\n43 Here is an exact deﬁnition: for every event E =\nk(cid:3)1 Bk(E)), where Bk(E) = Bk−1(E).\nω ∈ R ∩ (\n44 SBi (·) is not a monotone operator, and satisﬁes only a weak form of conjunctiveness [SBi (E) ∩ SBi (F ) ⊆\nSBi (E ∩ F )]. For more on this, see [11].\n\ni∈N Ei , Ei ∈ Ei , let B(E) =\n\nj (cid:5)=i Ej ). Require that\n\ni∈N Bi,h0 (\n\n(cid:25)\n\n(cid:25)\n\n(cid:25)\n\n(cid:25)\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n23\n\nWe explore the consequences of the following assumptions:\n\n(0) each player is rational [ = R],\n(1) mutual strong belief in (0) [ = SB(R)],\n(2) mutual strong belief in (0) & (1) [ = SB(R ∩ SB(R))],\n(3) mutual strong belief in (0), (1) & (2) [ = SB(R ∩ SB(R ∩ SB(R)))],\n\nand so on. . . . Such assumptions are more easily expressed with formulas if we introduce an\nauxiliary ‘correct strong belief’ operator:\n\n∀E ∈ E, CSB(E) = E ∩ SB(E).\n\nThe conjunction of assumptions (0)–(k) corresponds to the event CSBk(R), where for any E ∈ E,\nCSB0(E) = E and CSBk(E) = CSB(CSBk−1(E)).45 Rationalizability is deﬁned by considering\nthe limit as k → ∞:\n\nDeﬁnition 11. A state of the world ω is rationalizable if ω ∈\n\n(cid:25)\n\nk(cid:3)0 CSBk(R).\n\nBattigalli and Siniscalchi [11] show that the strategies consistent with event CSBk(R) in\nstandard games are those surviving the ﬁrst k + 1 steps of Pearce’s [65] extensive-form ratio-\nnalizability procedure. This explains the terminology of Deﬁnition 11. To illustrate the concept,\nwe note that it captures the forward induction solution of the Trust Game with guilt aversion\n(either Γ2 or Γ3). However, that conclusion requires only two layers of mutual correct strong\nbelief: the solution obtains at all states ω ∈ CSB2(R).\n\nTo illustrate the full power of Deﬁnition 11, we analyze a Generalized Trust Game with\nguilt aversion, reminiscent of Ben-Porath and Dekel’s [14] money-burning game: Ann can ei-\nther (evenly) distribute the total surplus of $2 (action D), or reinvest it in one out of L projects.\nProject (cid:8) = 1, . . . , L yields 2(1 + (cid:8)\nL ). Bob controls surplus distribution. He can either Grab\nor (evenly) Share. Trust(cid:8) denotes the action of investing in project (cid:8), and Share(cid:8) denotes\n1(Share(cid:8)|h0)\nthe conditional choice of sharing if Ann invests in project (cid:8). Let α(cid:8)(μ1) = μ1\nand β(cid:8)(μ2) =\n1)|Trust(cid:8)). Assume that Ann’s utility is her material payoff,\nwhereas Bob is averse to guilt. Applying the guilt formula of Subsection 3.3, the players’ utili-\nties are given by\n\n2(dα(cid:8)(μ1\n\nα(cid:8)(μ1\n\n1)μ2\n\n(cid:19)\n\nui(D) = 1,\n\ni = 1, 2,\n(cid:22)\n\nui(Trust(cid:8), Share) =\n\n1 + (cid:8)\nL\n\n(cid:23)\n\n,\n\ni = 1, 2,\n\nu1(Trust(cid:8), Grab) = 0,\n(cid:22)\n1 + (cid:8)\nL\n\nu2(Trust(cid:8), Grab) = 2\n\n(cid:23)\n\n− θ2α(cid:8)\n\n(cid:22)\n\n(cid:23)\n\n,\n\n1 + (cid:8)\nL\n\nBob (strictly) prefers to share the yield of project (cid:8) if and only if θ2β(cid:8) > 1. For L = 1 and θ2 = 5\n2\nwe obtain Γ3, and the forward induction argument works if and only if θ2 > 2. When L > 1\nrationalizability yields the efﬁcient sharing outcome also for much lower values of θ2:\n\n45 For example, (0) & (1) & (2) is R ∩ SB(R) ∩ SB(R ∩ SB(R)) = CSB2(R).\n\n\f24\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nProposition 12. In the Generalized Trust Game with guilt aversion, if θ2 > 1 + 1\nevery rationalizable state (s1, μ1, s2, μ2), s1 = TrustL, s2 = (Share(cid:8))L\n((cid:8) = 1, . . . , L).\n\nL then, for\n(cid:8)=1, α(cid:8)(μ1) = β(cid:8)(μ2) = 1\n\nProof. Available on request. (cid:2)\n\nOur extension of Pearce’s solution concept is well behaved:\n\nTheorem 13. If psychological utilities are continuous the set\nstates is nonempty and compact.\n\n(cid:25)\n\nk(cid:3)0 CSBk(R) of rationalizable\n\nProof. By deﬁnition\n\n(cid:8)\n(cid:7)\nCSBk+1(R) = CSB\nCSBk(R)\n\n(cid:8)\n(cid:7)\nCSBk(R)\n= CSBk(R) ∩ SB\n(cid:25)\nk\nWe prove by induction that each element CSBk(R) =\n(cid:8)=0 CSB(cid:8)(R) of the nested sequence\n{CSBk(R)}k(cid:3)0 is closed and nonempty. Lemma 3 implies Ω is compact; thus, the closed sub-\nk(cid:3)0 CSBk(R) is compact. Furthermore, the ﬁnite intersection property of compact spaces\nset\nimplies\n\n⊆ CSBk(R).\n\nk(cid:3)0 CSBk(R) (cid:5)= ∅.\n\n(cid:25)\n\n(cid:25)\n\nThe argument relies on three preliminary results, proved in Appendix A:\n\nLemma 14. Correspondence ri : Mi (cid:5) Si is nonempty valued. If ui is continuous, ri has a closed\ngraph and Ri is a nonempty closed set.\n\nLemma 15. For every closed event E ∈ E, SB(E) is closed.\n\nLemma 16. Let {E(cid:8)}(cid:8)=k\n· · · ⊆ E0), then\n\n(cid:25)\n\n(cid:8)=k\n(cid:8)=0 SB(E(cid:8)) is also nonempty.\n\n(cid:8)=0 be a decreasing sequence of nonempty events in E (∅ (cid:5)= Ek ⊆ Ek−1 ⊆\n\nFor notational convenience let CSB−1(E) = Ω. We prove by induction that, for each k (cid:2) 0,\n\nCSBk(R) is nonempty closed and can be expressed as\nk−1(cid:24)\n\n(cid:27)\n\n(cid:26)\n\nCSBk(R) = R ∩\n\n(cid:7)\nSB\n\n(cid:8)\nCSB(cid:8)(R)\n\n.\n\n(cid:8)=−1\n\nBasis step. The statement is true for k = 0 because by Lemma 14 CSB0(R) = R is nonempty\n\nclosed, and R can be expressed as\nR = R ∩ Ω = R ∩ CSB\n\n−1(R).\n\nInductive step. Suppose the statement is true for each (cid:8) = 0, . . . , k, then\n\n(cid:8)\n(cid:7)\nCSBk+1(R) = CSB\nCSBk(R)\n(cid:26)\nk−1(cid:24)\n\n(cid:8)\n(cid:7)\nCSBk(R)\n= CSBk(R) ∩ SB\n(cid:27)\n\n(cid:8)\n(cid:7)\nCSB(cid:8)(R)\nSB\n\n(cid:8)\n(cid:7)\nCSBk(R)\n∩ SB\n\n= R ∩\n\n(cid:26)\n\n(cid:8)=−1\nk(cid:24)\n\n(cid:8)=−1\n\n= R ∩\n\n(cid:8)\n(cid:7)\nCSB(cid:8)(R)\nSB\n\n(cid:27)\n\n.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n25\n\n(cid:25)\n\nBy the inductive hypothesis each CSB(cid:8)(R) is nonempty and closed ((cid:8) = 0, . . . , k). By Lemma 15\nalso SB(CSB(cid:8)(R)) is closed ((cid:8) = 0, . . . , k). R is also closed (Lemma 14). Hence CSBk+1(R) is\nclosed. {CSB(cid:8)(R)}(cid:8)=k\n(cid:8)=0 is a decreasing sequence of nonempty events in E. Therefore Lemma 16\n(cid:25)\nk\n(cid:8)=−1 SB(CSB(cid:8)(R)). Since\nimplies that\nthe latter is just an event about beliefs, modifying the strategies in ω we obtain another state in\ni∈N ri(μi) × {μi\n} ⊆ R. By Lemma 14, ri(μi) (cid:5)= ∅. We get\nthe same event. By deﬁnition of R,\nk(cid:24)\n(cid:8)\n(cid:7)\nCSB(cid:8)(R)\nSB\n\nk\n(cid:8)=−1 SB(CSB(cid:8)(R)) (cid:5)= ∅. Pick any ω = (si, μi)i∈N ∈\n\n} ⊆ R ∩\n\n∅ (cid:5)=\n\n(cid:4)\n\n(cid:2)\n\n(cid:27)\n\n(cid:26)\n\nri(μi) × {μi\n\n.\n\ni∈N\n\n(cid:8)=−1\nHence CSBk+1(R) (cid:5)= ∅. This proves the inductive step, and the theorem. (cid:2)\n\n6. Discussion and extensions\n\nWe compare our approach with that of GPS (6.1), consider imperfect information, chance\nmoves, and asymmetric information (6.2), own-plan dependence, dynamic inconsistency, and\nmulti-self utility (6.3), and ﬁnally discuss assumptions, solution concepts, and avenues for further\nresearch (6).\n\n6.1. Comparison with GPS\n\nIn Section 2 we presented our framework as a generalization of GPS. This is not literally true.\nThe reason is twofold. First, GPS allow for imperfect information and chance moves. As we show\nbelow, these complications can be included in our framework. Second, GPS allow for actual\nrandomization whereas we exclude it. Prima facie, this difference may seem immaterial. GPS\nassume players maximize expected (psychological) utility given beliefs, and in their framework\nthere is no incentive to randomize. It might seem that the only role played by randomization is\nto guarantee the existence of equilibrium, a result we obtain by looking at equilibrium in beliefs.\nHowever, unlike standard games, in psychological games there may be a difference (to a player’s\nutility) between a belief assigning probability one to a randomized choice that, say, picks a or b\nwith probability 1\n2 to each of a and b. These beliefs are\nequivalent if psychological utility functions satisfy a linearity property, which is not satisﬁed in\nall applications (see e.g. Sebald [72]). Note, however, that one can deal with this by adding to the\ngame explicit moves whereby a player chooses a lottery over elementary actions (which is what\nSebald does).\n\n2 , and the belief that assigns probability 1\n\nNow look at the version of GPS that is a special case of our framework: psychological games\nwith utilities of the form ui : Z × Mi → R, where Mi is the space of inﬁnite hierarchies of\ninitial beliefs of i, and ﬁrst-order beliefs are probability measures over pure strategies of i’s\nopponents. How much is lost by restricting the analysis to such games? We have argued that\nmany interesting phenomena such as sequential reciprocity, psychological forward induction, and\nregret cannot be analyzed. However, we can prove a partial equivalence result. Suppose the initial\nbeliefs of others enter the utility, that is, ui : Z ×\nj ∈N Mj → R. Then there is a psychological\ngame with utilities ui : Z × Mi → R that has the same sequential equilibrium assessments as the\nformer game.46 This does not mean that in this class of games conditional higher-order beliefs are\n\n(cid:2)\n\n46 The intuition is relatively simple: each initial belief hierarchy μi induces a probability measure f i (μi ) ∈ Δ(S−i ×\nM−i ) which can be used to compute an expectation ui (z, μi ) of ui (z, μi , ·). Since in a consistent assessment there\n\n\f26\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nirrelevant. First, the equivalence result only concerns sequential equilibria, and we argued that the\nnon-equilibrium analysis of psychological games is important (as exempliﬁed by psychological\nforward induction). Second, our very deﬁnition of sequential equilibrium makes essential use\nof conditional beliefs. Having such beliefs in the framework is conceptually useful as it helps\nunderstanding the epistemic assumptions underlying the SE concept.\n\nGPS deﬁne a ‘psychological subgame perfect equilibrium’ concept (p-SPE) in two steps. First,\nthey deﬁne a ‘psychological Nash equilibrium’ as a situation in which each player’s (randomized)\nstrategy is an ex ante best reply to his hierarchy of beliefs and beliefs of all orders are correct.\nThen, they deﬁne a p-SPE as a psychological equilibrium proﬁle (σi, μi)i∈N such that (σi)i∈N\nis a subgame perfect equilibrium of the standard game with utility functions ui(·, μi) : Z → R\n(i ∈ N ). Remark 10 shows that our SE coincides with p-SPE for games with utility functions à la\nGPS. This result can be extended to games with imperfect information (replacing p-SPE with a\nsimilarly deﬁned ‘psychological sequential equilibrium’ concept).\n\nGPS also deﬁne a ‘psychological trembling-hand perfect equilibrium’ (p-TPE): (σi, μi)i∈N is\na p-TPE if it is a psychological equilibrium and (σi)i∈N is a trembling-hand perfect equilibrium\nof the standard game with utility functions ui(·, μi) : Z → R (i ∈ N ). They show that some\ngames with continuous utility functions have no p-TPE. Yet, our argument to prove existence\nof SE (see the sketch in Subsection 4.2) shows that continuous games always have equilibria\nthat can be obtained as the limit of ε-equilibria with trembles. These two results are mutually\nconsistent because, even for the special case of games à la GPS, they concern different notions\nof trembling-hand perfection. GPS perturb the strategies, but not the beliefs; we perturb both, as\nequilibrium beliefs are determined by the strategies via the consistency condition.47\n\n6.2. Imperfectly observable actions, chance moves, and asymmetric information\n\nWe chose to focus on games with observable actions and no chance moves for the sake of\nsimplicity. But our concepts and results carry over to the more general case of games where\npast actions need not be perfectly observed and chance may play a role (as in GPS). Let N =\n{c, 1, . . . , n} where index c denotes the chance player, and let Hi be the partition of the ﬁnite\nset of histories H into information sets of player i (i (cid:5)= c). Assume perfect recall holds. Then\nthe set of strategy proﬁles consistent with any information set h of player i must have the form\nS(h) = Si(h) × S−i(h) [where Si(h) :=\nh∈h S−i(h)]. Consider, for the\nﬁrst-order beliefs of player i, the collection of conditioning events {Fi: Fi = S−i(h), h ∈ Hi}.\nLet Xk−1\n−i be the space of (k − 1)-order uncertainty for player i; we obtain the set of k-order cps’\nΔHi (Xk−1\n−i ), and the k-order uncertainty space Xk\n−j ). The resulting\n−i\nset of inﬁnite hierarchies of cps’ Mi is homeomorphic to ΔHi (S−i × M−i) via a belief mapping\nfi = (fi,h)h∈Hi .\n\nh∈h Si(h), S−i(h) :=\n\nj (cid:5)=i,0 ΔHj (Xk−1\n\n= Xk−1\n−i\n\n(cid:28)\n\n(cid:28)\n\n(cid:2)\n\n×\n\nNote that Hi speciﬁes i’s information at each node/history, including those where i is inactive\nand in particular including the terminal nodes. This would be redundant in standard games, but is\n\nis ‘common knowledge’ of the hierarchical beliefs, no observation will make the players change their mind about the\ninitial beliefs of the opponents, hence for any consistent assessment ui and ui have the same set of maximizing actions\nat each history. (If the game has only one stage ui and ui are fully equivalent, i.e., they have the same best response\ncorrespondences.)\n47 Kolpin [47] makes a related point.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n27\n\n(cid:2)\n\ncrucial to model some belief-dependent motivations, such as regret or blame avoidance, whereby\nplayers’ conditional beliefs matter even if they are inactive.48\n\nLet σc = σc(·|h) ∈\n\nh∈H \\Z Δo(Ac(h)) denote the strictly positive ‘objective’ probabilities of\nchance moves. It is routine to deﬁne closed and compact subsets of hierarchies ˆMi (i = 1, . . . , n)\nreﬂecting the assumption that each players’s beliefs about the chance player c are determined by\nσc and there is common certainty of this. The analysis of rationalizability is easily adapted to this\nenvironment. The deﬁnition of sequential equilibrium requires some care.\n\nAn assessment (σ, μ) = (σi, μi)n\n\ni=1 is consistent if there is a sequence of strictly positive\n\nbehavioral strategy proﬁles σ k → σ such that for all i = 1, . . . , n, h ∈ Hi , s−i ∈ S−i(h),\n\nμ1\ni (s−i|h) = lim\nk→∞\n\n(cid:18)\n\ns(cid:9)\n−i\n\nPrσc (sc)\nj (cid:5)=i,c Prσ k\n(cid:2)\n∈S−i (h) Prσc (s(cid:9)\nc)\n\nj\n\n(sj )\n\nj (cid:5)=i,c Prσ k\n\nj\n\n(s(cid:9)\nj )\n\n(cid:2)\n\n(since σc is strictly positive, the denominator is positive),49 and furthermore for all (cid:8) > 1, μ(cid:8)\nsigns probability 1 to μ(cid:8)−1\n−i\nand for all i = 1, . . . , n, h ∈ Hi , s∗\n∈ Si(h),\ni\n(cid:8)\n∈ arg max\nsi ∈Si (h)\n\ni as-\n(cf. Deﬁnition 6). (σ, μ) is a sequential equilibrium if it is consistent\n\n∗\n> 0 ⇒ s\ni\n\n[ui|h],\n\n(cid:7)\n∗\ns\ni\n\nEsi ,μi\n\n(cid:13)\n(cid:13)h\n\nPrσi\n\n[ui|h] is given by the obvious modiﬁcation of formula (2). A straightforward adap-\nwhere Esi ,μi\ntation of the trembling-hand argument used to prove Theorem 1 shows that if the utility functions\nare continuous a sequential equilibrium exists.\n\nThis extended framework also allows to analyze situations with incomplete information, mod-\neling them as games with asymmetric information about an initial chance move: at the empty\nhistory h0 the only active player is c (chance), Ac(h0) = Θ, where Θ ⊆ Θ0 × Θ1 × · · · × Θn,\neach player i observes only coordinate θi of θ = (θ0, θ1, . . . , θn); θ may affect payoffs, or actions\nsets, or the probabilities of future chance moves.50 This extension is important for applications.\nUnless one models interaction within a family or amongst friends, it is probably not realistic to\nassume that players know one another’s psychological propensities. For example, in the analysis\nof Γ2 (or Γ3) we assumed that Ann knows that Bob’s sensitivity to guilt is θ2 = 5\n2 , which may\nbe a stretch.51 Another example comes up in Caplin and Leahy’s [22] model of doctor-patient\ninteraction: the patient’s well-being depends on his anxiety and on whether or not he likes early\nresolution of uncertainty; the concerned doctor wants to help the patient but is uninformed of the\nnature of patient’s preferences regarding resolution of uncertainty. Yet another reason to allow\nfor incomplete information is that a player may care about the ex post beliefs of others about\nsome of his characteristics, which are not common knowledge, as in the models of Bernheim\n\n48 For example, the terminal information partition is crucial in Battigalli and Dufwenberg’s [8] model of guilt from\nblame and Tadelis’ [77] analysis of shame in a Trust Game.\n49 Kreps and Wilson [51] have a similar condition that refers to conditional probabilities of histories/nodes.\n50 We do not present a more direct formalization of incomplete information psychological games for reasons of space.\nA word of caution is in order. When incomplete information is represented as imperfect, asymmetric information about\nan initial chance move one introduces ﬁctitious ex ante beliefs (the initial beliefs in the imperfect information extensive\ngame). This does not affect the equilibrium analysis of standard games, but it is known to affect the rationalizable\noutcomes (see Battigalli et al. [7] for a discussion), and it may as well affect the analysis when players have belief-\ndependent preferences.\n51 Ample evidence in psychology suggests emotional sensitivities differ among people. See Krohne [52] for a general\ndiscussion, and Tangney [78] on guilt speciﬁcally.\n\n\f28\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nand Dufwenberg and Lundholm and several other economic models where agents are assumed\nto have an intrinsic concern about their reputation.52\n\nOne ﬁnal reﬂection relating to imperfect and asymmetric information before we proceed.\nStandard games of incomplete information can be regarded as a special case of our framework.\nWe have already clariﬁed in Section 2.1 how psychological games are different from games of\nincomplete information. But one may wonder whether our framework is “too general” in the\nfollowing sense: Is it possible that the same qualitative behavior explained by psychological\ngame models can be explained using standard games of incomplete information? The answer is\nNo, although it is possible in some cases (see, for example, Levine [54], and Gul and Pesendorfer\n[40] who put forward models of reciprocity featuring incomplete information about “behavioral\ntypes”). 53 The best way to see this is to remember that psychological preferences may be affected\nby players’ beliefs at information sets where they are inactive, such as their terminal information\nsets. Changing this information typically alters predicted behavior. This comparative statics effect\n(which has been demonstrated experimentally by Tadelis [77]) is impossible in standard games,\nbecause behavior predicted by sequential equilibrium or rationalizability in such games depends\nonly on the information of players when they are active.54\n\n6.3. Own-plan dependence, dynamic (in)consistency, and multi-self utility\n\nWe have argued that it is natural to let a player’s utility depend directly on other players’\nplans, but have so far assumed it does not depend directly on his own plan. This allowed us\nto apply standard dynamic programming techniques and prove Theorem 13. We next show that\nallowing for own-plan dependence is natural and gives rise to an interesting form of dynamic\ninconsistency.\n\nConsider the following version of the Trust Game: Ann can either trust Bob or opt out. If she\nopts out no surplus is created; if she trusts Bob the total surplus is $4 and Bob can either grab\n$3 or let Ann allocate the surplus (action Leave). If Bob let Ann choose the allocation, she can\neither split the surplus or reward Bob, giving him the $3 he could have grabbed.\n\nNow assume that if Ann gets less money than she expected she feels disappointed and that\nthe anticipation of this negative feeling affects her decisions. This is captured by the following\nutility function:\n\nu1(z, μ, s1) = π1(z) − θ\n\n(cid:11)(cid:12)\n\n(cid:7)\ns2\n\n(cid:13)\n(cid:13)h0\n\n(cid:8)\nπ1\n\n(cid:7)\n(cid:8)\nζ (s1, s2)\n\nμ1\n1\n\n(cid:14)\n− π1(z)\n\ns2\n\nwhere θ is a sensitivity parameter and the term in brackets in the Ann’s disappointment at z.\nΓ6 in Fig. 6 builds on u1, and thereby turns out to exhibit own-plan dependent utility for Ann.\nWe let u2(z, μ, s) = π2(z).\n\nThe utility assigned by Ann to terminal history (Trust, Grab) depends on her initial belief\n1(Leave|h0), and on how she plans to behave if Bob leaves for her to allocate the surplus.\nα = μ1\nThe own-plan dependence arises because Ann cannot feel disappointed if she plans to reward Bob\n\n52 Models with an intrinsic concern for reputation are sometimes presented as a reduced forms of repeated interaction\nmodels. See, for example, Morris [62] and Ottaviani and Sorensen [64].\n53 Gul and Pesendorfer offer a recursive construction of a canonical space of behavioral types.\n54 Battigalli and Guaitoli [9] put forward a notion of conjectural (or self-conﬁrming) equilibrium that crucially depends\non the terminal information structure. But such an equilibrium concept does not apply to one-shot interactions, as it is\nmeant to capture, with a “static” deﬁnition, stable patterns of behavior in situations of recurrent interaction.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n29\n\nFig. 6. Psychological game Γ6: aversion to disappointment.\n\n(column R), since in this case the resulting material payoff is 1 regardless of what Bob chooses.\nFor Ann to be disappointed at terminal history (Trust, Grab) requires that she plans to split and\nthat α > 0. In this case she (initially) expects a material payoff which is larger than her material\npayoff at history (Trust, Grab) [(1 − α) × 1 + α × 2 > 1]. The disappointment yields utility\n(1 − θ α) (column S). The expected utility of plan (Trust, Split) is thus (1 − α) × (1 − θ α) + α × 2,\nwhich could be lower than 1 (in fact, even lower than 0) if θ is large enough. In this case, the\nex ante expected utility maximizing plan is (Trust, Reward) (which prevents disappointment and\nyields 1). However, (Trust, Reward) is not dynamically consistent because the best choice after\nhistory (Trust, Leave) is to split. As a result there is no strategy that maximizes Ann’s expected\nutility at the beginning of the game and also at history (Trust, Leave). Ann cannot commit to\nrewarding Bob, so she should initially maximize under the constraint that she would split in\nthe endgame; the resulting outcome is Out. Such ‘consistent planning’ (Strotz [76]) implies that\nAnn’s strategy be immune to one-shot deviations. Therefore, the example shows that the One-\nShot-Deviation principle (cf. Proposition 8) fails with own-plan dependence. A similar kind of\ndynamic inconsistency arises in Caplin and Leahy’s [21] theory of psychological expected utility,\nin work by K˝oszegi and Rabin [49,50] on reference dependent preferences, and in Mariotti’s\n[60] abstract analysis of generalized extensive form games. These papers can be shown to be\nconsistent with our extended framework allowing own-plan dependence.\n\nWe now present an example where even the multi-self equilibrium fails to exist unless we\n\nallow for uncertainty about one’s own strategy.\n\nIn game Γ7 of Fig. 7 a pure strategy of Bob is immune to one-shot deviations if and only if it\n\ncorresponds to a pure Nash equilibrium of the following game between two selves of Bob:\n\nBob(cid:8)\\Bobr L(cid:9)(cid:9)\nL(cid:9)\n3, 0\nR(cid:9)\n2, 3\n\nR(cid:9)(cid:9)\n1, 1\n2, 1\n\nThis companion game has no pure equilibrium. The unique mixed equilibrium is ( 2\n3 R(cid:9), 1\n1\nof Γ7.\n\n3 L(cid:9) +\n2 R(cid:9)(cid:9)). Working backwards, Ann’s best reply is (cid:8). This is the unique multi-self SE\n\n2 L(cid:9)(cid:9) + 1\n\nThis example suggests that in order to make rationality possible in psychological games with\nown strategy dependence we have to allow a player to be uncertain about her own strategy. This\ncan be done, at the cost of additional complexity, within a richer framework where player i’s\nﬁrst-order beliefs are deﬁned over S rather than S−i (cf. Battigalli and Siniscalchi [10]).\n\n\f30\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\nFig. 7. Psychological game Γ7.\n\nBesides own-strategy dependence, a more direct way to allow for dynamic inconsistencies is\nto adopt a multi-self approach and model a player’s preferences with an array of ‘local’ utility\nfunctions (ui,h : Z ×M×S → R)h∈H \\Z. The sequential equilibrium analysis of Section 4 applies\nto this extended framework almost verbatim.\n\nThis formulation is relevant to reciprocity theory. We have already seen how our basic frame-\nwork could reproduce Dufwenberg and Kirchsteiger’s theory in an example (Γ5). However, to\nhandle general games one needs a multi-selves approach, and it is then possible to (essentially)\nreformulate Dufwenberg and Kirchsteiger’s model (details are available on request).\n\n6.4. Discussion of solution concepts\n\nWe have explained in Section 4 that SE relies on the assumption that players never change\ntheir beliefs about the (conditional) beliefs of their opponents. This assumption is questionable\nin standard games, and becomes even more questionable in psychological games where players’\nperceptions of opponents’ intentions (“What is my co-player trying to achieve?”) are key.\n\nOne way to assess the opponent’s intention is to draw inferences about his unobservable be-\nliefs from his observable actions via forward induction (FI) reasoning. It is well known that FI\nreasoning yields beliefs inconsistent with backward induction reasoning and, more generally,\nwith sequential equilibrium (see, e.g., Reny [69]). Yet in (generic) standard games at least one\nsequential equilibrium outcome is consistent with FI reasoning.55 The following simultaneous-\nmove example shows that this need not be true in games with belief-dependent preferences.\n\nLet A1 = {U, M, D}, A2 = {(cid:8), r}. Player 1 (row) only cares about his material payoff,\nwhereas the preferences of 2 (column) also depend on his terminal second-order beliefs. Let\nα1 := μ1\n\n[α1|a1]. The game is described by the following table.\n\n1((cid:8)|h0), β2(a1) := Eμ2\n\n(cid:8)\n3, 4[β2(U ) − β2(D)]\nU\nM 2, 4[β2(U ) − β2(D)]\nD 0, 4[β2(U ) − β2(D)]\n\nr\n\n0, 1\n2, 1\n3, 1\n\n55 For example, in generic ﬁnite games with complete and perfect information the backward induction terminal node is\nthe same as the terminal node implied by extensive form rationalizability and the iterated deletion of weakly dominated\nstrategies, even though the implied conditional beliefs about continuation strategies may well be very different, as shown\nby Reny [69]. For ﬁnite games with imperfect information see Govindan and Wilson [38].\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n31\n\nIn a sequential equilibrium β2(U ) − β2(D) = 0, because all conditional second order beliefs\n\nassign probability one to the true value of α1. Therefore the SE outcome must be (D, r).\n\nForward induction implies that 2 would believe whenever possible that 1 is rational (see Sec-\n3 ). Therefore FI implies that\n3 . Hence 4[β2(U ) − β2(D)] > 1, player 2 chooses (cid:8), and anticipating this 1\n\ntion 5). Note that U (D) is a best reply to α1 iff α1 (cid:2) 2\nβ2(U ) − β2(D) (cid:2) 1\nchooses U . The FI outcome is (U, (cid:8)).\n\n3 (α1 (cid:3) 1\n\nThis example suggests that it is reasonable to relax the consistency condition of SE to allow\nplayers to change their higher order beliefs at least when they observe unexpected moves by\nthe opponents. Other conditions on updated beliefs (such as strong belief in rationality) may be\nimposed to obtain sharp predictions.\n\nAnother reason to relax consistency is the following. Suppose that, as suggested at the end\nof Section 6.3, we explicitly model a player’s plan as a (possibly false or uncertain) belief about\nhis own contingent behavior. Then a player’s intentions are fully determined by his beliefs about\nhimself and others, and the consistency condition of SE implies that (in games with complete\ninformation) players never change their beliefs about the co-players’ intentions, no matter what\nthey do. This trivializes the analysis of extensive form psychological games. For example, Bob\ncan respond unkindly to an unexpectedly generous action by Ann simply because he initially\nbelieved, and continues to believe, that Ann meant to be unkind to him.\n\nFinally, we note that it would be interesting to approach psychological games from a learn-\ning point-of-view. Here is the main issue to be aware of if this task is addressed: It is often\nargued that players learn to play some equilibrium because through recurrent play they come\nto hold correct beliefs about the opponents’ actions (see, e.g., Fudenberg and Levine [33] and\nreferences therein). This may not be enough for psychological games; since payoffs depend on\nhierarchical beliefs, players would have to be able to learn others’ beliefs, but unlike actions\nbeliefs are typically not observable ex post. This suggests that a different notion of equilibrium\nis worth exploring, whereby beliefs are required to be consistent with observed outcomes, as in\nthe conjectural/self-conﬁrming equilibrium concept.56 The interactive epistemology analysis of\nself-conﬁrming equilibrium in (standard) signaling games by Battigalli and Siniscalchi [11] may\nprovide a useful starting point.\n\nAcknowledgments\n\nWe thank Geir Asheim, Oliver Board, Adam Brandenburger, Andrew Caplin, Amanda\nFriedenberg, Drew Fudenberg, Georg Kirchsteiger, Botond Koszegi, Jing Li, David Pearce,\nLudovic Renou, Klaus Ritzberger, Joel Sobel, and participants in several seminars for helpful\ndiscussions. For their kind hospitality, we thank the Economics Departments of the Stern School\nof Business at NYU, the European University Institute (Battigalli), and Göteborg University\n(Dufwenberg). We gratefully acknowledge ﬁnancial support from Bocconi University and MIUR\n(Battigalli) and from the NSF (Dufwenberg).\n\nAppendix A\n\nWe start with some preliminaries about rationality and backward induction on belief-induced\ndecision trees, and then prove Lemmata 14, 15 and 16. For any ﬁxed hierarchy of cps’ μi , we ob-\n\n56 See, for example, Battigalli and Guaitoli [9] and Fudenberg and Levine [32].\n\n\f32\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\ntain a well-deﬁned decision tree that can be solved by backward induction: deﬁne value functions\nVμi\n\n: (H \\Z) × Ai → R as follows\n\n: H → R and V μi\n\n• For terminal histories z ∈ Z, let\n\n(cid:3)\n\nVμi (z) :=\n\nui(z, μi, μ−i, s−i)fi,z(μi)(ds−i, dμ−i).\n\nS−i ×M−i\n\nlet\n\n• Assuming that Vμi (h, a) has been deﬁned for the immediate successors (h, a) of history h,\n(cid:13)\n(cid:8)\n(cid:7)\n(cid:13)h\nS−i(h, a−i)\n\n(cid:7)\n(cid:8)\nh, (ai, a−i)\n\nV μi (h, ai) :=\n\nVμi\n\nμ1\ni\n\n(cid:12)\n\n;\n\na−i ∈A−i (h)\n\nfor each ai ∈ Ai(h); then Vμi (h) is deﬁned as\n\nVμi (h) := max\nai ∈Ai (h)\n\nV μi (h, ai).\n\nRecall that, for any strategy si ∈ Si , Hi(si) = {h ∈ H \\Z: si ∈ Si(h)} denotes the set of histo-\n\nries allowed by si . The proof of the following result is available by request:\n\nLemma 17. The sequential best reply correspondence ri : Mi (cid:5) Si can be characterized as\nfollows\n\nri(μi) =\n\n(cid:20)\nsi: ∀h ∈ H (si), si,h ∈ arg max\nai ∈Ai (h)\n\n(cid:21)\n.\nV μi (h, ai)\n\nProof of Lemma 14. By Lemma 17 ri(μi) = {si: ∀h ∈ H (si), si,h ∈ arg maxai ∈Ai (h) V μi (h, ai)}.\nClearly, the RHS is nonempty. Therefore ri(·) is nonempty-valued and Ri is nonempty. The be-\n[ui|h] is continuous\nlief function fi is continuous (Lemma 3). If ui is also continuous, then Esi ,μi\n(in μi ), which implies that Ri is closed. (cid:2)\n\nProof of Lemma 15. We must show that for every closed event E ∈ E−i , SBi(E) is closed.\nSBi(∅) = ∅, a closed set, by deﬁnition. Suppose that E = Ωi × E−i where E−i is nonempty and\nclosed. Recall that SBi(E) =\nh:[h]∩E(cid:5)=∅ Bi,h(E). For each h,\n× Ω−i,\n\n(cid:7)\nS−i(h) × M−i\n\nBi,h(E) = Si × f\n\nE−i ∩\n\n(cid:7)\n(cid:7)\nΔ\n\n(cid:8)(cid:8)(cid:8)\n\n(cid:25)\n\n−1\ni,h\n\nwhere for any measurable space X and any F ⊆ X we let Δ(F ) denote the set of probability\nmeasures on X that assign probability one to F . Note that if F is closed, Δ(F ) is also closed.\nThe coordinate function fi,h : Mi → Δ(Ω−i) is continuous and M−i is closed (Lemma 3); hence\n−1\ni,h (Δ(E−i ∩ (S−i(h) × M−i))) are\nE−i ∩ (S−i(h) × M−i), Δ(E−i ∩ (S−i(h) × M−i)) and f\nclosed. It follows that Bi,h(E) (h ∈ H ) and SBi(E) are closed. (cid:2)\n\nProof of Lemma 16. Let {E(cid:8)}(cid:8)=k\nEk−1 ⊆ · · · ⊆ E0); we show that\nbe written E(cid:8) = E(cid:8)\ni\n\n× E(cid:8)\n\n(cid:25)\n\n(cid:8)=0 be a decreasing sequence of nonempty events in E (∅ (cid:5)= Ek ⊆\n(cid:8)=k\n(cid:8)=0 SB(E(cid:8)) is also nonempty. For each (cid:8) and i, E(cid:8) ∈ E can\n−i , where E(cid:8)\n−i\n(cid:8)=k(cid:24)\n(cid:7)\nΩi × E(cid:8)\n−i\n\n⊆ Ω−i , and by deﬁnition of SB(·)\n\nSBi\n\n(cid:8)\n.\n\n(cid:8)\n\n(cid:7)\nSB\n\nE(cid:8)\n\n=\n\n(cid:24)\n\n(cid:8)=k(cid:24)\n\n(cid:8)=0\n\ni∈N\n\n(cid:8)=0\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n33\n\n(cid:25)\n\n(cid:8)=k\n(cid:8)=0 SBi(Ωi × E(cid:8)\nTherefore we must show that\nset of cps’ μ ∈ ΔH (Ω−i) such that μ(E(cid:8)\n−i\nNote that\n(cid:8)=k(cid:24)\n\n(cid:8)=k(cid:24)\n\n(cid:26)\n\n(cid:7)\nΩi × E(cid:8)\n−i\n\n(cid:8)\n\n= Si × f\n\n−1\ni\n\n(cid:7)\nΩ−i; E(cid:8)\n−i\n\nΔH\n\nSBi\n\n(cid:8)=0\n\n(cid:27)\n(cid:8)\n\n× Ω−i.\n\n−i) (cid:5)= ∅ (i ∈ N ). Let ΔH (Ω−i; E(cid:8)\n\n−i) denote the\n∩(S−i(h)×M−i) (cid:5)= ∅.\n\n|h) = 1 for each h such that E(cid:8)\n−i\n\nWe show below that\nf\n\n(cid:8)=k\n(cid:8)=0 ΔH (Ω−i; E(cid:8)\n\n(cid:25)\n(\n\n−1\ni\n\n(cid:25)\n\n(cid:25)\n\n(cid:8)=0\n(cid:8)=k\n(cid:8)=0 ΔH (Ω−i; E(cid:8)\n−i) (cid:5)= ∅. Since fi is onto (Lemma 3), it follows that\n(cid:25)\n−i)) (cid:5)= ∅. Hence\n\n(cid:8)=k\n(cid:8)=0 SBi(Ωi × E(cid:8)\n\n−i) (cid:5)= ∅.\n\n(cid:8)=k\n(cid:8)=0 ΔH (Ω−i; E(cid:8)\n\nWe show that\n\n−i) (cid:5)= ∅ with a recursive construction. Say that h is ‘reached’\nby probability measure ν ∈ Δ(Ω−i) if ν(S−i(h) × M−i) > 0. Note that if h is reached by ν, every\npredecessor of h is also reached by ν. Say that μ(·|h) is ‘derived’ from ν, where ν reaches h, if\nfor every Borel set F−i ⊆ Ω−i\n\nμ(F−i|h) = ν(F−i ∩ (S−i(h) × M−i))\n\n.\n\nν(S−i(h) × M−i)\nPick any probability measure ν in the (nonempty) set Δ(Ek\n−i). For each h reached by ν let\nμ(·|h) be derived from ν. Thus, μ(·|h) has been deﬁned for a nonempty set of histories closed\nw.r.t. precedence (that is, if h is in the set every predecessor of h is in the set), the set is nonempty\nbecause it contains the initial history h0. Now suppose that μ(·|h) has been deﬁned for some set\nˆH (cid:5)= H , for each h ∈ H \\ ˆH such that the immediate\nof histories ˆH closed w.r.t. precedence. If\npredecessor of h belongs to ˆH , pick a probability measure νh in the set Δ(E(cid:8)(h)\n∩ (S−i(h) ×\n−i\nM−i)), where (cid:8)(h) is the highest index (cid:8) ∈ {−1, 0, . . . , k} such that E(cid:8)\n∩ (S−i(h) × M−i) (cid:5)= ∅,\n−i\nand by convention we let E−1 = Ω−i . Let μ(·|h(cid:9)) be derived from νh whenever h(cid:9) weakly follows\nh and is reached by νh. Now μ(·|h) is deﬁned for a set of histories ˆH (cid:9) closed under the precedence\nrelation and strictly larger than ˆH . Proceed in this way until the whole H is covered. We claim\nthat the resulting vector of probability measures (μ(·|h))h∈H is a cps μ ∈\n−i).\nTo see that (μ(·|h))h∈H is a cps we only have to check that the ‘chain rule’ (3) in Deﬁnition 1\nholds. Suppose that h precedes h(cid:9). To write formulas more transparently, let C = S−i(h) × M−i ,\nC(cid:9)\n−i). Since h precedes h(cid:9), S−i(h(cid:9)) ⊆\n−i\nS−i(h), hence C(cid:9)\n⊆ C−i . If h(cid:9) is not reached by μ(·|C−i) then (3) holds trivially as 0 = 0. If h(cid:9)\n−i\nis reached by μ(·|C−i), then μ(·|C−i) and μ(·|C(cid:9)\n−i) are both derived from the same measure –\nsay ν ∈ Δ(Ω−i) – reaching both h and h(cid:9); thus, for every Borel set F−i ⊆ C(cid:9)\n−i\n(cid:8)\n.\n\n= S−i(h(cid:9)) × M−i , μ(·|h) = μ(·|C−i), μ(·|h(cid:9)) = μ(·|C(cid:9)\n\n= ν(F−i)\nν(C(cid:9)\n−i)\n(cid:8)=k\n−i), note that by construction μ(E(cid:8)(h)|h) = 1 for all h ∈ H .\n(cid:8)=0 ΔH (Ω−i; E(cid:8)\n∩ (S−i(h) × M−i) (cid:5)= ∅. Then\n\nSuppose that, for any index (cid:8) ∈ {0, . . . , k} and any h ∈ H , E(cid:8)\n−i\n(cid:8)(h) (cid:2) (cid:8) and μ(E(cid:8)|h) (cid:2) μ(E(cid:8)(h)|h) = 1; hence μ(E(cid:8)|h) = 1 as desired. (cid:2)\n\nμ(F−i|C−i) = ν(F−i)\nν(C−i)\n\n(cid:8)=k\n(cid:8)=0 ΔH (Ω−i; E(cid:8)\n\nν(C(cid:9)\n−i)\nν(C−i)\n\nTo see that μ ∈\n\n(cid:13)\n(cid:13)C−i\n\n(cid:7)\nF−i\n\n(cid:7)\nC\n\n(cid:13)\n(cid:13)C\n\n= μ\n\n(cid:9)\n−i\n\n(cid:9)\n−i\n\n(cid:25)\n\n(cid:25)\n\nμ\n\n(cid:8)\n\nReferences\n\n[1] W. Albers, R. Pope, R. Selten, B. Vogt, Experimental evidence for attractions to chance, Ger. Econ. Rev. 1 (2000)\n\n113–130.\n\n[2] G.B. Asheim, The Consistent Preferences Approach to Deductive Reasoning in Games, Springer-Verlag, Berlin,\n\n2006.\n\n\f34\n\nP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n[3] G. Attanasi, R. Nagel, A survey of psychological games: Theoretical ﬁndings and experimental evidence, in:\nA. Innocenti, P. Sbriglia (Eds.), Games, Rationality and Behaviour. Essays on Behavioural Game Theory and Ex-\nperiments, Palgrave McMillan, Houndmills, 2007, pp. 204–232.\n\n[4] R.J. Aumann, A. Brandenburger, Epistemic conditions for Nash equilibrium, Econometrica 63 (1995) 1161–1180.\n[5] M. Bacharach, G. Guerra, D.J. Zizzo, The self-fulﬁlling property of trust: An experimental study, Theory Dec. 63\n\n(2007) 349–388.\n\n[6] P. Battigalli, G. Bonanno, Recent results on belief, knowledge and the epistemic foundations of game theory, Res.\n\nEcon. 53 (1999) 149–226.\n\n[7] P. Battigalli, A. Di Tillio, E. Grillo, A. Penta, Interactive epistemology and solution concepts for games with asym-\n\nmetric information, Mimeo, Bocconi University, 2007.\n\n[8] P. Battigalli, M. Dufwenberg, Guilt in games, Amer. Econ. Rev. 97 (2007) 170–176, Papers & Proceedings.\n[9] P. Battigalli, D. Guaitoli, Conjectural equilibria and rationalizability in a game with incomplete information, in:\nP. Battigalli, A. Montesano, F. Panunzi (Eds.), Decisions, Games and Markets, Kluwer Academic Publishers, Dor-\ndrecht, 1997, pp. 97–124.\n\n[10] P. Battigalli, M. Siniscalchi, Hierarchies of conditional beliefs and interactive epistemology in dynamic games,\n\nJ. Econ. Theory 88 (1999) 188–230.\n\n[11] P. Battigalli, M. Siniscalchi, Strong Belief and Forward Induction Reasoning, J. Econ. Theory 106 (2002) 356–391.\n[12] D.E. Bell, Regret in decision making under uncertainty, Oper. Res. 30 (1982) 961–981.\n[13] D.E. Bell, Disappointment in decision making under uncertainty, Oper. Res. 33 (1985) 1–27.\n[14] E. Ben-Porath, E. Dekel, Signaling future actions and the potential for sacriﬁce, J. Econ. Theory 57 (1992) 36–51.\n[15] D. Bernheim, Rationalizable strategic behavior, Econometrica 52 (1984) 1007–1028.\n[16] D. Bernheim, A theory of conformity, J. Polit. Economy 102 (1994) 841–877.\n[17] J. Bouckaert, G. Dhaene, Sequential reciprocity in two-player, two-stages games: An experimental analysis, Mimeo,\n\nUniversity of Antwerp and Catholic University of Leuven, 2007.\n\n[18] A. Brandenburger, E. Dekel, Hierarchies of beliefs and common knowledge, J. Econ. Theory 59 (1993) 189–198.\n[19] A. Caplin, Fear as a policy instrument: Economic and psychological perspectives on intertemporal choice, in:\nG. Loewenstein, D. Read, R.F. Baumeister (Eds.), Time and Decision, Russell Sage Foundation, New York, NY,\n2003.\n\n[20] A. Caplin, K. Eliaz, AIDS policy and psychology: A mechanism design approach, RAND J. Econ. 34 (2003) 631–\n\n646.\n\n[21] A. Caplin, J. Leahy, Psychological expected utility theory and anticipatory feelings, Quart. J. Econ. 116 (2001)\n\n55–79.\n\n[22] A. Caplin, J. Leahy, The supply of information by a concerned expert, Econ. J. 114 (2004) 487–505.\n[23] G. Charness, M. Dufwenberg, Promises and partnership, Econometrica 74 (2006) 1579–1601.\n[24] M. Dufwenberg, Marital investment, time consistency and emotions, J. Econ. Behav. Organ. 48 (2002) 57–69.\n[25] M. Dufwenberg, Time-consistent Wedlock with endogenous trust, Doctoral Dissertation, Uppsala University, 1995.\n[26] M. Dufwenberg, S. Gaechter, H. Hennig-Schmidt, The framing of games and the psychology of play, Mimeo,\n\nUniversity of Arizona, University of Nottingham, and University of Bonn, 2007.\n\n[27] M. Dufwenberg, U. Gneezy, Measuring beliefs in an experimental lost wallet game, Games Econ. Behav. 30 (2000)\n\n163–182.\n\n[28] M. Dufwenberg, G. Kirchsteiger, Reciprocity and wage undercutting, Europ. Econ. Rev. 44 (2000) 1069–1078.\n[29] M. Dufwenberg, G. Kirchsteiger, A theory of sequential reciprocity, Games Econ. Behav. 47 (2004) 268–298.\n[30] M. Dufwenberg, M. Lundholm, Social norms and moral hazard, Econ. J. 111 (2001) 506–525.\n[31] A. Falk, U. Fischbacher, A theory of reciprocity, Games Econ. Behav. 54 (2006) 293–315.\n[32] D. Fudenberg, D. Levine, Self-conﬁrming equilibrium, Econometrica 61 (1993) 523–545.\n[33] D. Fudenberg, D. Levine, The Theory of Learning in Games, MIT Press, Cambridge, MA, 1998.\n[34] D. Fudenberg, J. Tirole, Game Theory, MIT Press, Cambridge, MA, 1991.\n[35] J. Geanakoplos, The Hangman paradox and the Newcomb’s paradox as psychological games, Cowles Foundation\n\nDiscussion Paper No. 1128, 1996.\n\n[36] J. Geanakoplos, D. Pearce, E. Stacchetti, Psychological games and sequential rationality, Games Econ. Behav. 1\n\n(1989) 60–79.\n\n[37] I. Gilboa, D. Schmeidler, Information dependent games: Can common sense be common knowledge? Econ. Let-\n\nters 27 (1988) 215–221.\n\n[38] S. Govindan, R. Wilson, On forward induction, Mimeo, Department of Economics, University of Iowa, and Stanford\n\nBusiness School, Stanford University, 2008.\n\n[39] G. Guerra, D.J. Zizzo, Trust responsiveness and beliefs, J. Econ. Behav. Organ. 55 (2004) 25–30.\n\n\fP. Battigalli, M. Dufwenberg / Journal of Economic Theory 144 (2009) 1–35\n\n35\n\n[40] F. Gul, W. Pesendorfer, The canonical type space for interdependent preferences, Mimeo, Princeton University,\n\n2007.\n\n[41] J. Harsanyi, Games of incomplete information played by Bayesian players. Parts I, II, III, Manage. Sci. 14 (1967-\n\n1968) 159–182, 320–334, 486–502.\n\n[42] P.H. Huang, H.-M. Wu, More order without more law: A theory of social norms and organizational cultures, J. Law,\n\nEcon., Organ. 12 (1994) 390–406.\n\n[43] S. Huck, D. Kübler, Social pressure, uncertainty, and cooperation, Econ. Governance 1 (2000) 199–212.\n[44] E. Karni, Utility theory with probability dependent outcome valuation, J. Econ. Theory 57 (1992) 111–124.\n[45] E. Karni, E.E. Schlee, Utility theory with probability dependent outcome valuation: Extensions and applications,\n\nJ. Risk Uncertainty 10 (1995) 127–142.\n\n[46] A. Kechris, Classical Descriptive Set Theory, Springer-Verlag, Berlin, 1995.\n[47] V. Kolpin, Equilibrium reﬁnements in psychological games, Games Econ. Behav. 4 (1992) 218–231.\n[48] B. K˝oszegi, Emotional agency, Quart. J. Econ. 12 (2006) 121–156.\n[49] B. K˝oszegi, M. Rabin, A model of reference-dependent preferences, Quart. J. Econ. 121 (2006) 1133–1166.\n[50] B. K˝oszegi, M. Rabin, Reference-dependent risk attitudes, Amer. Econ. Rev. 97 (2007) 1047–1073.\n[51] D. Kreps, R. Wilson, Sequential equilibrium, Econometrica 50 (1982) 863–894.\n[52] H.W. Krohne, Individual differences in emotional reactions and coping, in: R.J. Davidson, K.R. Scherer, H.H.\n\nGoldsmith (Eds.), Handbook of Affective Sciences, Oxford University Press, Oxford, 2003, pp. 698–725.\n\n[53] H.W. Kuhn, Extensive games and the problem of information, in: H.W. Kuhn, A.W. Tucker (Eds.), Contributions to\n\nthe Theory of Games II, Princeton University Press, Princeton, NJ, 1953, pp. 193–216.\n\n[54] D.K. Levine, Modeling altruism and spitefulness in experiments, Rev. Econ. Dynam. 1 (1998) 593–622.\n[55] J. Li, The power of convention: A theory of social preferences, J. Econ. Behav. Organ. 65 (2008) 489–505.\n[56] G. Loomes, R. Sugden, Regret theory: An alternative theory of rational choice under uncertainty, Econ. J. 92 (1982)\n\n805–824.\n\n[57] G. Loomes, R. Sugden, Disappointment and dynamic consistency in choice under uncertainty, Rev. Econ. Stud. 53\n\n(1986) 271–282.\n\n[58] M. Machina, ‘Rational’ decision making versus ‘rational’ decision modeling, J. Math. Psychology 24 (1981) 163–\n\n175.\n\n[59] M. Machina, Dynamic consistency and non-expected utility models of choice under uncertainty, J. Econ. Lit. 24\n\n(1989) 163–175.\n\n[60] M. Mariotti, Strategic extensive form games: Deﬁnitions and comments on sequential rationality, with an application\n\nto rational procrastination, Mimeo, Queen Mary, University of London, 2003.\n\n[61] J.-F. Mertens, S. Zamir, Formulation of Bayesian analysis for games with incomplete information, Int. J. Game\n\nTheory 14 (1985) 1–29.\n\n[62] S. Morris, Political correctness, J. Polit. Economy 109 (2001) 231–265.\n[63] M. Osborne, A. Rubinstein, A Course in Game Theory, MIT Press, Cambridge, MA, 1994.\n[64] M. Ottaviani, P. Sorensen, Reputational cheap talk, RAND J. Econ. 37 (2006) 155–175.\n[65] D. Pearce, Rationalizable strategic behavior and the problem of perfection, Econometrica 52 (1984) 1029–1050.\n[66] R. Pope, Biases from omitted risk effects in standard gamble utilities, J. Health Econ. 23 (2004) 695–735.\n[67] M. Rabin, Incorporating fairness into game theory and economics, Amer. Econ. Rev. 83 (1993) 1281–1302.\n[68] M. Rabin, Psychology and economics, J. Econ. Lit. 83 (1998) 11–46.\n[69] P. Reny, Backward induction, normal form perfection and explicable equilibria, Econometrica 60 (1992) 626–649.\n[70] A. Renyi, On a new axiomatic theory of probability, Acta Math. Acad. Sci. Hungaricae 6 (1955) 285–335.\n[71] B.J. Rufﬂe, Gift giving with emotions, J. Econ. Behav. Organ. 39 (1999) 399–420.\n[72] A. Sebald, Procedural concerns and reciprocity, ECARES Discussion Paper 2007/62, Universite Libre de Bruxelles,\n\n2007.\n\n[73] U. Segal, J. Sobel, Tit for tat: Foundations for preferences for reciprocity in strategic settings, J. Econ. Theory 136\n\n(2007) 197–216.\n\n[74] R. Selten, Re-examination of the perfectness concept for equilibrium points in extensive games, Int. J. Game The-\n\nory 4 (1975) 25–55.\n\n[75] J. Sobel, Interdependent preferences and reciprocity, J. Econ. Lit. 43 (2005) 392–436.\n[76] R.H. Strotz, Myopia and inconsistency in dynamic utility maximization, Rev. Econ. Stud. 23 (1956) 165–180.\n[77] S. Tadelis, The power of shame and the rationality of trust, Mimeo, UC Berkeley, 2007.\n[78] J.P. Tangney, Recent advances in the empirical study of shame and guilt, Amer. Behav. Sci. 38 (1995) 1132–1145.\n\n"}